{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cdee9bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"9\"\n",
    "\n",
    "\n",
    "from math import pi\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from typing import Optional, Tuple, Callable, Union, List\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import vmap, jit, pmap\n",
    "from jax import random\n",
    "import optax\n",
    "import numpyro.distributions as dist\n",
    "\n",
    "\n",
    "# jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import numpy as np\n",
    "from astropy import units as u\n",
    "from astropy import constants as c\n",
    "\n",
    "\n",
    "import odisseo\n",
    "from odisseo import construct_initial_state\n",
    "from odisseo.integrators import leapfrog\n",
    "from odisseo.dynamics import direct_acc, DIRECT_ACC, DIRECT_ACC_LAXMAP, DIRECT_ACC_FOR_LOOP, DIRECT_ACC_MATRIX\n",
    "from odisseo.option_classes import SimulationConfig, SimulationParams, MNParams, NFWParams, PlummerParams, MN_POTENTIAL, NFW_POTENTIAL\n",
    "from odisseo.initial_condition import Plummer_sphere, ic_two_body, sample_position_on_sphere, inclined_circular_velocity, sample_position_on_circle, inclined_position\n",
    "from odisseo.utils import center_of_mass\n",
    "from odisseo.time_integration import time_integration\n",
    "from odisseo.units import CodeUnits\n",
    "from odisseo.visualization import create_3d_gif, create_projection_gif, energy_angular_momentum_plot\n",
    "from odisseo.potentials import MyamotoNagai, NFW\n",
    "from odisseo.option_classes import DIFFRAX_BACKEND, DOPRI5, TSIT5, SEMIIMPLICITEULER, LEAPFROGMIDPOINT, REVERSIBLEHEUN\n",
    "\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.size': 20,\n",
    "    'axes.labelsize': 20,\n",
    "    'xtick.labelsize': 13,\n",
    "    'ytick.labelsize': 13,\n",
    "    'legend.fontsize': 15,\n",
    "})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b31173d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 15:29:02.157045: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745933342.175765 2723962 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745933342.181166 2723962 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745933342.195903 2723962 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745933342.195930 2723962 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745933342.195932 2723962 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745933342.195933 2723962 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import haiku as hk\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "from jaxopt import Bisection\n",
    "from jaxopt.linear_solve import solve_normal_cg\n",
    "\n",
    "# tfp = tfp.substrates.jax\n",
    "tfb = tfp.bijectors\n",
    "tfd = tfp.distributions\n",
    "\n",
    "\n",
    "# This module is to store our implicit inverse functions\n",
    "@partial(jax.custom_vjp, nondiff_argnums=(0,))\n",
    "def root_bisection(f, params):\n",
    "    \"\"\"\n",
    "    f: optimality fn with input arg (params, x)\n",
    "    \"\"\"\n",
    "    bisec = Bisection(\n",
    "        optimality_fun=f,\n",
    "        lower=0.0,\n",
    "        upper=1.0,\n",
    "        check_bracket=False,\n",
    "        maxiter=100,\n",
    "        tol=1e-06,\n",
    "    )\n",
    "    return bisec.run(None, params).params\n",
    "\n",
    "\n",
    "def root_bisection_fwd(f, params):\n",
    "    z_star = root_bisection(f, params)\n",
    "    return z_star, (params, z_star)\n",
    "\n",
    "\n",
    "def root_bwd(f, res, z_star_bar):\n",
    "    params, z_star = res\n",
    "    _, vjp_a = jax.vjp(lambda p: f(z_star, p), params)\n",
    "    _, vjp_z = jax.vjp(lambda z: f(z, params), z_star)\n",
    "    return vjp_a(solve_normal_cg(lambda u: vjp_z(u)[0], -z_star_bar))\n",
    "\n",
    "\n",
    "root_bisection.defvjp(root_bisection_fwd, root_bwd)\n",
    "\n",
    "\n",
    "def make_inverse_fn(f):\n",
    "    \"\"\"Defines the inverse of the input function, and provides implicit gradients\n",
    "    of the inverse.\n",
    "\n",
    "    Args:\n",
    "      f: callable of input shape (params, x)\n",
    "    Retuns:\n",
    "      inv_f: callable of with args (params, y)\n",
    "    \"\"\"\n",
    "\n",
    "    def inv_fn(params, y):\n",
    "        def optimality_fn(x, params):\n",
    "            p, y = params\n",
    "            return f(p, x) - y\n",
    "\n",
    "        return root_bisection(optimality_fn, [params, y])\n",
    "\n",
    "    return inv_fn\n",
    "\n",
    "# Bijiector functions\n",
    "class MixtureAffineSigmoidBijector(tfp.bijectors.Bijector):\n",
    "    \"\"\"\n",
    "    Bijector based on a ramp function, and implemented using an implicit\n",
    "    layer.\n",
    "    This implementation is based on the Smooth Normalizing Flows described\n",
    "    in: https://arxiv.org/abs/2110.00351\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, a, b, c, p, name=\"MixtureAffineSigmoidBijector\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          rho: function of x that defines a ramp function between 0 and 1\n",
    "          a,b,c: scalar parameters of the coupling layer.\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__(forward_min_event_ndims=0, name=name)\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "        self.p = p\n",
    "\n",
    "        def sigmoid(x, a, b, c):\n",
    "            z = (jax.scipy.special.logit(x) + b) * a\n",
    "            y = jax.nn.sigmoid(z) * (1 - c) + c * x\n",
    "            return y\n",
    "\n",
    "        # Rescaled bijection\n",
    "        def f(params, x):\n",
    "            a, b, c, p = params\n",
    "            a_in, b_in = [0.0 - 1e-1, 1.0 + 1e-1]\n",
    "\n",
    "            x = (x - a_in) / (b_in - a_in)\n",
    "            x0 = (jnp.zeros_like(x) - a_in) / (b_in - a_in)\n",
    "            x1 = (jnp.ones_like(x) - a_in) / (b_in - a_in)\n",
    "\n",
    "            y = sigmoid(x, a, b, c)\n",
    "            y0 = sigmoid(x0, a, b, c)\n",
    "            y1 = sigmoid(x1, a, b, c)\n",
    "\n",
    "            y = (y - y0) / (y1 - y0)\n",
    "            return jnp.sum(p * (y * (1 - c) + c * x), axis=0)\n",
    "\n",
    "        self.f = f\n",
    "\n",
    "        # Inverse bijector\n",
    "        self.inv_f = make_inverse_fn(f)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        return jax.vmap(jax.vmap(self.f))([self.a, self.b, self.c, self.p], x)\n",
    "\n",
    "    def _inverse(self, y):\n",
    "        return jax.vmap(jax.vmap(self.inv_f))([self.a, self.b, self.c, self.p], y)\n",
    "\n",
    "    def _forward_log_det_jacobian(self, x):\n",
    "        def logdet_fn(x, a, b, c, p):\n",
    "            g = jax.grad(self.f, argnums=1)([a, b, c, p], x)\n",
    "            s, logdet = jnp.linalg.slogdet(jnp.atleast_2d(g))\n",
    "            return s * logdet\n",
    "\n",
    "        return jax.vmap(jax.vmap(logdet_fn))(x, self.a, self.b, self.c, self.p)\n",
    "\n",
    "\n",
    "#Coupling layers\n",
    "class AffineCoupling(hk.Module):\n",
    "    def __init__(\n",
    "        self, y, *args, layers=[128, 128], activation=jax.nn.leaky_relu, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        y, conditioning variable\n",
    "        layers, list of hidden layers\n",
    "        activation, activation function for hidden layers\n",
    "        \"\"\"\n",
    "        self.y = y\n",
    "        self.layers = layers\n",
    "        self.activation = activation\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __call__(self, x, output_units, **condition_kwargs):\n",
    "        net = jnp.concatenate([x, self.y], axis=-1)\n",
    "        for i, layer_size in enumerate(self.layers):\n",
    "            net = self.activation(hk.Linear(layer_size, name=\"layer%d\" % i)(net))\n",
    "\n",
    "        shifter = tfb.Shift(hk.Linear(output_units)(net))\n",
    "        scaler = tfb.Scale(jnp.clip(jnp.exp(hk.Linear(output_units)(net)), 1e-2, 1e2))\n",
    "        return tfb.Chain([shifter, scaler])\n",
    "\n",
    "\n",
    "class AffineSigmoidCoupling(hk.Module):\n",
    "    \"\"\"This is the coupling layer used in the Flow.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        y,\n",
    "        *args,\n",
    "        layers=[128, 128],\n",
    "        n_components=32,\n",
    "        activation=jax.nn.silu,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        y, conditioning variable\n",
    "        layers, list of hidden layers\n",
    "        n_components, number of mixture components\n",
    "        activation, activation function for hidden layers\n",
    "        \"\"\"\n",
    "        self.y = y\n",
    "        self.layers = layers\n",
    "        self.n_components = n_components\n",
    "        self.activation = activation\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __call__(self, x, output_units, **condition_kwargs):\n",
    "        net = jnp.concatenate([x, self.y], axis=-1)\n",
    "        for i, layer_size in enumerate(self.layers):\n",
    "            net = self.activation(hk.Linear(layer_size, name=\"layer%d\" % i)(net))\n",
    "\n",
    "        log_a_bound = 4\n",
    "        min_density_lower_bound = 1e-4\n",
    "        n_components = self.n_components\n",
    "\n",
    "        log_a = (\n",
    "            jax.nn.tanh(hk.Linear(output_units * n_components, name=\"l3\")(net))\n",
    "            * log_a_bound\n",
    "        )\n",
    "        b = hk.Linear(output_units * n_components, name=\"l4\")(net)\n",
    "        c = min_density_lower_bound + jax.nn.sigmoid(\n",
    "            hk.Linear(output_units * n_components, name=\"l5\")(net)\n",
    "        ) * (1 - min_density_lower_bound)\n",
    "        p = hk.Linear(output_units * n_components, name=\"l6\")(net)\n",
    "\n",
    "        log_a = log_a.reshape(-1, output_units, n_components)\n",
    "        b = b.reshape(-1, output_units, n_components)\n",
    "        c = c.reshape(-1, output_units, n_components)\n",
    "        p = p.reshape(-1, output_units, n_components)\n",
    "        p = jax.nn.softmax(p)\n",
    "\n",
    "        return MixtureAffineSigmoidBijector(jnp.exp(log_a), b, c, p)\n",
    "\n",
    "# Normalizing flow model \n",
    "class ConditionalRealNVP(hk.Module):\n",
    "    \"\"\"A normalizing flow based on RealNVP using specified bijector functions.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, d, *args, n_layers=3, bijector_fn=AffineSigmoidCoupling, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        d, dimensionality of the input\n",
    "        n_layers, number of layers\n",
    "        coupling_layer, list of coupling layers\n",
    "        \"\"\"\n",
    "        self.d = d\n",
    "        self.n_layer = n_layers\n",
    "        self.bijector_fn = bijector_fn\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __call__(self, y):\n",
    "        chain = tfb.Chain(\n",
    "            [\n",
    "                tfb.Permute(jnp.arange(self.d)[::-1])(\n",
    "                    tfb.RealNVP(\n",
    "                        self.d // 2, bijector_fn=self.bijector_fn(y, name=\"b%d\" % i)\n",
    "                    )\n",
    "                )\n",
    "                for i in range(self.n_layer)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        nvp = tfd.TransformedDistribution(\n",
    "            tfd.MultivariateNormalDiag(0.5 * jnp.ones(self.d), 0.05 * jnp.ones(self.d)),\n",
    "            bijector=chain,\n",
    "        )\n",
    "\n",
    "        return nvp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3c8a0b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "#mimic the argument parser used in the sbi_bm_lens\n",
    "\n",
    "class args_namedtuple(NamedTuple):\n",
    "\n",
    "    #seed\n",
    "    seed = 42\n",
    "\n",
    "    #activation function\n",
    "    activ_fun = \"silu\"\n",
    "\n",
    "    #Normalizing flow\n",
    "    nf = \"smooth\"\n",
    "\n",
    "    #score type\n",
    "    score_type = \"conditional\"\n",
    "\n",
    "    #sbi method\n",
    "    sbi_method = \"nle\"\n",
    "\n",
    "    #number of bijiector layers\n",
    "    n_bijector_layers = 5\n",
    "\n",
    "    #number of coupling layers\n",
    "    n_flow_layers = 3\n",
    "\n",
    "    #number of parameters\n",
    "    n_params = 5 #[\"t_end\", \"Mtot_plummer\", \"a_plummer\", \"M_NFW\", \"r_s\"]\n",
    "\n",
    "    #number of step\n",
    "    total_steps = 10_000\n",
    "\n",
    "    #batch size\n",
    "    bacth_size = 64\n",
    "\n",
    "    #score weight\n",
    "    score_weight = 1.0\n",
    "\n",
    "\n",
    "args = args_namedtuple()\n",
    "\n",
    "master_seed = jax.random.PRNGKey(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3d394311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done ✓\n"
     ]
    }
   ],
   "source": [
    "#### CREATE THE NDE #####\n",
    "\n",
    "#activation  function\n",
    "if args.activ_fun == \"silu\":\n",
    "    activ_fun = jax.nn.silu\n",
    "elif args.activ_fun == \"sin\":\n",
    "    activ_func = jnp.sin\n",
    "\n",
    "#score type\n",
    "if args.sbi_method == \"nle\":\n",
    "    score_type = \"conditional\"  #need to be pass to the model object (stochastic simulator)\n",
    "\n",
    "#NF type, either smooth or affine\n",
    "if args.nf == \"smooth\":\n",
    "    if args.sbi_method == \"npe\":\n",
    "        pass\n",
    "    elif args.sbi_method == \"nle\":\n",
    "        # how these qantities are comute (with dataset form prior)\n",
    "        # scale_y = jnp.std(dataset_y, axis=0) / 0.07\n",
    "        # shift_y = jnp.mean(dataset_y / scale_y, axis=0) - 0.5\n",
    "\n",
    "        #just place holder, need to be calculated from the dataset for normalization \n",
    "        scale = jnp.ones(6)\n",
    "        shift = jnp.ones(6)\n",
    "\n",
    "        #NF ARCHITECTURE\n",
    "        #128 neuron per layer\n",
    "        bijector_layers = [128] * args.n_bijector_layers\n",
    "        \n",
    "        bijector = partial(\n",
    "            AffineSigmoidCoupling,\n",
    "            layers=bijector_layers,\n",
    "            activation=activ_fun,\n",
    "            n_components=16,\n",
    "        )\n",
    "        \n",
    "        NF = partial(ConditionalRealNVP, \n",
    "                     n_layers=args.n_flow_layers, \n",
    "                     bijector_fn=bijector)\n",
    "\n",
    "        class NDE(hk.Module):\n",
    "            def __call__(self, y):\n",
    "                nvp = NF(args.n_params)(y)\n",
    "                return tfd.TransformedDistribution(\n",
    "                    nvp, tfb.Chain([tfb.Scale(scale), tfb.Shift(shift)])\n",
    "                )\n",
    "        \n",
    "elif args.nf == \"affine\":\n",
    "    bijector_layers = [128] * args.n_bijector_layers\n",
    "\n",
    "    bijector = partial(AffineCoupling, layers=bijector_layers, activation=activ_fun)\n",
    "\n",
    "    NF = partial(ConditionalRealNVP, n_layers=args.n_flow_layers, bijector_fn=bijector)\n",
    "\n",
    "    class NDE(hk.Module):\n",
    "        def __call__(self, y):\n",
    "            return NF(args.n_params)(y)\n",
    "        \n",
    "#FOR NPE NEED TO HAVE SMOOTH NORMALIZING FLOW\n",
    "if args.nf == \"affine\" and args.sbi_method == \"npe\" and args.score_weight > 0:\n",
    "    raise ValueError(\"NDE has to be smooth\")\n",
    "\n",
    "elif args.sbi_method == \"nle\":\n",
    "    nf_log_prob = hk.without_apply_rng(\n",
    "        hk.transform(lambda theta, y: NDE()(theta).log_prob(y).squeeze())\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"done ✓\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e42f54e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This should be used to create the compress dataset, in this simplified case\n",
    "#we are using it just to sample the prior, to be implemented the compression\n",
    "\n",
    "class JaxCompressedSimulator:\n",
    "    def __init__(self, ):\n",
    "\n",
    "        # self.compressor = compressor #not used\n",
    "        # self.params_compressor = params_compressor #not used\n",
    "        # self.opt_state = opt_state #not used\n",
    "\n",
    "        self.t_end = dist.Uniform(0.5, 10.0)\n",
    "        self.Mtot_plummer = dist.Uniform(1e3, 1e5)\n",
    "        self.a_plummer = dist.Uniform(0.1, 2.0)\n",
    "        self.M_NFW = dist.Uniform(5e11, 1.5e12)\n",
    "        self.r_s = dist.Uniform(1.0, 20.0)\n",
    "\n",
    "        self.stack = [\n",
    "            self.t_end,\n",
    "            self.Mtot_plummer,\n",
    "            self.a_plummer,\n",
    "            self.M_NFW,\n",
    "            self.r_s,\n",
    "        ]\n",
    "\n",
    "    def prior_sample(self, sample_shape, master_key):\n",
    "        samples = []\n",
    "\n",
    "        keys = jax.random.split(master_key, 6)\n",
    "\n",
    "        for i, distribution in enumerate(self.stack):\n",
    "            samples.append(distribution.sample(keys[i], sample_shape))\n",
    "\n",
    "        return jnp.stack(samples).T\n",
    "\n",
    "    def prior_log_prob(self, values):\n",
    "        logp = 0\n",
    "\n",
    "        for i, distribution in enumerate(self.stack):\n",
    "            logp += distribution.log_prob(values[..., i])\n",
    "\n",
    "        return logp\n",
    "\n",
    "compressed_simulator = JaxCompressedSimulator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "edb1df00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... inference\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incompatible shapes for broadcasting: shapes=[(1, 5), (6,)]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/jax/_src/lax/lax.py:218\u001b[39m, in \u001b[36m_broadcast_shapes_uncached\u001b[39m\u001b[34m(*shapes)\u001b[39m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_try_broadcast_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrank_promoted_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbroadcast_shapes\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    220\u001b[39m   \u001b[38;5;66;03m# Raise ValueError here for backward compatibility.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/jax/_src/lax/lax.py:135\u001b[39m, in \u001b[36m_try_broadcast_shapes\u001b[39m\u001b[34m(name, *shapes)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m got incompatible shapes for broadcasting: \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    136\u001b[39m                       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mtuple\u001b[39m,\u001b[38;5;250m \u001b[39mshapes)))\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(result_shape)\n",
      "\u001b[31mTypeError\u001b[39m: broadcast_shapes got incompatible shapes for broadcasting: (1, 5), (1, 6).",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/jax/_src/util.py:279\u001b[39m, in \u001b[36mcache.<locals>.wrap.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    278\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m f(*args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrace_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrace_context_in_key\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_ignore\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m              \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/jax/_src/util.py:273\u001b[39m, in \u001b[36mcache.<locals>.wrap.<locals>.cached\u001b[39m\u001b[34m(_, *args, **kwargs)\u001b[39m\n\u001b[32m    271\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache(max_size)\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached\u001b[39m(_, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/jax/_src/lax/lax.py:202\u001b[39m, in \u001b[36m_broadcast_shapes_cached\u001b[39m\u001b[34m(*shapes)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;129m@cache\u001b[39m()\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_broadcast_shapes_cached\u001b[39m(*shapes: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, ...]) -> \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, ...]:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_broadcast_shapes_uncached\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/jax/_src/lax/lax.py:221\u001b[39m, in \u001b[36m_broadcast_shapes_uncached\u001b[39m\u001b[34m(*shapes)\u001b[39m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    220\u001b[39m   \u001b[38;5;66;03m# Raise ValueError here for backward compatibility.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Incompatible shapes for broadcasting: shapes=[(1, 5), (6,)]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/jax/_src/lax/lax.py:135\u001b[39m, in \u001b[36m_try_broadcast_shapes\u001b[39m\u001b[34m(name, *shapes)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m got incompatible shapes for broadcasting: \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    136\u001b[39m                       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mtuple\u001b[39m,\u001b[38;5;250m \u001b[39mshapes)))\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(result_shape)\n",
      "\u001b[31mTypeError\u001b[39m: broadcast_shapes got incompatible shapes for broadcasting: (1, 5), (1, 6).",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[89]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m######## INFERENCE ########\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m... inference\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m params_init = \u001b[43mnf_log_prob\u001b[49m\u001b[43m.\u001b[49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaster_seed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mn_params\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mn_params\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m prior_mean = jnp.mean(compressed_simulator.prior_sample((\u001b[32m1000\u001b[39m,), master_seed), axis=\u001b[32m0\u001b[39m)\n\u001b[32m     10\u001b[39m nb_steps = args.total_steps - args.total_steps * \u001b[32m0.2\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/haiku/_src/transform.py:166\u001b[39m, in \u001b[36mwithout_state.<locals>.init_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minit_fn\u001b[39m(*args, **kwargs) -> hk.MutableParams:\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m   params, state = \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m state:\n\u001b[32m    168\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m base.NonEmptyStateError(\n\u001b[32m    169\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIf your transformed function uses `hk.\u001b[39m\u001b[33m{\u001b[39m\u001b[33mget,set}_state` then use \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    170\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`hk.transform_with_state`.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/haiku/_src/transform.py:422\u001b[39m, in \u001b[36mtransform_with_state.<locals>.init_fn\u001b[39m\u001b[34m(rng, *args, **kwargs)\u001b[39m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m base.new_context(rng=rng) \u001b[38;5;28;01mas\u001b[39;00m ctx:\n\u001b[32m    421\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    423\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m jax.errors.UnexpectedTracerError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    424\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m jax.errors.UnexpectedTracerError(unexpected_tracer_hint) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[84]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(theta, y)\u001b[39m\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNDE has to be smooth\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m args.sbi_method == \u001b[33m\"\u001b[39m\u001b[33mnle\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     64\u001b[39m     nf_log_prob = hk.without_apply_rng(\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m         hk.transform(\u001b[38;5;28;01mlambda\u001b[39;00m theta, y: \u001b[43mNDE\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m.squeeze())\n\u001b[32m     66\u001b[39m     )\n\u001b[32m     69\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mdone ✓\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/distributions/distribution.py:1287\u001b[39m, in \u001b[36mDistribution.log_prob\u001b[39m\u001b[34m(self, value, name, **kwargs)\u001b[39m\n\u001b[32m   1275\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, value, name=\u001b[33m'\u001b[39m\u001b[33mlog_prob\u001b[39m\u001b[33m'\u001b[39m, **kwargs):\n\u001b[32m   1276\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Log probability density/mass function.\u001b[39;00m\n\u001b[32m   1277\u001b[39m \n\u001b[32m   1278\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1285\u001b[39m \u001b[33;03m      values of type `self.dtype`.\u001b[39;00m\n\u001b[32m   1286\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1287\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/distributions/distribution.py:1269\u001b[39m, in \u001b[36mDistribution._call_log_prob\u001b[39m\u001b[34m(self, value, name, **kwargs)\u001b[39m\n\u001b[32m   1267\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._name_and_control_scope(name, value, kwargs):\n\u001b[32m   1268\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m_log_prob\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1269\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1270\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m_prob\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m   1271\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tf.math.log(\u001b[38;5;28mself\u001b[39m._prob(value, **kwargs))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/distributions/transformed_distribution.py:364\u001b[39m, in \u001b[36m_TransformedDistribution._log_prob\u001b[39m\u001b[34m(self, y, **kwargs)\u001b[39m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_log_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, y, **kwargs):\n\u001b[32m    363\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bijector._is_injective:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m     log_prob, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexperimental_local_measure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackward_compat\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    366\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m log_prob\n\u001b[32m    368\u001b[39m   \u001b[38;5;66;03m# TODO(b/197680518): Support base measure handling for non-injective\u001b[39;00m\n\u001b[32m    369\u001b[39m   \u001b[38;5;66;03m# bijectors.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/distributions/transformed_distribution.py:611\u001b[39m, in \u001b[36m_TransformedDistribution.experimental_local_measure\u001b[39m\u001b[34m(self, y, backward_compat, **kwargs)\u001b[39m\n\u001b[32m    607\u001b[39m distribution_kwargs, bijector_kwargs = \u001b[38;5;28mself\u001b[39m._kwargs_split_fn(kwargs)\n\u001b[32m    609\u001b[39m \u001b[38;5;66;03m# For caching to work, it is imperative that the bijector is the first to\u001b[39;00m\n\u001b[32m    610\u001b[39m \u001b[38;5;66;03m# modify the input.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m611\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbijector\u001b[49m\u001b[43m.\u001b[49m\u001b[43minverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbijector_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    612\u001b[39m event_ndims = \u001b[38;5;28mself\u001b[39m.bijector.inverse_event_ndims(\n\u001b[32m    613\u001b[39m     tf.nest.map_structure(ps.rank_from_shape, \u001b[38;5;28mself\u001b[39m._event_shape_tensor(),\n\u001b[32m    614\u001b[39m                           \u001b[38;5;28mself\u001b[39m.event_shape), **bijector_kwargs)\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bijector._is_injective:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/bijectors/bijector.py:1390\u001b[39m, in \u001b[36mBijector.inverse\u001b[39m\u001b[34m(self, y, name, **kwargs)\u001b[39m\n\u001b[32m   1372\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minverse\u001b[39m(\u001b[38;5;28mself\u001b[39m, y, name=\u001b[33m'\u001b[39m\u001b[33minverse\u001b[39m\u001b[33m'\u001b[39m, **kwargs):\n\u001b[32m   1373\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Returns the inverse `Bijector` evaluation, i.e., X = g^{-1}(Y).\u001b[39;00m\n\u001b[32m   1374\u001b[39m \n\u001b[32m   1375\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1388\u001b[39m \u001b[33;03m    NotImplementedError: if `_inverse` is not implemented.\u001b[39;00m\n\u001b[32m   1389\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1390\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_inverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/bijectors/bijector.py:1370\u001b[39m, in \u001b[36mBijector._call_inverse\u001b[39m\u001b[34m(self, y, name, **kwargs)\u001b[39m\n\u001b[32m   1368\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_injective:  \u001b[38;5;66;03m# No caching for non-injective\u001b[39;00m\n\u001b[32m   1369\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inverse(y, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1370\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cache\u001b[49m\u001b[43m.\u001b[49m\u001b[43minverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/internal/cache_util.py:347\u001b[39m, in \u001b[36mBijectorCache.inverse\u001b[39m\u001b[34m(self, y, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minverse\u001b[39m(\u001b[38;5;28mself\u001b[39m, y, **kwargs):\n\u001b[32m    337\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Invokes the 'inverse' transformation, or looks up previous results.\u001b[39;00m\n\u001b[32m    338\u001b[39m \n\u001b[32m    339\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    345\u001b[39m \u001b[33;03m    The output of the bijector's `_inverse` method, or a cached result.\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_lookup\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inverse_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/internal/cache_util.py:493\u001b[39m, in \u001b[36mBijectorCache._lookup\u001b[39m\u001b[34m(self, input, forward_name, inverse_name, **kwargs)\u001b[39m\n\u001b[32m    487\u001b[39m   output = output_ref()\n\u001b[32m    488\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m   \u001b[38;5;66;03m# Get the output structure, and declare a\u001b[39;00m\n\u001b[32m    490\u001b[39m   \u001b[38;5;66;03m# weakref to clear it from the cache once it gets GCed\u001b[39;00m\n\u001b[32m    491\u001b[39m   output = nest.map_structure(\n\u001b[32m    492\u001b[39m       _containerize,\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m       \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    494\u001b[39m   output_ref = WeakStructRef(\n\u001b[32m    495\u001b[39m       output,\n\u001b[32m    496\u001b[39m       subkey=(\u001b[38;5;28mself\u001b[39m.bijector, \u001b[38;5;28mself\u001b[39m.bijector_class, inverse_name, kwargs),\n\u001b[32m    497\u001b[39m       callback=\u001b[38;5;28mself\u001b[39m.storage.maybe_del)\n\u001b[32m    498\u001b[39m   \u001b[38;5;66;03m# Set the input->output mapping.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/internal/cache_util.py:532\u001b[39m, in \u001b[36mBijectorCache._invoke\u001b[39m\u001b[34m(self, input, fn_name, kwargs, attributes)\u001b[39m\n\u001b[32m    530\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_invoke\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, fn_name, kwargs, attributes):  \u001b[38;5;66;03m# pylint: disable=unused-argument\u001b[39;00m\n\u001b[32m    531\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Invokes the wrapped function. Override to customize behavior.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbijector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/bijectors/composition.py:614\u001b[39m, in \u001b[36mComposition._inverse\u001b[39m\u001b[34m(self, y, **kwargs)\u001b[39m\n\u001b[32m    610\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_injective:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m    611\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    612\u001b[39m       \u001b[33m'\u001b[39m\u001b[33mInvert is not implemented for compositions of \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    613\u001b[39m       \u001b[33m'\u001b[39m\u001b[33mnon-injective bijectors.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m614\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_walk_inverse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m.\u001b[49m\u001b[43minverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/bijectors/composition.py:352\u001b[39m, in \u001b[36mComposition._call_walk_inverse\u001b[39m\u001b[34m(self, step_fn, *args, **kwargs)\u001b[39m\n\u001b[32m    348\u001b[39m args = \u001b[38;5;28mtuple\u001b[39m(nest_util.coerce_structure(\u001b[38;5;28mself\u001b[39m.inverse_min_event_ndims, y)\n\u001b[32m    349\u001b[39m              \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m args)\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m352\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_walk_inverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[38;5;66;03m# Convert a tuple of structures to a structure of tuples. This\u001b[39;00m\n\u001b[32m    355\u001b[39m \u001b[38;5;66;03m# allows `_walk` methods to route aligned structures of inputs/outputs\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;66;03m# independently, obviates the need for conditional tuple unpacking.\u001b[39;00m\n\u001b[32m    357\u001b[39m packed_args = pack_structs_like(\u001b[38;5;28mself\u001b[39m.inverse_min_event_ndims, *args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/bijectors/chain.py:150\u001b[39m, in \u001b[36m_Chain._walk_inverse\u001b[39m\u001b[34m(self, step_fn, y, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Applies `transform_fn` to `y` sequentially over nested bijectors.\"\"\"\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m bij \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bijectors:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   y = \u001b[43mstep_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbij\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbij\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/bijectors/composition.py:615\u001b[39m, in \u001b[36mComposition._inverse.<locals>.<lambda>\u001b[39m\u001b[34m(b, y, **kwargs)\u001b[39m\n\u001b[32m    610\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_injective:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m    611\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    612\u001b[39m       \u001b[33m'\u001b[39m\u001b[33mInvert is not implemented for compositions of \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    613\u001b[39m       \u001b[33m'\u001b[39m\u001b[33mnon-injective bijectors.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    614\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_walk_inverse(\n\u001b[32m--> \u001b[39m\u001b[32m615\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m b, y, **kwargs: \u001b[43mb\u001b[49m\u001b[43m.\u001b[49m\u001b[43minverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    616\u001b[39m     y, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/bijectors/bijector.py:1390\u001b[39m, in \u001b[36mBijector.inverse\u001b[39m\u001b[34m(self, y, name, **kwargs)\u001b[39m\n\u001b[32m   1372\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minverse\u001b[39m(\u001b[38;5;28mself\u001b[39m, y, name=\u001b[33m'\u001b[39m\u001b[33minverse\u001b[39m\u001b[33m'\u001b[39m, **kwargs):\n\u001b[32m   1373\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Returns the inverse `Bijector` evaluation, i.e., X = g^{-1}(Y).\u001b[39;00m\n\u001b[32m   1374\u001b[39m \n\u001b[32m   1375\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1388\u001b[39m \u001b[33;03m    NotImplementedError: if `_inverse` is not implemented.\u001b[39;00m\n\u001b[32m   1389\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1390\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_inverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/bijectors/bijector.py:1370\u001b[39m, in \u001b[36mBijector._call_inverse\u001b[39m\u001b[34m(self, y, name, **kwargs)\u001b[39m\n\u001b[32m   1368\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_injective:  \u001b[38;5;66;03m# No caching for non-injective\u001b[39;00m\n\u001b[32m   1369\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inverse(y, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1370\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cache\u001b[49m\u001b[43m.\u001b[49m\u001b[43minverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/internal/cache_util.py:347\u001b[39m, in \u001b[36mBijectorCache.inverse\u001b[39m\u001b[34m(self, y, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minverse\u001b[39m(\u001b[38;5;28mself\u001b[39m, y, **kwargs):\n\u001b[32m    337\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Invokes the 'inverse' transformation, or looks up previous results.\u001b[39;00m\n\u001b[32m    338\u001b[39m \n\u001b[32m    339\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    345\u001b[39m \u001b[33;03m    The output of the bijector's `_inverse` method, or a cached result.\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_lookup\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inverse_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/internal/cache_util.py:493\u001b[39m, in \u001b[36mBijectorCache._lookup\u001b[39m\u001b[34m(self, input, forward_name, inverse_name, **kwargs)\u001b[39m\n\u001b[32m    487\u001b[39m   output = output_ref()\n\u001b[32m    488\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m   \u001b[38;5;66;03m# Get the output structure, and declare a\u001b[39;00m\n\u001b[32m    490\u001b[39m   \u001b[38;5;66;03m# weakref to clear it from the cache once it gets GCed\u001b[39;00m\n\u001b[32m    491\u001b[39m   output = nest.map_structure(\n\u001b[32m    492\u001b[39m       _containerize,\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m       \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    494\u001b[39m   output_ref = WeakStructRef(\n\u001b[32m    495\u001b[39m       output,\n\u001b[32m    496\u001b[39m       subkey=(\u001b[38;5;28mself\u001b[39m.bijector, \u001b[38;5;28mself\u001b[39m.bijector_class, inverse_name, kwargs),\n\u001b[32m    497\u001b[39m       callback=\u001b[38;5;28mself\u001b[39m.storage.maybe_del)\n\u001b[32m    498\u001b[39m   \u001b[38;5;66;03m# Set the input->output mapping.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/internal/cache_util.py:532\u001b[39m, in \u001b[36mBijectorCache._invoke\u001b[39m\u001b[34m(self, input, fn_name, kwargs, attributes)\u001b[39m\n\u001b[32m    530\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_invoke\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, fn_name, kwargs, attributes):  \u001b[38;5;66;03m# pylint: disable=unused-argument\u001b[39;00m\n\u001b[32m    531\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Invokes the wrapped function. Override to customize behavior.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbijector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/bijectors/scale.py:122\u001b[39m, in \u001b[36mScale._inverse\u001b[39m\u001b[34m(self, y)\u001b[39m\n\u001b[32m    120\u001b[39m x = tf.identity(y)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m   x = \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.log_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    124\u001b[39m   x = x * tf.exp(-\u001b[38;5;28mself\u001b[39m.log_scale)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py:579\u001b[39m, in \u001b[36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    577\u001b[39m args = (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[32m    581\u001b[39m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "    \u001b[31m[... skipping hidden 14 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/jax/_src/numpy/ufuncs.py:2473\u001b[39m, in \u001b[36mtrue_divide\u001b[39m\u001b[34m(x1, x2)\u001b[39m\n\u001b[32m   2438\u001b[39m \u001b[38;5;129m@export\u001b[39m\n\u001b[32m   2439\u001b[39m \u001b[38;5;129m@partial\u001b[39m(jit, inline=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   2440\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrue_divide\u001b[39m(x1: ArrayLike, x2: ArrayLike, /) -> Array:\n\u001b[32m   2441\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calculates the division of x1 by x2 element-wise\u001b[39;00m\n\u001b[32m   2442\u001b[39m \n\u001b[32m   2443\u001b[39m \u001b[33;03m  JAX implementation of :func:`numpy.true_divide`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2471\u001b[39m \u001b[33;03m    :func:`jax.numpy.floor_divide` for integer division\u001b[39;00m\n\u001b[32m   2472\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2473\u001b[39m   x1, x2 = \u001b[43mpromote_args_inexact\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrue_divide\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2474\u001b[39m   jnp_error._set_error_if_divide_by_zero(x2)\n\u001b[32m   2475\u001b[39m   out = lax.div(x1, x2)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/jax/_src/numpy/util.py:245\u001b[39m, in \u001b[36mpromote_args_inexact\u001b[39m\u001b[34m(fun_name, *args)\u001b[39m\n\u001b[32m    243\u001b[39m _check_no_float0s(fun_name, *args)\n\u001b[32m    244\u001b[39m check_for_prngkeys(fun_name, *args)\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpromote_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mpromote_dtypes_inexact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/jax/_src/numpy/util.py:64\u001b[39m, in \u001b[36mpromote_shapes\u001b[39m\u001b[34m(fun_name, *args)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.numpy_rank_promotion.value != \u001b[33m\"\u001b[39m\u001b[33mallow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     63\u001b[39m   _rank_promotion_warning_or_error(fun_name, shapes)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m result_rank = \u001b[38;5;28mlen\u001b[39m(\u001b[43mlax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbroadcast_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [lax.broadcast_to_rank(arg, result_rank) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/jax/_src/lax/lax.py:221\u001b[39m, in \u001b[36m_broadcast_shapes_uncached\u001b[39m\u001b[34m(*shapes)\u001b[39m\n\u001b[32m    218\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m _try_broadcast_shapes(*rank_promoted_shapes, name=\u001b[33m'\u001b[39m\u001b[33mbroadcast_shapes\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    220\u001b[39m   \u001b[38;5;66;03m# Raise ValueError here for backward compatibility.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Incompatible shapes for broadcasting: shapes=[(1, 5), (6,)]"
     ]
    }
   ],
   "source": [
    "######## INFERENCE ########\n",
    "print(\"... inference\")\n",
    "\n",
    "params_init = nf_log_prob.init(\n",
    "    master_seed, 0.5 * jnp.ones([1, args.n_params]), 0.5 * jnp.ones([1, args.n_params])\n",
    ")\n",
    "\n",
    "prior_mean = jnp.mean(compressed_simulator.prior_sample((1000,), master_seed), axis=0)\n",
    "\n",
    "nb_steps = args.total_steps - args.total_steps * 0.2\n",
    "\n",
    "lr_scheduler = optax.exponential_decay(\n",
    "    init_value=0.001,\n",
    "    transition_steps=nb_steps // 50,\n",
    "    decay_rate=0.9,\n",
    "    end_value=1e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390ead59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLE:\n",
    "    def __init__(self, NDE, init_params_nde, args):\n",
    "        self.NDE = NDE\n",
    "        self.params = init_params_nde\n",
    "        self.dim = args.n_params\n",
    "\n",
    "    def log_prob_fn(self, params, theta, y):\n",
    "        return self.NDE.apply(params, theta, y)\n",
    "\n",
    "    def loss_nll_and_score(self, params, mu, batch, score, weight_score):\n",
    "        print(\"mu shape\", mu.shape)\n",
    "        print(\"batch shape\", batch.shape)   \n",
    "        lp, out = jax.vmap(\n",
    "            jax.value_and_grad(\n",
    "                lambda theta, x: self.log_prob_fn(\n",
    "                    params, theta, x\n",
    "                ).squeeze()\n",
    "            )\n",
    "        )(mu, batch)\n",
    "\n",
    "        return (\n",
    "            -jnp.mean(lp) + weight_score * jnp.sum((out - score) ** 2, axis=-1).mean()\n",
    "        )\n",
    "\n",
    "    def loss_nll(self, params, mu, batch, score, weight_score):\n",
    "        lp = self.log_prob_fn(params, mu, batch)\n",
    "\n",
    "        return -jnp.mean(lp)\n",
    "\n",
    "    def train(\n",
    "        self, data_path, learning_rate, total_steps=30_000, batch_size=128, score_weight=0\n",
    "    ):\n",
    "        #let's take only N<1000 \n",
    "        pattern = re.compile(r\"chunk_(\\d+)\\.npz\")  # capture any number of digits\n",
    "\n",
    "        files = sorted(\n",
    "            f for f in Path(data_path).glob(\"chunk_*.npz\")\n",
    "            if (m := pattern.fullmatch(f.name)) and int(m.group(1)) < 1000\n",
    "        )\n",
    "        theta_list, x_list, score_list = [], [], []\n",
    "        for f in files:\n",
    "            data = np.load(f)\n",
    "            theta_list.append(data[\"theta\"])\n",
    "            x_list.append(data[\"x\"])\n",
    "            score_list.append(data[\"score\"])\n",
    "        dataset_theta = jnp.concatenate(theta_list)\n",
    "        dataset_y = jnp.concatenate(x_list)\n",
    "        if score_weight != 0:\n",
    "            dataset_score = jnp.concatenate(score_list)\n",
    "            loss_fn = self.loss_nll_and_score\n",
    "        else:\n",
    "            loss_fn = self.loss_nll\n",
    "\n",
    "        nb_simu = len(dataset_theta)\n",
    "\n",
    "        print(\"nb of simulations used for training: \", nb_simu)\n",
    "\n",
    "        params = self.params\n",
    "        optimizer = optax.adam(learning_rate)\n",
    "        opt_state = optimizer.init(params)\n",
    "\n",
    "        @jax.jit\n",
    "        def update(params, opt_state, mu, batch, score, weight_score):\n",
    "            \"\"\"Single SGD update step.\"\"\"\n",
    "            loss, grads = jax.value_and_grad(loss_fn)(\n",
    "                params, mu, batch, score, weight_score\n",
    "            )\n",
    "            updates, new_opt_state = optimizer.update(grads, opt_state, params)\n",
    "            new_params = optax.apply_updates(params, updates)\n",
    "\n",
    "            return loss, new_params, new_opt_state\n",
    "\n",
    "        print(\"... start training\")\n",
    "\n",
    "        batch_loss = []\n",
    "        lr_scheduler_store = []\n",
    "        pbar = tqdm(range(total_steps))\n",
    "\n",
    "        for batch in pbar:\n",
    "            inds = np.random.randint(0, nb_simu, batch_size)\n",
    "            ex_theta = dataset_theta[inds]\n",
    "            ex_y = dataset_y[inds]\n",
    "            if score_weight != 0:\n",
    "                ex_score = dataset_score[inds]\n",
    "            else:\n",
    "                ex_score = None\n",
    "\n",
    "            if not jnp.isnan(ex_y).any():\n",
    "                l, params, opt_state = update(\n",
    "                    params, opt_state, ex_theta, ex_y, ex_score, score_weight\n",
    "                )\n",
    "\n",
    "                batch_loss.append(l)\n",
    "                pbar.set_description(f\"loss {l:.3f}\")\n",
    "\n",
    "                if jnp.isnan(l):\n",
    "                    break\n",
    "\n",
    "        self.params = params\n",
    "        self.loss = batch_loss\n",
    "\n",
    "        print(\"done ✓\")\n",
    "\n",
    "    def sample(\n",
    "        self,\n",
    "        log_prob_prior,\n",
    "        observation,\n",
    "        init_point,\n",
    "        key,\n",
    "        num_results=3e4,\n",
    "        num_burnin_steps=5e2,\n",
    "        num_chains=12,\n",
    "    ):\n",
    "        print(\"... running hmc\")\n",
    "\n",
    "        @jax.vmap\n",
    "        def unnormalized_log_prob(theta):\n",
    "            prior = log_prob_prior(theta)\n",
    "\n",
    "            likelihood = self.log_prob_fn(\n",
    "                self.params,\n",
    "                theta.reshape([1, self.dim]),\n",
    "                jnp.array(observation).reshape([1, self.dim]),\n",
    "            )\n",
    "\n",
    "            return likelihood + prior\n",
    "\n",
    "        # Initialize the HMC transition kernel.\n",
    "        adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(\n",
    "            tfp.mcmc.HamiltonianMonteCarlo(\n",
    "                target_log_prob_fn=unnormalized_log_prob,\n",
    "                num_leapfrog_steps=3,\n",
    "                step_size=1e-2,\n",
    "            ),\n",
    "            num_adaptation_steps=int(num_burnin_steps * 0.8),\n",
    "        )\n",
    "\n",
    "        # Run the chain (with burn-in).\n",
    "        # @jax.jit\n",
    "        def run_chain():\n",
    "            # Run the chain (with burn-in).\n",
    "            samples, is_accepted = tfp.mcmc.sample_chain(\n",
    "                num_results=num_results,\n",
    "                num_burnin_steps=num_burnin_steps,\n",
    "                current_state=jnp.array(init_point) * jnp.ones([num_chains, self.dim]),\n",
    "                kernel=adaptive_hmc,\n",
    "                trace_fn=lambda _, pkr: pkr.inner_results.is_accepted,\n",
    "                seed=key,\n",
    "            )\n",
    "\n",
    "            return samples, is_accepted\n",
    "\n",
    "        samples_hmc, is_accepted_hmc = run_chain()\n",
    "        sample_nd = samples_hmc[is_accepted_hmc]\n",
    "\n",
    "        print(\"done ✓\")\n",
    "\n",
    "        return sample_nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bcf105bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.sbi_method == \"nle\":\n",
    "    inference = SNLE(NDE=nf_log_prob, init_params_nde=params_init, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d2d763b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb of simulations used for training:  5000\n",
      "... start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu shape (64,)\n",
      "batch shape (64, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot concatenate arrays with different numbers of dimensions: got (64, 2), (64,).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[88]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43minference\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./data/data_NFW/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtotal_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbacth_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscore_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscore_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 89\u001b[39m, in \u001b[36mSNLE.train\u001b[39m\u001b[34m(self, data_path, learning_rate, total_steps, batch_size, score_weight)\u001b[39m\n\u001b[32m     86\u001b[39m     ex_score = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m jnp.isnan(ex_y).any():\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     l, params, opt_state = \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mex_theta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mex_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mex_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_weight\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     batch_loss.append(l)\n\u001b[32m     94\u001b[39m     pbar.set_description(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "    \u001b[31m[... skipping hidden 14 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mSNLE.train.<locals>.update\u001b[39m\u001b[34m(params, opt_state, mu, batch, score, weight_score)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;129m@jax\u001b[39m.jit\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupdate\u001b[39m(params, opt_state, mu, batch, score, weight_score):\n\u001b[32m     64\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Single SGD update step.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     loss, grads = \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_score\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m     updates, new_opt_state = optimizer.update(grads, opt_state, params)\n\u001b[32m     69\u001b[39m     new_params = optax.apply_updates(params, updates)\n",
      "    \u001b[31m[... skipping hidden 16 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mSNLE.loss_nll_and_score\u001b[39m\u001b[34m(self, params, mu, batch, score, weight_score)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mmu shape\u001b[39m\u001b[33m\"\u001b[39m, mu.shape)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mbatch shape\u001b[39m\u001b[33m\"\u001b[39m, batch.shape)   \n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m lp, out = \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue_and_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_prob_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m     22\u001b[39m     -jnp.mean(lp) + weight_score * jnp.sum((out - score) ** \u001b[32m2\u001b[39m, axis=-\u001b[32m1\u001b[39m).mean()\n\u001b[32m     23\u001b[39m )\n",
      "    \u001b[31m[... skipping hidden 23 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mSNLE.loss_nll_and_score.<locals>.<lambda>\u001b[39m\u001b[34m(theta, x)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mmu shape\u001b[39m\u001b[33m\"\u001b[39m, mu.shape)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mbatch shape\u001b[39m\u001b[33m\"\u001b[39m, batch.shape)   \n\u001b[32m     13\u001b[39m lp, out = jax.vmap(\n\u001b[32m     14\u001b[39m     jax.value_and_grad(\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m theta, x: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_prob_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.squeeze()\n\u001b[32m     18\u001b[39m     )\n\u001b[32m     19\u001b[39m )(mu, batch)\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m     22\u001b[39m     -jnp.mean(lp) + weight_score * jnp.sum((out - score) ** \u001b[32m2\u001b[39m, axis=-\u001b[32m1\u001b[39m).mean()\n\u001b[32m     23\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mSNLE.log_prob_fn\u001b[39m\u001b[34m(self, params, theta, y)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlog_prob_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m, params, theta, y):\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mNDE\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/haiku/_src/multi_transform.py:315\u001b[39m, in \u001b[36mwithout_apply_rng.<locals>.apply_fn\u001b[39m\u001b[34m(params, *args, **kwargs)\u001b[39m\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_fn\u001b[39m(params, *args, **kwargs):\n\u001b[32m    314\u001b[39m   check_rng_kwarg(kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/haiku/_src/transform.py:183\u001b[39m, in \u001b[36mwithout_state.<locals>.apply_fn\u001b[39m\u001b[34m(params, *args, **kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[32m    177\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    178\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mHaiku transform adds three arguments (params, state, rng) to apply. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    179\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mIf the functions you are transforming use the same names you must \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    180\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mpass them positionally (e.g. `f.apply(.., my_state)` and not by \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    181\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mname (e.g. `f.apply(.., state=my_state)`)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m out, state = \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m state:\n\u001b[32m    185\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m base.NonEmptyStateError(\n\u001b[32m    186\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mIf your transformed function uses `hk.\u001b[39m\u001b[33m{\u001b[39m\u001b[33mget,set}_state` then use \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    187\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33m`hk.transform_with_state`.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/haiku/_src/transform.py:456\u001b[39m, in \u001b[36mtransform_with_state.<locals>.apply_fn\u001b[39m\u001b[34m(params, state, rng, *args, **kwargs)\u001b[39m\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m base.new_context(params=params, state=state, rng=rng) \u001b[38;5;28;01mas\u001b[39;00m ctx:\n\u001b[32m    455\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     out = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m jax.errors.UnexpectedTracerError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m jax.errors.UnexpectedTracerError(unexpected_tracer_hint) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[84]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(theta, y)\u001b[39m\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNDE has to be smooth\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m args.sbi_method == \u001b[33m\"\u001b[39m\u001b[33mnle\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     64\u001b[39m     nf_log_prob = hk.without_apply_rng(\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m         hk.transform(\u001b[38;5;28;01mlambda\u001b[39;00m theta, y: \u001b[43mNDE\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m.squeeze())\n\u001b[32m     66\u001b[39m     )\n\u001b[32m     69\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mdone ✓\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/distributions/distribution.py:1287\u001b[39m, in \u001b[36mDistribution.log_prob\u001b[39m\u001b[34m(self, value, name, **kwargs)\u001b[39m\n\u001b[32m   1275\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, value, name=\u001b[33m'\u001b[39m\u001b[33mlog_prob\u001b[39m\u001b[33m'\u001b[39m, **kwargs):\n\u001b[32m   1276\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Log probability density/mass function.\u001b[39;00m\n\u001b[32m   1277\u001b[39m \n\u001b[32m   1278\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1285\u001b[39m \u001b[33;03m      values of type `self.dtype`.\u001b[39;00m\n\u001b[32m   1286\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1287\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/distributions/distribution.py:1269\u001b[39m, in \u001b[36mDistribution._call_log_prob\u001b[39m\u001b[34m(self, value, name, **kwargs)\u001b[39m\n\u001b[32m   1267\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._name_and_control_scope(name, value, kwargs):\n\u001b[32m   1268\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m_log_prob\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1269\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1270\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m_prob\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m   1271\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tf.math.log(\u001b[38;5;28mself\u001b[39m._prob(value, **kwargs))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/distributions/transformed_distribution.py:364\u001b[39m, in \u001b[36m_TransformedDistribution._log_prob\u001b[39m\u001b[34m(self, y, **kwargs)\u001b[39m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_log_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, y, **kwargs):\n\u001b[32m    363\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bijector._is_injective:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m     log_prob, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexperimental_local_measure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackward_compat\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    366\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m log_prob\n\u001b[32m    368\u001b[39m   \u001b[38;5;66;03m# TODO(b/197680518): Support base measure handling for non-injective\u001b[39;00m\n\u001b[32m    369\u001b[39m   \u001b[38;5;66;03m# bijectors.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/distributions/transformed_distribution.py:619\u001b[39m, in \u001b[36m_TransformedDistribution.experimental_local_measure\u001b[39m\u001b[34m(self, y, backward_compat, **kwargs)\u001b[39m\n\u001b[32m    617\u001b[39m local_measure_fn = \u001b[38;5;28mself\u001b[39m.distribution.experimental_local_measure\n\u001b[32m    618\u001b[39m density_corr_fn = \u001b[38;5;28mself\u001b[39m.bijector.experimental_compute_density_correction\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m base_log_prob, tangent_space = \u001b[43mlocal_measure_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackward_compat\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackward_compat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdistribution_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    621\u001b[39m correction, new_tangent_space = density_corr_fn(\n\u001b[32m    622\u001b[39m     x,\n\u001b[32m    623\u001b[39m     tangent_space,\n\u001b[32m    624\u001b[39m     backward_compat=backward_compat,\n\u001b[32m    625\u001b[39m     event_ndims=event_ndims,\n\u001b[32m    626\u001b[39m     **bijector_kwargs)\n\u001b[32m    627\u001b[39m log_prob = base_log_prob - tf.cast(correction, base_log_prob.dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/distributions/transformed_distribution.py:611\u001b[39m, in \u001b[36m_TransformedDistribution.experimental_local_measure\u001b[39m\u001b[34m(self, y, backward_compat, **kwargs)\u001b[39m\n\u001b[32m    607\u001b[39m distribution_kwargs, bijector_kwargs = \u001b[38;5;28mself\u001b[39m._kwargs_split_fn(kwargs)\n\u001b[32m    609\u001b[39m \u001b[38;5;66;03m# For caching to work, it is imperative that the bijector is the first to\u001b[39;00m\n\u001b[32m    610\u001b[39m \u001b[38;5;66;03m# modify the input.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m611\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbijector\u001b[49m\u001b[43m.\u001b[49m\u001b[43minverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbijector_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    612\u001b[39m event_ndims = \u001b[38;5;28mself\u001b[39m.bijector.inverse_event_ndims(\n\u001b[32m    613\u001b[39m     tf.nest.map_structure(ps.rank_from_shape, \u001b[38;5;28mself\u001b[39m._event_shape_tensor(),\n\u001b[32m    614\u001b[39m                           \u001b[38;5;28mself\u001b[39m.event_shape), **bijector_kwargs)\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bijector._is_injective:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/bijectors/bijector.py:1390\u001b[39m, in \u001b[36mBijector.inverse\u001b[39m\u001b[34m(self, y, name, **kwargs)\u001b[39m\n\u001b[32m   1372\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minverse\u001b[39m(\u001b[38;5;28mself\u001b[39m, y, name=\u001b[33m'\u001b[39m\u001b[33minverse\u001b[39m\u001b[33m'\u001b[39m, **kwargs):\n\u001b[32m   1373\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Returns the inverse `Bijector` evaluation, i.e., X = g^{-1}(Y).\u001b[39;00m\n\u001b[32m   1374\u001b[39m \n\u001b[32m   1375\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1388\u001b[39m \u001b[33;03m    NotImplementedError: if `_inverse` is not implemented.\u001b[39;00m\n\u001b[32m   1389\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1390\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_inverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/bijectors/bijector.py:1370\u001b[39m, in \u001b[36mBijector._call_inverse\u001b[39m\u001b[34m(self, y, name, **kwargs)\u001b[39m\n\u001b[32m   1368\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_injective:  \u001b[38;5;66;03m# No caching for non-injective\u001b[39;00m\n\u001b[32m   1369\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inverse(y, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1370\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cache\u001b[49m\u001b[43m.\u001b[49m\u001b[43minverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/internal/cache_util.py:347\u001b[39m, in \u001b[36mBijectorCache.inverse\u001b[39m\u001b[34m(self, y, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minverse\u001b[39m(\u001b[38;5;28mself\u001b[39m, y, **kwargs):\n\u001b[32m    337\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Invokes the 'inverse' transformation, or looks up previous results.\u001b[39;00m\n\u001b[32m    338\u001b[39m \n\u001b[32m    339\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    345\u001b[39m \u001b[33;03m    The output of the bijector's `_inverse` method, or a cached result.\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_lookup\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inverse_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/internal/cache_util.py:493\u001b[39m, in \u001b[36mBijectorCache._lookup\u001b[39m\u001b[34m(self, input, forward_name, inverse_name, **kwargs)\u001b[39m\n\u001b[32m    487\u001b[39m   output = output_ref()\n\u001b[32m    488\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m   \u001b[38;5;66;03m# Get the output structure, and declare a\u001b[39;00m\n\u001b[32m    490\u001b[39m   \u001b[38;5;66;03m# weakref to clear it from the cache once it gets GCed\u001b[39;00m\n\u001b[32m    491\u001b[39m   output = nest.map_structure(\n\u001b[32m    492\u001b[39m       _containerize,\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m       \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    494\u001b[39m   output_ref = WeakStructRef(\n\u001b[32m    495\u001b[39m       output,\n\u001b[32m    496\u001b[39m       subkey=(\u001b[38;5;28mself\u001b[39m.bijector, \u001b[38;5;28mself\u001b[39m.bijector_class, inverse_name, kwargs),\n\u001b[32m    497\u001b[39m       callback=\u001b[38;5;28mself\u001b[39m.storage.maybe_del)\n\u001b[32m    498\u001b[39m   \u001b[38;5;66;03m# Set the input->output mapping.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/internal/cache_util.py:532\u001b[39m, in \u001b[36mBijectorCache._invoke\u001b[39m\u001b[34m(self, input, fn_name, kwargs, attributes)\u001b[39m\n\u001b[32m    530\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_invoke\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, fn_name, kwargs, attributes):  \u001b[38;5;66;03m# pylint: disable=unused-argument\u001b[39;00m\n\u001b[32m    531\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Invokes the wrapped function. Override to customize behavior.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbijector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/bijectors/composition.py:614\u001b[39m, in \u001b[36mComposition._inverse\u001b[39m\u001b[34m(self, y, **kwargs)\u001b[39m\n\u001b[32m    610\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_injective:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m    611\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    612\u001b[39m       \u001b[33m'\u001b[39m\u001b[33mInvert is not implemented for compositions of \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    613\u001b[39m       \u001b[33m'\u001b[39m\u001b[33mnon-injective bijectors.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m614\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_walk_inverse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m.\u001b[49m\u001b[43minverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/bijectors/composition.py:352\u001b[39m, in \u001b[36mComposition._call_walk_inverse\u001b[39m\u001b[34m(self, step_fn, *args, **kwargs)\u001b[39m\n\u001b[32m    348\u001b[39m args = \u001b[38;5;28mtuple\u001b[39m(nest_util.coerce_structure(\u001b[38;5;28mself\u001b[39m.inverse_min_event_ndims, y)\n\u001b[32m    349\u001b[39m              \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m args)\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m352\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_walk_inverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[38;5;66;03m# Convert a tuple of structures to a structure of tuples. This\u001b[39;00m\n\u001b[32m    355\u001b[39m \u001b[38;5;66;03m# allows `_walk` methods to route aligned structures of inputs/outputs\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;66;03m# independently, obviates the need for conditional tuple unpacking.\u001b[39;00m\n\u001b[32m    357\u001b[39m packed_args = pack_structs_like(\u001b[38;5;28mself\u001b[39m.inverse_min_event_ndims, *args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/bijectors/chain.py:150\u001b[39m, in \u001b[36m_Chain._walk_inverse\u001b[39m\u001b[34m(self, step_fn, y, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Applies `transform_fn` to `y` sequentially over nested bijectors.\"\"\"\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m bij \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bijectors:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   y = \u001b[43mstep_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbij\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbij\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/bijectors/composition.py:615\u001b[39m, in \u001b[36mComposition._inverse.<locals>.<lambda>\u001b[39m\u001b[34m(b, y, **kwargs)\u001b[39m\n\u001b[32m    610\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_injective:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m    611\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    612\u001b[39m       \u001b[33m'\u001b[39m\u001b[33mInvert is not implemented for compositions of \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    613\u001b[39m       \u001b[33m'\u001b[39m\u001b[33mnon-injective bijectors.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    614\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_walk_inverse(\n\u001b[32m--> \u001b[39m\u001b[32m615\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m b, y, **kwargs: \u001b[43mb\u001b[49m\u001b[43m.\u001b[49m\u001b[43minverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    616\u001b[39m     y, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/bijectors/bijector.py:1390\u001b[39m, in \u001b[36mBijector.inverse\u001b[39m\u001b[34m(self, y, name, **kwargs)\u001b[39m\n\u001b[32m   1372\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minverse\u001b[39m(\u001b[38;5;28mself\u001b[39m, y, name=\u001b[33m'\u001b[39m\u001b[33minverse\u001b[39m\u001b[33m'\u001b[39m, **kwargs):\n\u001b[32m   1373\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Returns the inverse `Bijector` evaluation, i.e., X = g^{-1}(Y).\u001b[39;00m\n\u001b[32m   1374\u001b[39m \n\u001b[32m   1375\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1388\u001b[39m \u001b[33;03m    NotImplementedError: if `_inverse` is not implemented.\u001b[39;00m\n\u001b[32m   1389\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1390\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_inverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/bijectors/bijector.py:1370\u001b[39m, in \u001b[36mBijector._call_inverse\u001b[39m\u001b[34m(self, y, name, **kwargs)\u001b[39m\n\u001b[32m   1368\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_injective:  \u001b[38;5;66;03m# No caching for non-injective\u001b[39;00m\n\u001b[32m   1369\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inverse(y, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1370\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cache\u001b[49m\u001b[43m.\u001b[49m\u001b[43minverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/internal/cache_util.py:347\u001b[39m, in \u001b[36mBijectorCache.inverse\u001b[39m\u001b[34m(self, y, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minverse\u001b[39m(\u001b[38;5;28mself\u001b[39m, y, **kwargs):\n\u001b[32m    337\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Invokes the 'inverse' transformation, or looks up previous results.\u001b[39;00m\n\u001b[32m    338\u001b[39m \n\u001b[32m    339\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    345\u001b[39m \u001b[33;03m    The output of the bijector's `_inverse` method, or a cached result.\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_lookup\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inverse_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/internal/cache_util.py:493\u001b[39m, in \u001b[36mBijectorCache._lookup\u001b[39m\u001b[34m(self, input, forward_name, inverse_name, **kwargs)\u001b[39m\n\u001b[32m    487\u001b[39m   output = output_ref()\n\u001b[32m    488\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m   \u001b[38;5;66;03m# Get the output structure, and declare a\u001b[39;00m\n\u001b[32m    490\u001b[39m   \u001b[38;5;66;03m# weakref to clear it from the cache once it gets GCed\u001b[39;00m\n\u001b[32m    491\u001b[39m   output = nest.map_structure(\n\u001b[32m    492\u001b[39m       _containerize,\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m       \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    494\u001b[39m   output_ref = WeakStructRef(\n\u001b[32m    495\u001b[39m       output,\n\u001b[32m    496\u001b[39m       subkey=(\u001b[38;5;28mself\u001b[39m.bijector, \u001b[38;5;28mself\u001b[39m.bijector_class, inverse_name, kwargs),\n\u001b[32m    497\u001b[39m       callback=\u001b[38;5;28mself\u001b[39m.storage.maybe_del)\n\u001b[32m    498\u001b[39m   \u001b[38;5;66;03m# Set the input->output mapping.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/internal/cache_util.py:532\u001b[39m, in \u001b[36mBijectorCache._invoke\u001b[39m\u001b[34m(self, input, fn_name, kwargs, attributes)\u001b[39m\n\u001b[32m    530\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_invoke\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, fn_name, kwargs, attributes):  \u001b[38;5;66;03m# pylint: disable=unused-argument\u001b[39;00m\n\u001b[32m    531\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Invokes the wrapped function. Override to customize behavior.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbijector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/bijectors/composition.py:614\u001b[39m, in \u001b[36mComposition._inverse\u001b[39m\u001b[34m(self, y, **kwargs)\u001b[39m\n\u001b[32m    610\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_injective:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m    611\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    612\u001b[39m       \u001b[33m'\u001b[39m\u001b[33mInvert is not implemented for compositions of \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    613\u001b[39m       \u001b[33m'\u001b[39m\u001b[33mnon-injective bijectors.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m614\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_walk_inverse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m.\u001b[49m\u001b[43minverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/bijectors/composition.py:352\u001b[39m, in \u001b[36mComposition._call_walk_inverse\u001b[39m\u001b[34m(self, step_fn, *args, **kwargs)\u001b[39m\n\u001b[32m    348\u001b[39m args = \u001b[38;5;28mtuple\u001b[39m(nest_util.coerce_structure(\u001b[38;5;28mself\u001b[39m.inverse_min_event_ndims, y)\n\u001b[32m    349\u001b[39m              \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m args)\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m352\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_walk_inverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[38;5;66;03m# Convert a tuple of structures to a structure of tuples. This\u001b[39;00m\n\u001b[32m    355\u001b[39m \u001b[38;5;66;03m# allows `_walk` methods to route aligned structures of inputs/outputs\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;66;03m# independently, obviates the need for conditional tuple unpacking.\u001b[39;00m\n\u001b[32m    357\u001b[39m packed_args = pack_structs_like(\u001b[38;5;28mself\u001b[39m.inverse_min_event_ndims, *args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/bijectors/chain.py:150\u001b[39m, in \u001b[36m_Chain._walk_inverse\u001b[39m\u001b[34m(self, step_fn, y, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Applies `transform_fn` to `y` sequentially over nested bijectors.\"\"\"\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m bij \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bijectors:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   y = \u001b[43mstep_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbij\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbij\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/bijectors/composition.py:615\u001b[39m, in \u001b[36mComposition._inverse.<locals>.<lambda>\u001b[39m\u001b[34m(b, y, **kwargs)\u001b[39m\n\u001b[32m    610\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_injective:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m    611\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    612\u001b[39m       \u001b[33m'\u001b[39m\u001b[33mInvert is not implemented for compositions of \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    613\u001b[39m       \u001b[33m'\u001b[39m\u001b[33mnon-injective bijectors.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    614\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_walk_inverse(\n\u001b[32m--> \u001b[39m\u001b[32m615\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m b, y, **kwargs: \u001b[43mb\u001b[49m\u001b[43m.\u001b[49m\u001b[43minverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    616\u001b[39m     y, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/bijectors/bijector.py:1390\u001b[39m, in \u001b[36mBijector.inverse\u001b[39m\u001b[34m(self, y, name, **kwargs)\u001b[39m\n\u001b[32m   1372\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minverse\u001b[39m(\u001b[38;5;28mself\u001b[39m, y, name=\u001b[33m'\u001b[39m\u001b[33minverse\u001b[39m\u001b[33m'\u001b[39m, **kwargs):\n\u001b[32m   1373\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Returns the inverse `Bijector` evaluation, i.e., X = g^{-1}(Y).\u001b[39;00m\n\u001b[32m   1374\u001b[39m \n\u001b[32m   1375\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1388\u001b[39m \u001b[33;03m    NotImplementedError: if `_inverse` is not implemented.\u001b[39;00m\n\u001b[32m   1389\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1390\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_inverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/bijectors/bijector.py:1370\u001b[39m, in \u001b[36mBijector._call_inverse\u001b[39m\u001b[34m(self, y, name, **kwargs)\u001b[39m\n\u001b[32m   1368\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_injective:  \u001b[38;5;66;03m# No caching for non-injective\u001b[39;00m\n\u001b[32m   1369\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inverse(y, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1370\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cache\u001b[49m\u001b[43m.\u001b[49m\u001b[43minverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/internal/cache_util.py:347\u001b[39m, in \u001b[36mBijectorCache.inverse\u001b[39m\u001b[34m(self, y, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minverse\u001b[39m(\u001b[38;5;28mself\u001b[39m, y, **kwargs):\n\u001b[32m    337\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Invokes the 'inverse' transformation, or looks up previous results.\u001b[39;00m\n\u001b[32m    338\u001b[39m \n\u001b[32m    339\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    345\u001b[39m \u001b[33;03m    The output of the bijector's `_inverse` method, or a cached result.\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_lookup\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inverse_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/internal/cache_util.py:493\u001b[39m, in \u001b[36mBijectorCache._lookup\u001b[39m\u001b[34m(self, input, forward_name, inverse_name, **kwargs)\u001b[39m\n\u001b[32m    487\u001b[39m   output = output_ref()\n\u001b[32m    488\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m   \u001b[38;5;66;03m# Get the output structure, and declare a\u001b[39;00m\n\u001b[32m    490\u001b[39m   \u001b[38;5;66;03m# weakref to clear it from the cache once it gets GCed\u001b[39;00m\n\u001b[32m    491\u001b[39m   output = nest.map_structure(\n\u001b[32m    492\u001b[39m       _containerize,\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m       \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    494\u001b[39m   output_ref = WeakStructRef(\n\u001b[32m    495\u001b[39m       output,\n\u001b[32m    496\u001b[39m       subkey=(\u001b[38;5;28mself\u001b[39m.bijector, \u001b[38;5;28mself\u001b[39m.bijector_class, inverse_name, kwargs),\n\u001b[32m    497\u001b[39m       callback=\u001b[38;5;28mself\u001b[39m.storage.maybe_del)\n\u001b[32m    498\u001b[39m   \u001b[38;5;66;03m# Set the input->output mapping.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/internal/cache_util.py:532\u001b[39m, in \u001b[36mBijectorCache._invoke\u001b[39m\u001b[34m(self, input, fn_name, kwargs, attributes)\u001b[39m\n\u001b[32m    530\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_invoke\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, fn_name, kwargs, attributes):  \u001b[38;5;66;03m# pylint: disable=unused-argument\u001b[39;00m\n\u001b[32m    531\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Invokes the wrapped function. Override to customize behavior.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbijector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tensorflow_probability/substrates/jax/bijectors/real_nvp.py:298\u001b[39m, in \u001b[36mRealNVP._inverse\u001b[39m\u001b[34m(self, y, **condition_kwargs)\u001b[39m\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._reverse_mask:\n\u001b[32m    296\u001b[39m   y0, y1 = y1, y0\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m x1 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bijector_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bijector_input_units\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m                       \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcondition_kwargs\u001b[49m\u001b[43m)\u001b[49m.inverse(y1)\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._reverse_mask:\n\u001b[32m    302\u001b[39m   x1, y0 = y0, x1\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/haiku/_src/module.py:464\u001b[39m, in \u001b[36mwrap_method.<locals>.wrapped\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    461\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m method_name != \u001b[33m\"\u001b[39m\u001b[33m__call__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    462\u001b[39m     f = jax.named_call(f, name=method_name)\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m out = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[38;5;66;03m# Module names are set in the constructor. If `f` is the constructor then\u001b[39;00m\n\u001b[32m    467\u001b[39m \u001b[38;5;66;03m# its name will only be set **after** `f` has run. For methods other\u001b[39;00m\n\u001b[32m    468\u001b[39m \u001b[38;5;66;03m# than `__init__` we need the name before running in order to wrap their\u001b[39;00m\n\u001b[32m    469\u001b[39m \u001b[38;5;66;03m# execution with `named_call`.\u001b[39;00m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/contextlib.py:81\u001b[39m, in \u001b[36mContextDecorator.__call__.<locals>.inner\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args, **kwds):\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._recreate_cm():\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/haiku/_src/module.py:305\u001b[39m, in \u001b[36mrun_interceptors\u001b[39m\u001b[34m(bound_method, method_name, self, orig_class, *args, **kwargs)\u001b[39m\n\u001b[32m    303\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Runs any method interceptors or the original method.\"\"\"\u001b[39;00m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m interceptor_stack:\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m ctx = MethodContext(module=\u001b[38;5;28mself\u001b[39m,\n\u001b[32m    308\u001b[39m                     method_name=method_name,\n\u001b[32m    309\u001b[39m                     orig_method=bound_method,\n\u001b[32m    310\u001b[39m                     orig_class=orig_class)\n\u001b[32m    311\u001b[39m interceptor_stack_copy = interceptor_stack.clone()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 177\u001b[39m, in \u001b[36mAffineSigmoidCoupling.__call__\u001b[39m\u001b[34m(self, x, output_units, **condition_kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, output_units, **condition_kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     net = \u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43my\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, layer_size \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.layers):\n\u001b[32m    179\u001b[39m         net = \u001b[38;5;28mself\u001b[39m.activation(hk.Linear(layer_size, name=\u001b[33m\"\u001b[39m\u001b[33mlayer\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m\"\u001b[39m % i)(net))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py:4648\u001b[39m, in \u001b[36mconcatenate\u001b[39m\u001b[34m(arrays, axis, dtype)\u001b[39m\n\u001b[32m   4646\u001b[39m k = \u001b[32m16\u001b[39m\n\u001b[32m   4647\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(arrays_out) > \u001b[32m1\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m4648\u001b[39m   arrays_out = [\u001b[43mlax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays_out\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m+\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4649\u001b[39m                 \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(arrays_out), k)]\n\u001b[32m   4650\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_out[\u001b[32m0\u001b[39m]\n",
      "    \u001b[31m[... skipping hidden 28 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/jax/_src/lax/lax.py:6644\u001b[39m, in \u001b[36m_concatenate_shape_rule\u001b[39m\u001b[34m(*operands, **kwargs)\u001b[39m\n\u001b[32m   6642\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m({operand.ndim \u001b[38;5;28;01mfor\u001b[39;00m operand \u001b[38;5;129;01min\u001b[39;00m operands}) != \u001b[32m1\u001b[39m:\n\u001b[32m   6643\u001b[39m   msg = \u001b[33m\"\u001b[39m\u001b[33mCannot concatenate arrays with different numbers of dimensions: got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m6644\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg.format(\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mstr\u001b[39m(o.shape) \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m operands)))\n\u001b[32m   6645\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[32m0\u001b[39m <= dimension < operands[\u001b[32m0\u001b[39m].ndim:\n\u001b[32m   6646\u001b[39m   msg = \u001b[33m\"\u001b[39m\u001b[33mconcatenate dimension out of bounds: dimension \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m for shapes \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: Cannot concatenate arrays with different numbers of dimensions: got (64, 2), (64,)."
     ]
    }
   ],
   "source": [
    "inference.train(\n",
    "    data_path=\"./data/data_NFW/\",\n",
    "    total_steps=args.total_steps,\n",
    "    batch_size=args.bacth_size,\n",
    "    score_weight=args.score_weight,\n",
    "    learning_rate=lr_scheduler,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13f6fed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbi_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
