{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "beeabd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"9\"\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "import numpyro \n",
    "import numpyro.distributions as dist\n",
    "\n",
    "from numpyro.handlers import condition, reparam, seed, trace\n",
    "from numpyro.infer.reparam import LocScaleReparam, TransformReparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cdee9bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import Optional, Tuple, Callable, Union, List\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import vmap, jit, pmap\n",
    "from jax import random\n",
    "import optax\n",
    "\n",
    "# jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import numpy as np\n",
    "from astropy import units as u\n",
    "from astropy import constants as c\n",
    "\n",
    "\n",
    "import odisseo\n",
    "from odisseo import construct_initial_state\n",
    "from odisseo.integrators import leapfrog\n",
    "from odisseo.dynamics import direct_acc, DIRECT_ACC, DIRECT_ACC_LAXMAP, DIRECT_ACC_FOR_LOOP, DIRECT_ACC_MATRIX\n",
    "from odisseo.option_classes import SimulationConfig, SimulationParams, MNParams, NFWParams, PlummerParams, MN_POTENTIAL, NFW_POTENTIAL\n",
    "from odisseo.initial_condition import Plummer_sphere, ic_two_body, sample_position_on_sphere, inclined_circular_velocity, sample_position_on_circle, inclined_position\n",
    "from odisseo.utils import center_of_mass\n",
    "from odisseo.time_integration import time_integration\n",
    "from odisseo.units import CodeUnits\n",
    "from odisseo.visualization import create_3d_gif, create_projection_gif, energy_angular_momentum_plot\n",
    "from odisseo.potentials import MyamotoNagai, NFW\n",
    "from odisseo.option_classes import DIFFRAX_BACKEND, DOPRI5, TSIT5, SEMIIMPLICITEULER, LEAPFROGMIDPOINT, REVERSIBLEHEUN\n",
    "\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.size': 20,\n",
    "    'axes.labelsize': 20,\n",
    "    'xtick.labelsize': 13,\n",
    "    'ytick.labelsize': 13,\n",
    "    'legend.fontsize': 15,\n",
    "})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b31173d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 15:29:02.157045: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745933342.175765 2723962 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745933342.181166 2723962 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745933342.195903 2723962 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745933342.195930 2723962 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745933342.195932 2723962 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745933342.195933 2723962 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import haiku as hk\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "from jaxopt import Bisection\n",
    "from jaxopt.linear_solve import solve_normal_cg\n",
    "\n",
    "# tfp = tfp.substrates.jax\n",
    "tfb = tfp.bijectors\n",
    "tfd = tfp.distributions\n",
    "\n",
    "\n",
    "# This module is to store our implicit inverse functions\n",
    "@partial(jax.custom_vjp, nondiff_argnums=(0,))\n",
    "def root_bisection(f, params):\n",
    "    \"\"\"\n",
    "    f: optimality fn with input arg (params, x)\n",
    "    \"\"\"\n",
    "    bisec = Bisection(\n",
    "        optimality_fun=f,\n",
    "        lower=0.0,\n",
    "        upper=1.0,\n",
    "        check_bracket=False,\n",
    "        maxiter=100,\n",
    "        tol=1e-06,\n",
    "    )\n",
    "    return bisec.run(None, params).params\n",
    "\n",
    "\n",
    "def root_bisection_fwd(f, params):\n",
    "    z_star = root_bisection(f, params)\n",
    "    return z_star, (params, z_star)\n",
    "\n",
    "\n",
    "def root_bwd(f, res, z_star_bar):\n",
    "    params, z_star = res\n",
    "    _, vjp_a = jax.vjp(lambda p: f(z_star, p), params)\n",
    "    _, vjp_z = jax.vjp(lambda z: f(z, params), z_star)\n",
    "    return vjp_a(solve_normal_cg(lambda u: vjp_z(u)[0], -z_star_bar))\n",
    "\n",
    "\n",
    "root_bisection.defvjp(root_bisection_fwd, root_bwd)\n",
    "\n",
    "\n",
    "def make_inverse_fn(f):\n",
    "    \"\"\"Defines the inverse of the input function, and provides implicit gradients\n",
    "    of the inverse.\n",
    "\n",
    "    Args:\n",
    "      f: callable of input shape (params, x)\n",
    "    Retuns:\n",
    "      inv_f: callable of with args (params, y)\n",
    "    \"\"\"\n",
    "\n",
    "    def inv_fn(params, y):\n",
    "        def optimality_fn(x, params):\n",
    "            p, y = params\n",
    "            return f(p, x) - y\n",
    "\n",
    "        return root_bisection(optimality_fn, [params, y])\n",
    "\n",
    "    return inv_fn\n",
    "\n",
    "# Bijiector functions\n",
    "class MixtureAffineSigmoidBijector(tfp.bijectors.Bijector):\n",
    "    \"\"\"\n",
    "    Bijector based on a ramp function, and implemented using an implicit\n",
    "    layer.\n",
    "    This implementation is based on the Smooth Normalizing Flows described\n",
    "    in: https://arxiv.org/abs/2110.00351\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, a, b, c, p, name=\"MixtureAffineSigmoidBijector\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          rho: function of x that defines a ramp function between 0 and 1\n",
    "          a,b,c: scalar parameters of the coupling layer.\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__(forward_min_event_ndims=0, name=name)\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "        self.p = p\n",
    "\n",
    "        def sigmoid(x, a, b, c):\n",
    "            z = (jax.scipy.special.logit(x) + b) * a\n",
    "            y = jax.nn.sigmoid(z) * (1 - c) + c * x\n",
    "            return y\n",
    "\n",
    "        # Rescaled bijection\n",
    "        def f(params, x):\n",
    "            a, b, c, p = params\n",
    "            a_in, b_in = [0.0 - 1e-1, 1.0 + 1e-1]\n",
    "\n",
    "            x = (x - a_in) / (b_in - a_in)\n",
    "            x0 = (jnp.zeros_like(x) - a_in) / (b_in - a_in)\n",
    "            x1 = (jnp.ones_like(x) - a_in) / (b_in - a_in)\n",
    "\n",
    "            y = sigmoid(x, a, b, c)\n",
    "            y0 = sigmoid(x0, a, b, c)\n",
    "            y1 = sigmoid(x1, a, b, c)\n",
    "\n",
    "            y = (y - y0) / (y1 - y0)\n",
    "            return jnp.sum(p * (y * (1 - c) + c * x), axis=0)\n",
    "\n",
    "        self.f = f\n",
    "\n",
    "        # Inverse bijector\n",
    "        self.inv_f = make_inverse_fn(f)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        return jax.vmap(jax.vmap(self.f))([self.a, self.b, self.c, self.p], x)\n",
    "\n",
    "    def _inverse(self, y):\n",
    "        return jax.vmap(jax.vmap(self.inv_f))([self.a, self.b, self.c, self.p], y)\n",
    "\n",
    "    def _forward_log_det_jacobian(self, x):\n",
    "        def logdet_fn(x, a, b, c, p):\n",
    "            g = jax.grad(self.f, argnums=1)([a, b, c, p], x)\n",
    "            s, logdet = jnp.linalg.slogdet(jnp.atleast_2d(g))\n",
    "            return s * logdet\n",
    "\n",
    "        return jax.vmap(jax.vmap(logdet_fn))(x, self.a, self.b, self.c, self.p)\n",
    "\n",
    "\n",
    "#Coupling layers\n",
    "class AffineCoupling(hk.Module):\n",
    "    def __init__(\n",
    "        self, y, *args, layers=[128, 128], activation=jax.nn.leaky_relu, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        y, conditioning variable\n",
    "        layers, list of hidden layers\n",
    "        activation, activation function for hidden layers\n",
    "        \"\"\"\n",
    "        self.y = y\n",
    "        self.layers = layers\n",
    "        self.activation = activation\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __call__(self, x, output_units, **condition_kwargs):\n",
    "        net = jnp.concatenate([x, self.y], axis=-1)\n",
    "        for i, layer_size in enumerate(self.layers):\n",
    "            net = self.activation(hk.Linear(layer_size, name=\"layer%d\" % i)(net))\n",
    "\n",
    "        shifter = tfb.Shift(hk.Linear(output_units)(net))\n",
    "        scaler = tfb.Scale(jnp.clip(jnp.exp(hk.Linear(output_units)(net)), 1e-2, 1e2))\n",
    "        return tfb.Chain([shifter, scaler])\n",
    "\n",
    "\n",
    "class AffineSigmoidCoupling(hk.Module):\n",
    "    \"\"\"This is the coupling layer used in the Flow.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        y,\n",
    "        *args,\n",
    "        layers=[128, 128],\n",
    "        n_components=32,\n",
    "        activation=jax.nn.silu,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        y, conditioning variable\n",
    "        layers, list of hidden layers\n",
    "        n_components, number of mixture components\n",
    "        activation, activation function for hidden layers\n",
    "        \"\"\"\n",
    "        self.y = y\n",
    "        self.layers = layers\n",
    "        self.n_components = n_components\n",
    "        self.activation = activation\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __call__(self, x, output_units, **condition_kwargs):\n",
    "        net = jnp.concatenate([x, self.y], axis=-1)\n",
    "        for i, layer_size in enumerate(self.layers):\n",
    "            net = self.activation(hk.Linear(layer_size, name=\"layer%d\" % i)(net))\n",
    "\n",
    "        log_a_bound = 4\n",
    "        min_density_lower_bound = 1e-4\n",
    "        n_components = self.n_components\n",
    "\n",
    "        log_a = (\n",
    "            jax.nn.tanh(hk.Linear(output_units * n_components, name=\"l3\")(net))\n",
    "            * log_a_bound\n",
    "        )\n",
    "        b = hk.Linear(output_units * n_components, name=\"l4\")(net)\n",
    "        c = min_density_lower_bound + jax.nn.sigmoid(\n",
    "            hk.Linear(output_units * n_components, name=\"l5\")(net)\n",
    "        ) * (1 - min_density_lower_bound)\n",
    "        p = hk.Linear(output_units * n_components, name=\"l6\")(net)\n",
    "\n",
    "        log_a = log_a.reshape(-1, output_units, n_components)\n",
    "        b = b.reshape(-1, output_units, n_components)\n",
    "        c = c.reshape(-1, output_units, n_components)\n",
    "        p = p.reshape(-1, output_units, n_components)\n",
    "        p = jax.nn.softmax(p)\n",
    "\n",
    "        return MixtureAffineSigmoidBijector(jnp.exp(log_a), b, c, p)\n",
    "\n",
    "# Normalizing flow model \n",
    "class ConditionalRealNVP(hk.Module):\n",
    "    \"\"\"A normalizing flow based on RealNVP using specified bijector functions.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, d, *args, n_layers=3, bijector_fn=AffineSigmoidCoupling, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        d, dimensionality of the input\n",
    "        n_layers, number of layers\n",
    "        coupling_layer, list of coupling layers\n",
    "        \"\"\"\n",
    "        self.d = d\n",
    "        self.n_layer = n_layers\n",
    "        self.bijector_fn = bijector_fn\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __call__(self, y):\n",
    "        chain = tfb.Chain(\n",
    "            [\n",
    "                tfb.Permute(jnp.arange(self.d)[::-1])(\n",
    "                    tfb.RealNVP(\n",
    "                        self.d // 2, bijector_fn=self.bijector_fn(y, name=\"b%d\" % i)\n",
    "                    )\n",
    "                )\n",
    "                for i in range(self.n_layer)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        nvp = tfd.TransformedDistribution(\n",
    "            tfd.MultivariateNormalDiag(0.5 * jnp.ones(self.d), 0.05 * jnp.ones(self.d)),\n",
    "            bijector=chain,\n",
    "        )\n",
    "\n",
    "        return nvp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c8a0b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "#mimic the argument parser used in the sbi_bm_lens\n",
    "\n",
    "class args_namedtuple(NamedTuple):\n",
    "\n",
    "    #seed\n",
    "    seed = 42\n",
    "\n",
    "    #activation function\n",
    "    activ_fun = \"silu\"\n",
    "\n",
    "    #Normalizing flow\n",
    "    nf = \"smooth\"\n",
    "\n",
    "    #score type\n",
    "    score_type = \"conditional\"\n",
    "\n",
    "    #sbi method\n",
    "    sbi_method = \"nle\"\n",
    "\n",
    "    #number of bijiector layers\n",
    "    n_bijector_layers = 5\n",
    "\n",
    "    #number of coupling layers\n",
    "    n_flow_layers = 3\n",
    "\n",
    "    #number of parameters\n",
    "    n_params = 5 #[\"t_end\", \"Mtot_plummer\", \"a_plummer\", \"M_NFW\", \"r_s\"]\n",
    "\n",
    "    #number of step\n",
    "    total_steps = 10_000\n",
    "\n",
    "\n",
    "args = args_namedtuple()\n",
    "\n",
    "master_seed = jax.random.PRNGKey(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d394311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done ✓\n"
     ]
    }
   ],
   "source": [
    "#### CREATE THE NDE #####\n",
    "\n",
    "#activation  function\n",
    "if args.activ_fun == \"silu\":\n",
    "    activ_fun = jax.nn.silu\n",
    "elif args.activ_fun == \"sin\":\n",
    "    activ_func = jnp.sin\n",
    "\n",
    "#score type\n",
    "if args.sbi_method == \"nle\":\n",
    "    score_type = \"conditional\"  #need to be pass to the model object (stochastic simulator)\n",
    "\n",
    "#NF type, either smooth or affine\n",
    "if args.nf == \"smooth\":\n",
    "    if args.sbi_method == \"npe\":\n",
    "        pass\n",
    "    elif args.sbi_method == \"nle\":\n",
    "        # how these qantities are comute (with dataset form prior)\n",
    "        # scale_y = jnp.std(dataset_y, axis=0) / 0.07\n",
    "        # shift_y = jnp.mean(dataset_y / scale_y, axis=0) - 0.5\n",
    "\n",
    "        #just place holder, need to be calculated from the dataset for normalization \n",
    "        scale = jnp.ones(args.n_params)\n",
    "        shift = jnp.ones(args.n_params)\n",
    "\n",
    "        #NF ARCHITECTURE\n",
    "        #128 neuron per layer\n",
    "        bijector_layers = [128] * args.n_bijector_layers\n",
    "        \n",
    "        bijector = partial(\n",
    "            AffineSigmoidCoupling,\n",
    "            layers=bijector_layers,\n",
    "            activation=activ_fun,\n",
    "            n_components=16,\n",
    "        )\n",
    "        \n",
    "        NF = partial(ConditionalRealNVP, \n",
    "                     n_layers=args.n_flow_layers, \n",
    "                     bijector_fn=bijector)\n",
    "\n",
    "        class NDE(hk.Module):\n",
    "            def __call__(self, y):\n",
    "                nvp = NF(args.n_params)(y)\n",
    "                return tfd.TransformedDistribution(\n",
    "                    nvp, tfb.Chain([tfb.Scale(scale), tfb.Shift(shift)])\n",
    "                )\n",
    "        \n",
    "elif args.nf == \"affine\":\n",
    "    bijector_layers = [128] * args.n_bijector_layers\n",
    "\n",
    "    bijector = partial(AffineCoupling, layers=bijector_layers, activation=activ_fun)\n",
    "\n",
    "    NF = partial(ConditionalRealNVP, n_layers=args.n_flow_layers, bijector_fn=bijector)\n",
    "\n",
    "    class NDE(hk.Module):\n",
    "        def __call__(self, y):\n",
    "            return NF(args.n_params)(y)\n",
    "        \n",
    "#FOR NPE NEED TO HAVE SMOOTH NORMALIZING FLOW\n",
    "if args.nf == \"affine\" and args.sbi_method == \"npe\" and args.score_weight > 0:\n",
    "    raise ValueError(\"NDE has to be smooth\")\n",
    "\n",
    "elif args.sbi_method == \"nle\":\n",
    "    nf_log_prob = hk.without_apply_rng(\n",
    "        hk.transform(lambda theta, y: NDE()(theta).log_prob(y).squeeze())\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"done ✓\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e42f54e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This should be used to create the compress dataset, in this simplified case\n",
    "#we are using it just to sample the prior, to be implemented the compression\n",
    "\n",
    "class JaxCompressedSimulator:\n",
    "    def __init__(self, ):\n",
    "\n",
    "        # self.compressor = compressor #not used\n",
    "        # self.params_compressor = params_compressor #not used\n",
    "        # self.opt_state = opt_state #not used\n",
    "\n",
    "        self.t_end = dist.Uniform(0.5, 10.0)\n",
    "        self.Mtot_plummer = dist.Uniform(1e3, 1e5)\n",
    "        self.a_plummer = dist.Uniform(0.1, 2.0)\n",
    "        self.M_NFW = dist.Uniform(5e11, 1.5e12)\n",
    "        self.r_s = dist.Uniform(1.0, 20.0)\n",
    "\n",
    "        self.stack = [\n",
    "            self.t_end,\n",
    "            self.Mtot_plummer,\n",
    "            self.a_plummer,\n",
    "            self.M_NFW,\n",
    "            self.r_s,\n",
    "        ]\n",
    "\n",
    "    def prior_sample(self, sample_shape, master_key):\n",
    "        samples = []\n",
    "\n",
    "        keys = jax.random.split(master_key, 6)\n",
    "\n",
    "        for i, distribution in enumerate(self.stack):\n",
    "            samples.append(distribution.sample(keys[i], sample_shape))\n",
    "\n",
    "        return jnp.stack(samples).T\n",
    "\n",
    "    def prior_log_prob(self, values):\n",
    "        logp = 0\n",
    "\n",
    "        for i, distribution in enumerate(self.stack):\n",
    "            logp += distribution.log_prob(values[..., i])\n",
    "\n",
    "        return logp\n",
    "\n",
    "compressed_simulator = JaxCompressedSimulator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "edb1df00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... inference\n"
     ]
    }
   ],
   "source": [
    "######## INFERENCE ########\n",
    "print(\"... inference\")\n",
    "\n",
    "params_init = nf_log_prob.init(\n",
    "    master_seed, 0.5 * jnp.ones([1, args.n_params]), 0.5 * jnp.ones([1, args.n_params])\n",
    ")\n",
    "\n",
    "prior_mean = jnp.mean(compressed_simulator.prior_sample((1000,), master_seed), axis=0)\n",
    "\n",
    "nb_steps = args.total_steps - args.total_steps * 0.2\n",
    "\n",
    "lr_scheduler = optax.exponential_decay(\n",
    "    init_value=0.001,\n",
    "    transition_steps=nb_steps // 50,\n",
    "    decay_rate=0.9,\n",
    "    end_value=1e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390ead59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLE:\n",
    "    def __init__(self, NDE, init_params_nde, args):\n",
    "        self.NDE = NDE\n",
    "        self.params = init_params_nde\n",
    "        self.dim = args.n_params\n",
    "\n",
    "    def log_prob_fn(self, params, theta, y):\n",
    "        return self.NDE.apply(params, theta, y)\n",
    "\n",
    "    def loss_nll_and_score(self, params, mu, batch, score, weight_score):\n",
    "        lp, out = jax.vmap(\n",
    "            jax.value_and_grad(\n",
    "                lambda theta, x: self.log_prob_fn(\n",
    "                    params, theta.reshape([1, self.dim]), x.reshape([1, self.dim])\n",
    "                ).squeeze()\n",
    "            )\n",
    "        )(mu, batch)\n",
    "\n",
    "        return (\n",
    "            -jnp.mean(lp) + weight_score * jnp.sum((out - score) ** 2, axis=-1).mean()\n",
    "        )\n",
    "\n",
    "    def loss_nll(self, params, mu, batch, score, weight_score):\n",
    "        lp = self.log_prob_fn(params, mu, batch)\n",
    "\n",
    "        return -jnp.mean(lp)\n",
    "\n",
    "    def train(\n",
    "        self, data, learning_rate, total_steps=30_000, batch_size=128, score_weight=0\n",
    "    ):\n",
    "        dataset_theta = data[\"theta\"]\n",
    "        dataset_y = data[\"y\"]\n",
    "        if score_weight != 0:\n",
    "            dataset_score = data[\"score\"]\n",
    "            loss_fn = self.loss_nll_and_score\n",
    "        else:\n",
    "            loss_fn = self.loss_nll\n",
    "\n",
    "        nb_simu = len(dataset_theta)\n",
    "\n",
    "        print(\"nb of simulations used for training: \", nb_simu)\n",
    "\n",
    "        params = self.params\n",
    "        optimizer = optax.adam(learning_rate)\n",
    "        opt_state = optimizer.init(params)\n",
    "\n",
    "        @jax.jit\n",
    "        def update(params, opt_state, mu, batch, score, weight_score):\n",
    "            \"\"\"Single SGD update step.\"\"\"\n",
    "            loss, grads = jax.value_and_grad(loss_fn)(\n",
    "                params, mu, batch, score, weight_score\n",
    "            )\n",
    "            updates, new_opt_state = optimizer.update(grads, opt_state, params)\n",
    "            new_params = optax.apply_updates(params, updates)\n",
    "\n",
    "            return loss, new_params, new_opt_state\n",
    "\n",
    "        print(\"... start training\")\n",
    "\n",
    "        batch_loss = []\n",
    "        lr_scheduler_store = []\n",
    "        pbar = tqdm(range(total_steps))\n",
    "\n",
    "        for batch in pbar:\n",
    "            inds = np.random.randint(0, nb_simu, batch_size)\n",
    "            ex_theta = dataset_theta[inds]\n",
    "            ex_y = dataset_y[inds]\n",
    "            if score_weight != 0:\n",
    "                ex_score = dataset_score[inds]\n",
    "            else:\n",
    "                ex_score = None\n",
    "\n",
    "            if not jnp.isnan(ex_y).any():\n",
    "                l, params, opt_state = update(\n",
    "                    params, opt_state, ex_theta, ex_y, ex_score, score_weight\n",
    "                )\n",
    "\n",
    "                batch_loss.append(l)\n",
    "                pbar.set_description(f\"loss {l:.3f}\")\n",
    "\n",
    "                if jnp.isnan(l):\n",
    "                    break\n",
    "\n",
    "        self.params = params\n",
    "        self.loss = batch_loss\n",
    "\n",
    "        print(\"done ✓\")\n",
    "\n",
    "    def sample(\n",
    "        self,\n",
    "        log_prob_prior,\n",
    "        observation,\n",
    "        init_point,\n",
    "        key,\n",
    "        num_results=3e4,\n",
    "        num_burnin_steps=5e2,\n",
    "        num_chains=12,\n",
    "    ):\n",
    "        print(\"... running hmc\")\n",
    "\n",
    "        @jax.vmap\n",
    "        def unnormalized_log_prob(theta):\n",
    "            prior = log_prob_prior(theta)\n",
    "\n",
    "            likelihood = self.log_prob_fn(\n",
    "                self.params,\n",
    "                theta.reshape([1, self.dim]),\n",
    "                jnp.array(observation).reshape([1, self.dim]),\n",
    "            )\n",
    "\n",
    "            return likelihood + prior\n",
    "\n",
    "        # Initialize the HMC transition kernel.\n",
    "        adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(\n",
    "            tfp.mcmc.HamiltonianMonteCarlo(\n",
    "                target_log_prob_fn=unnormalized_log_prob,\n",
    "                num_leapfrog_steps=3,\n",
    "                step_size=1e-2,\n",
    "            ),\n",
    "            num_adaptation_steps=int(num_burnin_steps * 0.8),\n",
    "        )\n",
    "\n",
    "        # Run the chain (with burn-in).\n",
    "        # @jax.jit\n",
    "        def run_chain():\n",
    "            # Run the chain (with burn-in).\n",
    "            samples, is_accepted = tfp.mcmc.sample_chain(\n",
    "                num_results=num_results,\n",
    "                num_burnin_steps=num_burnin_steps,\n",
    "                current_state=jnp.array(init_point) * jnp.ones([num_chains, self.dim]),\n",
    "                kernel=adaptive_hmc,\n",
    "                trace_fn=lambda _, pkr: pkr.inner_results.is_accepted,\n",
    "                seed=key,\n",
    "            )\n",
    "\n",
    "            return samples, is_accepted\n",
    "\n",
    "        samples_hmc, is_accepted_hmc = run_chain()\n",
    "        sample_nd = samples_hmc[is_accepted_hmc]\n",
    "\n",
    "        print(\"done ✓\")\n",
    "\n",
    "        return sample_nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf105bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.sbi_method == \"nle\":\n",
    "    inference = SNLE(NDE=nf_log_prob, init_params_nde=params_init, dim=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbi_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
