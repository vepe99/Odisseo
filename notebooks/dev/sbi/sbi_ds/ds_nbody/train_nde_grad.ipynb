{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdee9bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/home/vgiusepp/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "\n",
    "\n",
    "from math import pi\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "\n",
    "from typing import Optional, Tuple, Callable, Union, List\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import vmap, jit, pmap\n",
    "from jax import random\n",
    "import optax\n",
    "import numpyro.distributions as dist\n",
    "\n",
    "\n",
    "# jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import numpy as np\n",
    "from astropy import units as u\n",
    "from astropy import constants as c\n",
    "\n",
    "\n",
    "import odisseo\n",
    "from odisseo import construct_initial_state\n",
    "from odisseo.integrators import leapfrog\n",
    "from odisseo.dynamics import direct_acc, DIRECT_ACC, DIRECT_ACC_LAXMAP, DIRECT_ACC_FOR_LOOP, DIRECT_ACC_MATRIX\n",
    "from odisseo.option_classes import SimulationConfig, SimulationParams, MNParams, NFWParams, PlummerParams, MN_POTENTIAL, NFW_POTENTIAL\n",
    "from odisseo.initial_condition import Plummer_sphere, ic_two_body, sample_position_on_sphere, inclined_circular_velocity, sample_position_on_circle, inclined_position\n",
    "from odisseo.utils import center_of_mass\n",
    "from odisseo.time_integration import time_integration\n",
    "from odisseo.units import CodeUnits\n",
    "from odisseo.visualization import create_3d_gif, create_projection_gif, energy_angular_momentum_plot\n",
    "from odisseo.potentials import MyamotoNagai, NFW\n",
    "from odisseo.option_classes import DIFFRAX_BACKEND, DOPRI5, TSIT5, SEMIIMPLICITEULER, LEAPFROGMIDPOINT, REVERSIBLEHEUN\n",
    "\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.size': 20,\n",
    "    'axes.labelsize': 20,\n",
    "    'xtick.labelsize': 13,\n",
    "    'ytick.labelsize': 13,\n",
    "    'legend.fontsize': 15,\n",
    "})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b31173d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 21:59:51.458524: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746043191.476746   60017 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746043191.482008   60017 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746043191.496331   60017 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746043191.496356   60017 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746043191.496358   60017 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746043191.496359   60017 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import haiku as hk\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "from jaxopt import Bisection\n",
    "from jaxopt.linear_solve import solve_normal_cg\n",
    "\n",
    "# tfp = tfp.substrates.jax\n",
    "tfb = tfp.bijectors\n",
    "tfd = tfp.distributions\n",
    "\n",
    "\n",
    "# This module is to store our implicit inverse functions\n",
    "@partial(jax.custom_vjp, nondiff_argnums=(0,))\n",
    "def root_bisection(f, params):\n",
    "    \"\"\"\n",
    "    f: optimality fn with input arg (params, x)\n",
    "    \"\"\"\n",
    "    bisec = Bisection(\n",
    "        optimality_fun=f,\n",
    "        lower=0.0,\n",
    "        upper=1.0,\n",
    "        check_bracket=False,\n",
    "        maxiter=100,\n",
    "        tol=1e-06,\n",
    "    )\n",
    "    return bisec.run(None, params).params\n",
    "\n",
    "\n",
    "def root_bisection_fwd(f, params):\n",
    "    z_star = root_bisection(f, params)\n",
    "    return z_star, (params, z_star)\n",
    "\n",
    "\n",
    "def root_bwd(f, res, z_star_bar):\n",
    "    params, z_star = res\n",
    "    _, vjp_a = jax.vjp(lambda p: f(z_star, p), params)\n",
    "    _, vjp_z = jax.vjp(lambda z: f(z, params), z_star)\n",
    "    return vjp_a(solve_normal_cg(lambda u: vjp_z(u)[0], -z_star_bar))\n",
    "\n",
    "\n",
    "root_bisection.defvjp(root_bisection_fwd, root_bwd)\n",
    "\n",
    "\n",
    "def make_inverse_fn(f):\n",
    "    \"\"\"Defines the inverse of the input function, and provides implicit gradients\n",
    "    of the inverse.\n",
    "\n",
    "    Args:\n",
    "      f: callable of input shape (params, x)\n",
    "    Retuns:\n",
    "      inv_f: callable of with args (params, y)\n",
    "    \"\"\"\n",
    "\n",
    "    def inv_fn(params, y):\n",
    "        def optimality_fn(x, params):\n",
    "            p, y = params\n",
    "            return f(p, x) - y\n",
    "\n",
    "        return root_bisection(optimality_fn, [params, y])\n",
    "\n",
    "    return inv_fn\n",
    "\n",
    "# Bijiector functions\n",
    "class MixtureAffineSigmoidBijector(tfp.bijectors.Bijector):\n",
    "    \"\"\"\n",
    "    Bijector based on a ramp function, and implemented using an implicit\n",
    "    layer.\n",
    "    This implementation is based on the Smooth Normalizing Flows described\n",
    "    in: https://arxiv.org/abs/2110.00351\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, a, b, c, p, name=\"MixtureAffineSigmoidBijector\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          rho: function of x that defines a ramp function between 0 and 1\n",
    "          a,b,c: scalar parameters of the coupling layer.\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__(forward_min_event_ndims=0, name=name)\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "        self.p = p\n",
    "\n",
    "        def sigmoid(x, a, b, c):\n",
    "            z = (jax.scipy.special.logit(x) + b) * a\n",
    "            y = jax.nn.sigmoid(z) * (1 - c) + c * x\n",
    "            return y\n",
    "\n",
    "        # Rescaled bijection\n",
    "        def f(params, x):\n",
    "            a, b, c, p = params\n",
    "            a_in, b_in = [0.0 - 1e-1, 1.0 + 1e-1]\n",
    "\n",
    "            x = (x - a_in) / (b_in - a_in)\n",
    "            x0 = (jnp.zeros_like(x) - a_in) / (b_in - a_in)\n",
    "            x1 = (jnp.ones_like(x) - a_in) / (b_in - a_in)\n",
    "\n",
    "            y = sigmoid(x, a, b, c)\n",
    "            y0 = sigmoid(x0, a, b, c)\n",
    "            y1 = sigmoid(x1, a, b, c)\n",
    "\n",
    "            y = (y - y0) / (y1 - y0)\n",
    "            return jnp.sum(p * (y * (1 - c) + c * x), axis=0)\n",
    "\n",
    "        self.f = f\n",
    "\n",
    "        # Inverse bijector\n",
    "        self.inv_f = make_inverse_fn(f)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        return jax.vmap(jax.vmap(self.f))([self.a, self.b, self.c, self.p], x)\n",
    "\n",
    "    def _inverse(self, y):\n",
    "        return jax.vmap(jax.vmap(self.inv_f))([self.a, self.b, self.c, self.p], y)\n",
    "\n",
    "    def _forward_log_det_jacobian(self, x):\n",
    "        def logdet_fn(x, a, b, c, p):\n",
    "            g = jax.grad(self.f, argnums=1)([a, b, c, p], x)\n",
    "            s, logdet = jnp.linalg.slogdet(jnp.atleast_2d(g))\n",
    "            return s * logdet\n",
    "\n",
    "        return jax.vmap(jax.vmap(logdet_fn))(x, self.a, self.b, self.c, self.p)\n",
    "\n",
    "\n",
    "#Coupling layers\n",
    "class AffineCoupling(hk.Module):\n",
    "    def __init__(\n",
    "        self, y, *args, layers=[128, 128], activation=jax.nn.leaky_relu, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        y, conditioning variable\n",
    "        layers, list of hidden layers\n",
    "        activation, activation function for hidden layers\n",
    "        \"\"\"\n",
    "        self.y = y\n",
    "        self.layers = layers\n",
    "        self.activation = activation\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __call__(self, x, output_units, **condition_kwargs):\n",
    "        net = jnp.concatenate([x, self.y], axis=-1)\n",
    "        for i, layer_size in enumerate(self.layers):\n",
    "            net = self.activation(hk.Linear(layer_size, name=\"layer%d\" % i)(net))\n",
    "\n",
    "        shifter = tfb.Shift(hk.Linear(output_units)(net))\n",
    "        scaler = tfb.Scale(jnp.clip(jnp.exp(hk.Linear(output_units)(net)), 1e-2, 1e2))\n",
    "        return tfb.Chain([shifter, scaler])\n",
    "\n",
    "\n",
    "class AffineSigmoidCoupling(hk.Module):\n",
    "    \"\"\"This is the coupling layer used in the Flow.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        y,\n",
    "        *args,\n",
    "        layers=[128, 128],\n",
    "        n_components=32,\n",
    "        activation=jax.nn.silu,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        y, conditioning variable\n",
    "        layers, list of hidden layers\n",
    "        n_components, number of mixture components\n",
    "        activation, activation function for hidden layers\n",
    "        \"\"\"\n",
    "        self.y = y\n",
    "        self.layers = layers\n",
    "        self.n_components = n_components\n",
    "        self.activation = activation\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __call__(self, x, output_units, **condition_kwargs):\n",
    "        net = jnp.concatenate([x, self.y], axis=-1)\n",
    "        for i, layer_size in enumerate(self.layers):\n",
    "            net = self.activation(hk.Linear(layer_size, name=\"layer%d\" % i)(net))\n",
    "\n",
    "        log_a_bound = 4\n",
    "        min_density_lower_bound = 1e-4\n",
    "        n_components = self.n_components\n",
    "\n",
    "        log_a = (\n",
    "            jax.nn.tanh(hk.Linear(output_units * n_components, name=\"l3\")(net))\n",
    "            * log_a_bound\n",
    "        )\n",
    "        b = hk.Linear(output_units * n_components, name=\"l4\")(net)\n",
    "        c = min_density_lower_bound + jax.nn.sigmoid(\n",
    "            hk.Linear(output_units * n_components, name=\"l5\")(net)\n",
    "        ) * (1 - min_density_lower_bound)\n",
    "        p = hk.Linear(output_units * n_components, name=\"l6\")(net)\n",
    "\n",
    "        log_a = log_a.reshape(-1, output_units, n_components)\n",
    "        b = b.reshape(-1, output_units, n_components)\n",
    "        c = c.reshape(-1, output_units, n_components)\n",
    "        p = p.reshape(-1, output_units, n_components)\n",
    "        p = jax.nn.softmax(p)\n",
    "\n",
    "        return MixtureAffineSigmoidBijector(jnp.exp(log_a), b, c, p)\n",
    "\n",
    "# Normalizing flow model \n",
    "class ConditionalRealNVP(hk.Module):\n",
    "    \"\"\"A normalizing flow based on RealNVP using specified bijector functions.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, d, *args, n_layers=3, bijector_fn=AffineSigmoidCoupling, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        d, dimensionality of the input\n",
    "        n_layers, number of layers\n",
    "        coupling_layer, list of coupling layers\n",
    "        \"\"\"\n",
    "        self.d = d\n",
    "        self.n_layer = n_layers\n",
    "        self.bijector_fn = bijector_fn\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __call__(self, y):\n",
    "        chain = tfb.Chain(\n",
    "            [\n",
    "                tfb.Permute(jnp.arange(self.d)[::-1])(\n",
    "                    tfb.RealNVP(\n",
    "                        self.d // 2, bijector_fn=self.bijector_fn(y, name=\"b%d\" % i)\n",
    "                    )\n",
    "                )\n",
    "                for i in range(self.n_layer)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        nvp = tfd.TransformedDistribution(\n",
    "            tfd.MultivariateNormalDiag(0.5 * jnp.ones(self.d), 0.05 * jnp.ones(self.d)),\n",
    "            bijector=chain,\n",
    "        )\n",
    "\n",
    "        return nvp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c8a0b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "#mimic the argument parser used in the sbi_bm_lens\n",
    "\n",
    "class args_namedtuple(NamedTuple):\n",
    "\n",
    "    #seed\n",
    "    seed = 42\n",
    "\n",
    "    #activation function\n",
    "    activ_fun = \"silu\"\n",
    "\n",
    "    #Normalizing flow\n",
    "    nf = \"affine\"\n",
    "\n",
    "    #score type\n",
    "    score_type = \"conditional\"\n",
    "\n",
    "    #sbi method\n",
    "    sbi_method = \"nle\"\n",
    "\n",
    "    #number of bijiector layers\n",
    "    n_bijector_layers = 5\n",
    "\n",
    "    #number of coupling layers\n",
    "    n_flow_layers = 3\n",
    "\n",
    "    #number of observables\n",
    "    n_observables = 10 \n",
    "    \n",
    "    #number of parameters\n",
    "    n_parameters = 5 #[\"t_end\", \"Mtot_plummer\", \"a_plummer\", \"M_NFW\", \"r_s\"]\n",
    "\n",
    "    #number of step\n",
    "    total_steps = 10_000\n",
    "\n",
    "    #batch size\n",
    "    bacth_size = 64\n",
    "\n",
    "    #score weight\n",
    "    score_weight = 1.0\n",
    "\n",
    "\n",
    "args = args_namedtuple()\n",
    "\n",
    "master_seed = jax.random.PRNGKey(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d394311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done ✓\n"
     ]
    }
   ],
   "source": [
    "#### CREATE THE NDE #####\n",
    "\n",
    "#activation  function\n",
    "if args.activ_fun == \"silu\":\n",
    "    activ_fun = jax.nn.silu\n",
    "elif args.activ_fun == \"sin\":\n",
    "    activ_func = jnp.sin\n",
    "\n",
    "#score type\n",
    "if args.sbi_method == \"nle\":\n",
    "    score_type = \"conditional\"  #need to be pass to the model object (stochastic simulator)\n",
    "\n",
    "#NF type, either smooth or affine\n",
    "if args.nf == \"smooth\":\n",
    "    if args.sbi_method == \"npe\":\n",
    "        pass\n",
    "    elif args.sbi_method == \"nle\":\n",
    "        # how these qantities are comute (with dataset form prior)\n",
    "        # scale_y = jnp.std(dataset_y, axis=0) / 0.07\n",
    "        # shift_y = jnp.mean(dataset_y / scale_y, axis=0) - 0.5\n",
    "\n",
    "        #just place holder, need to be calculated from the dataset for normalization \n",
    "        scale = jnp.ones(5)\n",
    "        shift = jnp.ones(5)\n",
    "\n",
    "        #NF ARCHITECTURE\n",
    "        #128 neuron per layer\n",
    "        bijector_layers = [128] * args.n_bijector_layers\n",
    "        \n",
    "        bijector = partial(\n",
    "            AffineSigmoidCoupling,\n",
    "            layers=bijector_layers,\n",
    "            activation=activ_fun,\n",
    "            n_components=16,\n",
    "        )\n",
    "        \n",
    "        NF = partial(ConditionalRealNVP, \n",
    "                     n_layers=args.n_flow_layers, \n",
    "                     bijector_fn=bijector)\n",
    "\n",
    "        class NDE(hk.Module):\n",
    "            def __call__(self, y):\n",
    "                nvp = NF(args.n_observables)(y)\n",
    "                return tfd.TransformedDistribution(\n",
    "                    nvp, tfb.Chain([tfb.Scale(scale), tfb.Shift(shift)])\n",
    "                )\n",
    "        \n",
    "elif args.nf == \"affine\":\n",
    "    bijector_layers = [128] * args.n_bijector_layers\n",
    "\n",
    "    bijector = partial(AffineCoupling, layers=bijector_layers, activation=activ_fun)\n",
    "\n",
    "    NF = partial(ConditionalRealNVP, n_layers=args.n_flow_layers, bijector_fn=bijector)\n",
    "\n",
    "    class NDE(hk.Module):\n",
    "        def __call__(self, y):\n",
    "            return NF(args.n_observables)(y)\n",
    "        \n",
    "#FOR NPE NEED TO HAVE SMOOTH NORMALIZING FLOW\n",
    "if args.nf == \"affine\" and args.sbi_method == \"npe\" and args.score_weight > 0:\n",
    "    raise ValueError(\"NDE has to be smooth\")\n",
    "\n",
    "elif args.sbi_method == \"nle\":\n",
    "    nf_log_prob = hk.without_apply_rng(\n",
    "        hk.transform(lambda theta, y: NDE()(theta).log_prob(y).squeeze())\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"done ✓\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e42f54e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This should be used to create the compress dataset, in this simplified case\n",
    "#we are using it just to sample the prior, to be implemented the compression\n",
    "\n",
    "class JaxCompressedSimulator:\n",
    "    def __init__(self, ):\n",
    "\n",
    "        # self.compressor = compressor #not used\n",
    "        # self.params_compressor = params_compressor #not used\n",
    "        # self.opt_state = opt_state #not used\n",
    "\n",
    "        self.t_end = dist.Uniform(0.5, 10.0)\n",
    "        self.Mtot_plummer = dist.Uniform(1e3, 1e5)\n",
    "        self.a_plummer = dist.Uniform(0.1, 2.0)\n",
    "        self.M_NFW = dist.Uniform(5e11, 1.5e12)\n",
    "        self.r_s = dist.Uniform(1.0, 20.0)\n",
    "\n",
    "        self.stack = [\n",
    "            self.t_end,\n",
    "            self.Mtot_plummer,\n",
    "            self.a_plummer,\n",
    "            self.M_NFW,\n",
    "            self.r_s,\n",
    "        ]\n",
    "\n",
    "    def prior_sample(self, sample_shape, master_key):\n",
    "        samples = []\n",
    "\n",
    "        keys = jax.random.split(master_key, 6)\n",
    "\n",
    "        for i, distribution in enumerate(self.stack):\n",
    "            samples.append(distribution.sample(keys[i], sample_shape))\n",
    "\n",
    "        return jnp.stack(samples).T\n",
    "\n",
    "    def prior_log_prob(self, values):\n",
    "        logp = 0\n",
    "\n",
    "        for i, distribution in enumerate(self.stack):\n",
    "            logp += distribution.log_prob(values[..., i])\n",
    "\n",
    "        return logp\n",
    "\n",
    "compressed_simulator = JaxCompressedSimulator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edb1df00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... inference\n"
     ]
    }
   ],
   "source": [
    "######## INFERENCE ########\n",
    "print(\"... inference\")\n",
    "\n",
    "params_init = nf_log_prob.init(\n",
    "    master_seed, theta=0.5 * jnp.ones([1, 5]), y=0.5 * jnp.ones([1, args.n_observables])\n",
    ")\n",
    "\n",
    "prior_mean = jnp.mean(compressed_simulator.prior_sample((1000,), master_seed), axis=0)\n",
    "\n",
    "nb_steps = args.total_steps - args.total_steps * 0.2\n",
    "\n",
    "lr_scheduler = optax.exponential_decay(\n",
    "    init_value=0.001,\n",
    "    transition_steps=nb_steps // 50,\n",
    "    decay_rate=0.9,\n",
    "    end_value=1e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1ef99c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSetsEncoder(hk.Module):\n",
    "    def __init__(self, output_dim, hidden_dim: int = 128, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def __call__(self, x):  # x: [N_particles, 6] or [B, N_particles, 6]\n",
    "        mlp_phi = hk.nets.MLP([self.hidden_dim, self.hidden_dim, self.output_dim])\n",
    "\n",
    "        if x.ndim == 2:\n",
    "            # Unbatched case: [N_particles, 6]\n",
    "            x_phi = mlp_phi(x)  # [N_particles, output_dim]\n",
    "            summary = jnp.mean(x_phi, axis=0)  # [output_dim]\n",
    "        elif x.ndim == 3:\n",
    "            # Batched case: [B, N_particles, 6]\n",
    "            # Flatten for MLP: [B * N_particles, 6]\n",
    "            B, N, D = x.shape\n",
    "            x_flat = x.reshape(-1, D)\n",
    "            x_phi_flat = mlp_phi(x_flat)  # [B * N_particles, output_dim]\n",
    "            x_phi = x_phi_flat.reshape(B, N, self.output_dim)\n",
    "            summary = jnp.mean(x_phi, axis=1)  # [B, output_dim]\n",
    "        else:\n",
    "            raise ValueError(f\"Input must be of shape (N, D) or (B, N, D), got {x.shape}\")\n",
    "\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "390ead59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLE:\n",
    "    def __init__(self, NDE, init_params_nde, args):\n",
    "        self.NDE = NDE\n",
    "        self.params = init_params_nde\n",
    "        self.dim = args.n_observables\n",
    "\n",
    "    def log_prob_fn(self, params, theta, y):\n",
    "        return self.NDE.apply(params, theta, y)\n",
    "\n",
    "    def loss_nll_and_score(self, params, mu, batch, score, weight_score):\n",
    "        lp, out = jax.vmap(\n",
    "            jax.value_and_grad(\n",
    "                lambda theta, x: self.log_prob_fn(\n",
    "                    params, theta.reshape([1, 5]), x.reshape([1, self.dim])\n",
    "                ).squeeze()\n",
    "            )\n",
    "        )(mu, batch)\n",
    "\n",
    "        return (\n",
    "            -jnp.mean(lp) + weight_score * jnp.sum((out - score) ** 2, axis=-1).mean()\n",
    "        )\n",
    "\n",
    "    def loss_nll(self, params, mu, batch, score, weight_score):\n",
    "        lp = self.log_prob_fn(params, mu, batch)\n",
    "\n",
    "        return -jnp.mean(lp)\n",
    "\n",
    "    def train(\n",
    "        self, data_path, learning_rate, total_steps=30_000, batch_size=128, score_weight=0\n",
    "    ):\n",
    "        #let's take only N<1000 \n",
    "        #load data\n",
    "        data_path = './data/data_NFW/'\n",
    "        pattern = re.compile(r\"chunk_(\\d+)\\.npz\")  # capture any number of digits\n",
    "        files = sorted(\n",
    "            f for f in Path(data_path).glob(\"chunk_*.npz\")\n",
    "            if (m := pattern.fullmatch(f.name)) and int(m.group(1)) < 1000\n",
    "        )\n",
    "        theta_list, x_list, score_list = [], [], []\n",
    "\n",
    "        for f in files:\n",
    "            data = np.load(f)\n",
    "            theta_list.append(data[\"theta\"])\n",
    "            x_list.append(data[\"x\"])\n",
    "            score_list.append(data[\"score\"]) \n",
    "            \n",
    "        dataset_theta = jnp.array(theta_list,).reshape(-1, 5)\n",
    "        dataset_y = jnp.array(x_list, ).reshape(-1, 10_000, 6)\n",
    "\n",
    "        if score_weight != 0:\n",
    "            dataset_score = jnp.stack(score_list,).reshape(-1, 5)\n",
    "            loss_fn = self.loss_nll_and_score\n",
    "        else:\n",
    "            loss_fn = self.loss_nll\n",
    "        \n",
    "        #normalization function\n",
    "        \n",
    "        def normalize(dataset, is_observable = False, dataset_name=None):\n",
    "            if dataset_name is not None:\n",
    "                mean = jnp.load(f'./params_compressor/normalization_for_compressor_{dataset_name}.npz')['mean']\n",
    "                std = jnp.load(f'./params_compressor/normalization_for_compressor_{dataset_name}.npz')['std']\n",
    "            if is_observable:\n",
    "                # the shape in this case is (N, N_particles, 6)\n",
    "                dataset_original_shape = dataset.shape\n",
    "                normalized_dataset = dataset.reshape(-1, dataset.shape[-1])\n",
    "                normalized_dataset = (normalized_dataset - mean)/ (std + 1e-8) \n",
    "                normalized_dataset = normalized_dataset.reshape(dataset_original_shape)\n",
    "            else:\n",
    "                normalized_dataset = (dataset - mean)/ (std + 1e-8) \n",
    "\n",
    "            return normalized_dataset\n",
    "\n",
    "        dataset_theta = normalize(dataset_theta, dataset_name='theta')\n",
    "        dataset_y = normalize(dataset_y, is_observable=True, dataset_name='y')\n",
    "        dataset_score = normalize(dataset_score, dataset_name='score')\n",
    "\n",
    "\n",
    "        #compress the observation y, first we need to load the parameters and the opt_state of the compressor pretrained\n",
    "\n",
    "        compress_dim = 10 #trained with this\n",
    "        compressor = hk.transform_with_state(\n",
    "                        lambda y: DeepSetsEncoder(compress_dim)(y)\n",
    "                    )\n",
    "        \n",
    "        a_file = open(\n",
    "            \"./params_compressor/opt_state_SetNet_vmim.pkl\",\n",
    "            \"rb\",\n",
    "        )\n",
    "        opt_state_SetNet = pickle.load(a_file)\n",
    "\n",
    "        a_file = open(\n",
    "            \"./params_compressor/params_nd_compressor_vmim.pkl\",\n",
    "            \"rb\",\n",
    "        )\n",
    "        parameters_compressor = pickle.load(a_file)\n",
    "        #compressing like this is too much memory, let's do in the batches\n",
    "        # dataset_y, _ = compressor.apply(\n",
    "        #                         parameters_compressor, opt_state_SetNet, None, dataset_y\n",
    "        #                     )\n",
    "        # print('dataset y has been compressed')\n",
    "\n",
    "        nb_simu = len(dataset_theta)\n",
    "\n",
    "        print(\"nb of simulations used for training: \", nb_simu)\n",
    "\n",
    "        params = self.params\n",
    "        optimizer = optax.adam(learning_rate)\n",
    "        opt_state = optimizer.init(params)\n",
    "\n",
    "        @jax.jit\n",
    "        def update(params, opt_state, mu, batch, score, weight_score):\n",
    "            \"\"\"Single SGD update step.\"\"\"\n",
    "            loss, grads = jax.value_and_grad(loss_fn)(\n",
    "                params, mu, batch, score, weight_score\n",
    "            )\n",
    "            updates, new_opt_state = optimizer.update(grads, opt_state, params)\n",
    "            new_params = optax.apply_updates(params, updates)\n",
    "\n",
    "            return loss, new_params, new_opt_state\n",
    "\n",
    "        print(\"... start training\")\n",
    "\n",
    "        batch_loss = []\n",
    "        lr_scheduler_store = []\n",
    "        pbar = tqdm(range(total_steps))\n",
    "\n",
    "        for batch in pbar:\n",
    "            inds = np.random.randint(0, nb_simu, batch_size)\n",
    "            ex_theta = dataset_theta[inds]\n",
    "            #compressing the y\n",
    "            ex_y, _ = compressor.apply(\n",
    "                                parameters_compressor, opt_state_SetNet, None, dataset_y[inds]\n",
    "                            )\n",
    "            if score_weight != 0:\n",
    "                ex_score = dataset_score[inds]\n",
    "            else:\n",
    "                ex_score = None\n",
    "\n",
    "            if not jnp.isnan(ex_y).any():\n",
    "                l, params, opt_state = update(\n",
    "                    params, opt_state, ex_theta, ex_y, ex_score, score_weight\n",
    "                )\n",
    "\n",
    "                batch_loss.append(l)\n",
    "                pbar.set_description(f\"loss {l:.3f}\")\n",
    "\n",
    "                if jnp.isnan(l):\n",
    "                    break\n",
    "\n",
    "        self.params = params\n",
    "        self.loss = batch_loss\n",
    "\n",
    "        print(\"done ✓\")\n",
    "\n",
    "    def sample(\n",
    "        self,\n",
    "        log_prob_prior,\n",
    "        observation,\n",
    "        init_point,\n",
    "        key,\n",
    "        num_results=3e4,\n",
    "        num_burnin_steps=5e2,\n",
    "        num_chains=12,\n",
    "    ):\n",
    "        print(\"... running hmc\")\n",
    "\n",
    "        @jax.vmap\n",
    "        def unnormalized_log_prob(theta):\n",
    "            prior = log_prob_prior(theta)\n",
    "\n",
    "            likelihood = self.log_prob_fn(\n",
    "                self.params,\n",
    "                theta.reshape([1, self.dim]),\n",
    "                jnp.array(observation).reshape([1, self.dim]),\n",
    "            )\n",
    "\n",
    "            return likelihood + prior\n",
    "\n",
    "        # Initialize the HMC transition kernel.\n",
    "        adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(\n",
    "            tfp.mcmc.HamiltonianMonteCarlo(\n",
    "                target_log_prob_fn=unnormalized_log_prob,\n",
    "                num_leapfrog_steps=3,\n",
    "                step_size=1e-2,\n",
    "            ),\n",
    "            num_adaptation_steps=int(num_burnin_steps * 0.8),\n",
    "        )\n",
    "\n",
    "        # Run the chain (with burn-in).\n",
    "        # @jax.jit\n",
    "        def run_chain():\n",
    "            # Run the chain (with burn-in).\n",
    "            samples, is_accepted = tfp.mcmc.sample_chain(\n",
    "                num_results=num_results,\n",
    "                num_burnin_steps=num_burnin_steps,\n",
    "                current_state=jnp.array(init_point) * jnp.ones([num_chains, self.dim]),\n",
    "                kernel=adaptive_hmc,\n",
    "                trace_fn=lambda _, pkr: pkr.inner_results.is_accepted,\n",
    "                seed=key,\n",
    "            )\n",
    "\n",
    "            return samples, is_accepted\n",
    "\n",
    "        samples_hmc, is_accepted_hmc = run_chain()\n",
    "        sample_nd = samples_hmc[is_accepted_hmc]\n",
    "\n",
    "        print(\"done ✓\")\n",
    "\n",
    "        return sample_nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcf105bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.sbi_method == \"nle\":\n",
    "    inference = SNLE(NDE=nf_log_prob, init_params_nde=params_init, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2d763b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb of simulations used for training:  1000\n",
      "... start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss -34.314: 100%|██████████| 10000/10000 [03:46<00:00, 44.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done ✓\n"
     ]
    }
   ],
   "source": [
    "inference.train(\n",
    "    data_path=\"./data/data_NFW/\",\n",
    "    total_steps=args.total_steps,\n",
    "    batch_size=args.bacth_size,\n",
    "    score_weight=args.score_weight,\n",
    "    learning_rate=lr_scheduler,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13f6fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_fid = \n",
    "\n",
    "posterior_sample = inference.sample(\n",
    "    log_prob_prior=compressed_simulator.prior_log_prob,\n",
    "    observation=m_data_comressed,\n",
    "    init_point=prior_mean,\n",
    "    key=master_seed,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbi_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
