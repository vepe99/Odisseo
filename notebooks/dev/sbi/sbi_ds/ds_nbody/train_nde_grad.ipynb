{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdee9bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/home/vgiusepp/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"9\"\n",
    "\n",
    "\n",
    "from math import pi\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from typing import Optional, Tuple, Callable, Union, List\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import vmap, jit, pmap\n",
    "from jax import random\n",
    "import optax\n",
    "import numpyro.distributions as dist\n",
    "\n",
    "\n",
    "# jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import numpy as np\n",
    "from astropy import units as u\n",
    "from astropy import constants as c\n",
    "\n",
    "\n",
    "import odisseo\n",
    "from odisseo import construct_initial_state\n",
    "from odisseo.integrators import leapfrog\n",
    "from odisseo.dynamics import direct_acc, DIRECT_ACC, DIRECT_ACC_LAXMAP, DIRECT_ACC_FOR_LOOP, DIRECT_ACC_MATRIX\n",
    "from odisseo.option_classes import SimulationConfig, SimulationParams, MNParams, NFWParams, PlummerParams, MN_POTENTIAL, NFW_POTENTIAL\n",
    "from odisseo.initial_condition import Plummer_sphere, ic_two_body, sample_position_on_sphere, inclined_circular_velocity, sample_position_on_circle, inclined_position\n",
    "from odisseo.utils import center_of_mass\n",
    "from odisseo.time_integration import time_integration\n",
    "from odisseo.units import CodeUnits\n",
    "from odisseo.visualization import create_3d_gif, create_projection_gif, energy_angular_momentum_plot\n",
    "from odisseo.potentials import MyamotoNagai, NFW\n",
    "from odisseo.option_classes import DIFFRAX_BACKEND, DOPRI5, TSIT5, SEMIIMPLICITEULER, LEAPFROGMIDPOINT, REVERSIBLEHEUN\n",
    "\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.size': 20,\n",
    "    'axes.labelsize': 20,\n",
    "    'xtick.labelsize': 13,\n",
    "    'ytick.labelsize': 13,\n",
    "    'legend.fontsize': 15,\n",
    "})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b31173d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 09:28:39.313011: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745998119.331202 3466024 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745998119.336481 3466024 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745998119.350981 3466024 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745998119.351003 3466024 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745998119.351004 3466024 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745998119.351005 3466024 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import haiku as hk\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "from jaxopt import Bisection\n",
    "from jaxopt.linear_solve import solve_normal_cg\n",
    "\n",
    "# tfp = tfp.substrates.jax\n",
    "tfb = tfp.bijectors\n",
    "tfd = tfp.distributions\n",
    "\n",
    "\n",
    "# This module is to store our implicit inverse functions\n",
    "@partial(jax.custom_vjp, nondiff_argnums=(0,))\n",
    "def root_bisection(f, params):\n",
    "    \"\"\"\n",
    "    f: optimality fn with input arg (params, x)\n",
    "    \"\"\"\n",
    "    bisec = Bisection(\n",
    "        optimality_fun=f,\n",
    "        lower=0.0,\n",
    "        upper=1.0,\n",
    "        check_bracket=False,\n",
    "        maxiter=100,\n",
    "        tol=1e-06,\n",
    "    )\n",
    "    return bisec.run(None, params).params\n",
    "\n",
    "\n",
    "def root_bisection_fwd(f, params):\n",
    "    z_star = root_bisection(f, params)\n",
    "    return z_star, (params, z_star)\n",
    "\n",
    "\n",
    "def root_bwd(f, res, z_star_bar):\n",
    "    params, z_star = res\n",
    "    _, vjp_a = jax.vjp(lambda p: f(z_star, p), params)\n",
    "    _, vjp_z = jax.vjp(lambda z: f(z, params), z_star)\n",
    "    return vjp_a(solve_normal_cg(lambda u: vjp_z(u)[0], -z_star_bar))\n",
    "\n",
    "\n",
    "root_bisection.defvjp(root_bisection_fwd, root_bwd)\n",
    "\n",
    "\n",
    "def make_inverse_fn(f):\n",
    "    \"\"\"Defines the inverse of the input function, and provides implicit gradients\n",
    "    of the inverse.\n",
    "\n",
    "    Args:\n",
    "      f: callable of input shape (params, x)\n",
    "    Retuns:\n",
    "      inv_f: callable of with args (params, y)\n",
    "    \"\"\"\n",
    "\n",
    "    def inv_fn(params, y):\n",
    "        def optimality_fn(x, params):\n",
    "            p, y = params\n",
    "            return f(p, x) - y\n",
    "\n",
    "        return root_bisection(optimality_fn, [params, y])\n",
    "\n",
    "    return inv_fn\n",
    "\n",
    "# Bijiector functions\n",
    "class MixtureAffineSigmoidBijector(tfp.bijectors.Bijector):\n",
    "    \"\"\"\n",
    "    Bijector based on a ramp function, and implemented using an implicit\n",
    "    layer.\n",
    "    This implementation is based on the Smooth Normalizing Flows described\n",
    "    in: https://arxiv.org/abs/2110.00351\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, a, b, c, p, name=\"MixtureAffineSigmoidBijector\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          rho: function of x that defines a ramp function between 0 and 1\n",
    "          a,b,c: scalar parameters of the coupling layer.\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__(forward_min_event_ndims=0, name=name)\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "        self.p = p\n",
    "\n",
    "        def sigmoid(x, a, b, c):\n",
    "            z = (jax.scipy.special.logit(x) + b) * a\n",
    "            y = jax.nn.sigmoid(z) * (1 - c) + c * x\n",
    "            return y\n",
    "\n",
    "        # Rescaled bijection\n",
    "        def f(params, x):\n",
    "            a, b, c, p = params\n",
    "            a_in, b_in = [0.0 - 1e-1, 1.0 + 1e-1]\n",
    "\n",
    "            x = (x - a_in) / (b_in - a_in)\n",
    "            x0 = (jnp.zeros_like(x) - a_in) / (b_in - a_in)\n",
    "            x1 = (jnp.ones_like(x) - a_in) / (b_in - a_in)\n",
    "\n",
    "            y = sigmoid(x, a, b, c)\n",
    "            y0 = sigmoid(x0, a, b, c)\n",
    "            y1 = sigmoid(x1, a, b, c)\n",
    "\n",
    "            y = (y - y0) / (y1 - y0)\n",
    "            return jnp.sum(p * (y * (1 - c) + c * x), axis=0)\n",
    "\n",
    "        self.f = f\n",
    "\n",
    "        # Inverse bijector\n",
    "        self.inv_f = make_inverse_fn(f)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        return jax.vmap(jax.vmap(self.f))([self.a, self.b, self.c, self.p], x)\n",
    "\n",
    "    def _inverse(self, y):\n",
    "        return jax.vmap(jax.vmap(self.inv_f))([self.a, self.b, self.c, self.p], y)\n",
    "\n",
    "    def _forward_log_det_jacobian(self, x):\n",
    "        def logdet_fn(x, a, b, c, p):\n",
    "            g = jax.grad(self.f, argnums=1)([a, b, c, p], x)\n",
    "            s, logdet = jnp.linalg.slogdet(jnp.atleast_2d(g))\n",
    "            return s * logdet\n",
    "\n",
    "        return jax.vmap(jax.vmap(logdet_fn))(x, self.a, self.b, self.c, self.p)\n",
    "\n",
    "\n",
    "#Coupling layers\n",
    "class AffineCoupling(hk.Module):\n",
    "    def __init__(\n",
    "        self, y, *args, layers=[128, 128], activation=jax.nn.leaky_relu, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        y, conditioning variable\n",
    "        layers, list of hidden layers\n",
    "        activation, activation function for hidden layers\n",
    "        \"\"\"\n",
    "        self.y = y\n",
    "        self.layers = layers\n",
    "        self.activation = activation\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __call__(self, x, output_units, **condition_kwargs):\n",
    "        net = jnp.concatenate([x, self.y], axis=-1)\n",
    "        for i, layer_size in enumerate(self.layers):\n",
    "            net = self.activation(hk.Linear(layer_size, name=\"layer%d\" % i)(net))\n",
    "\n",
    "        shifter = tfb.Shift(hk.Linear(output_units)(net))\n",
    "        scaler = tfb.Scale(jnp.clip(jnp.exp(hk.Linear(output_units)(net)), 1e-2, 1e2))\n",
    "        return tfb.Chain([shifter, scaler])\n",
    "\n",
    "\n",
    "class AffineSigmoidCoupling(hk.Module):\n",
    "    \"\"\"This is the coupling layer used in the Flow.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        y,\n",
    "        *args,\n",
    "        layers=[128, 128],\n",
    "        n_components=32,\n",
    "        activation=jax.nn.silu,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        y, conditioning variable\n",
    "        layers, list of hidden layers\n",
    "        n_components, number of mixture components\n",
    "        activation, activation function for hidden layers\n",
    "        \"\"\"\n",
    "        self.y = y\n",
    "        self.layers = layers\n",
    "        self.n_components = n_components\n",
    "        self.activation = activation\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __call__(self, x, output_units, **condition_kwargs):\n",
    "        net = jnp.concatenate([x, self.y], axis=-1)\n",
    "        for i, layer_size in enumerate(self.layers):\n",
    "            net = self.activation(hk.Linear(layer_size, name=\"layer%d\" % i)(net))\n",
    "\n",
    "        log_a_bound = 4\n",
    "        min_density_lower_bound = 1e-4\n",
    "        n_components = self.n_components\n",
    "\n",
    "        log_a = (\n",
    "            jax.nn.tanh(hk.Linear(output_units * n_components, name=\"l3\")(net))\n",
    "            * log_a_bound\n",
    "        )\n",
    "        b = hk.Linear(output_units * n_components, name=\"l4\")(net)\n",
    "        c = min_density_lower_bound + jax.nn.sigmoid(\n",
    "            hk.Linear(output_units * n_components, name=\"l5\")(net)\n",
    "        ) * (1 - min_density_lower_bound)\n",
    "        p = hk.Linear(output_units * n_components, name=\"l6\")(net)\n",
    "\n",
    "        log_a = log_a.reshape(-1, output_units, n_components)\n",
    "        b = b.reshape(-1, output_units, n_components)\n",
    "        c = c.reshape(-1, output_units, n_components)\n",
    "        p = p.reshape(-1, output_units, n_components)\n",
    "        p = jax.nn.softmax(p)\n",
    "\n",
    "        return MixtureAffineSigmoidBijector(jnp.exp(log_a), b, c, p)\n",
    "\n",
    "# Normalizing flow model \n",
    "class ConditionalRealNVP(hk.Module):\n",
    "    \"\"\"A normalizing flow based on RealNVP using specified bijector functions.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, d, *args, n_layers=3, bijector_fn=AffineSigmoidCoupling, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        d, dimensionality of the input\n",
    "        n_layers, number of layers\n",
    "        coupling_layer, list of coupling layers\n",
    "        \"\"\"\n",
    "        self.d = d\n",
    "        self.n_layer = n_layers\n",
    "        self.bijector_fn = bijector_fn\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __call__(self, y):\n",
    "        chain = tfb.Chain(\n",
    "            [\n",
    "                tfb.Permute(jnp.arange(self.d)[::-1])(\n",
    "                    tfb.RealNVP(\n",
    "                        self.d // 2, bijector_fn=self.bijector_fn(y, name=\"b%d\" % i)\n",
    "                    )\n",
    "                )\n",
    "                for i in range(self.n_layer)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        nvp = tfd.TransformedDistribution(\n",
    "            tfd.MultivariateNormalDiag(0.5 * jnp.ones(self.d), 0.05 * jnp.ones(self.d)),\n",
    "            bijector=chain,\n",
    "        )\n",
    "\n",
    "        return nvp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c8a0b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "#mimic the argument parser used in the sbi_bm_lens\n",
    "\n",
    "class args_namedtuple(NamedTuple):\n",
    "\n",
    "    #seed\n",
    "    seed = 42\n",
    "\n",
    "    #activation function\n",
    "    activ_fun = \"silu\"\n",
    "\n",
    "    #Normalizing flow\n",
    "    nf = \"smooth\"\n",
    "\n",
    "    #score type\n",
    "    score_type = \"conditional\"\n",
    "\n",
    "    #sbi method\n",
    "    sbi_method = \"nle\"\n",
    "\n",
    "    #number of bijiector layers\n",
    "    n_bijector_layers = 5\n",
    "\n",
    "    #number of coupling layers\n",
    "    n_flow_layers = 3\n",
    "\n",
    "    #number of parameters\n",
    "    n_params = 5 #[\"t_end\", \"Mtot_plummer\", \"a_plummer\", \"M_NFW\", \"r_s\"]\n",
    "\n",
    "    #number of step\n",
    "    total_steps = 10_000\n",
    "\n",
    "    #batch size\n",
    "    bacth_size = 64\n",
    "\n",
    "    #score weight\n",
    "    score_weight = 1.0\n",
    "\n",
    "\n",
    "args = args_namedtuple()\n",
    "\n",
    "master_seed = jax.random.PRNGKey(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d394311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done ✓\n"
     ]
    }
   ],
   "source": [
    "#### CREATE THE NDE #####\n",
    "\n",
    "#activation  function\n",
    "if args.activ_fun == \"silu\":\n",
    "    activ_fun = jax.nn.silu\n",
    "elif args.activ_fun == \"sin\":\n",
    "    activ_func = jnp.sin\n",
    "\n",
    "#score type\n",
    "if args.sbi_method == \"nle\":\n",
    "    score_type = \"conditional\"  #need to be pass to the model object (stochastic simulator)\n",
    "\n",
    "#NF type, either smooth or affine\n",
    "if args.nf == \"smooth\":\n",
    "    if args.sbi_method == \"npe\":\n",
    "        pass\n",
    "    elif args.sbi_method == \"nle\":\n",
    "        # how these qantities are comute (with dataset form prior)\n",
    "        # scale_y = jnp.std(dataset_y, axis=0) / 0.07\n",
    "        # shift_y = jnp.mean(dataset_y / scale_y, axis=0) - 0.5\n",
    "\n",
    "        #just place holder, need to be calculated from the dataset for normalization \n",
    "        scale = jnp.ones(5)\n",
    "        shift = jnp.ones(5)\n",
    "\n",
    "        #NF ARCHITECTURE\n",
    "        #128 neuron per layer\n",
    "        bijector_layers = [128] * args.n_bijector_layers\n",
    "        \n",
    "        bijector = partial(\n",
    "            AffineSigmoidCoupling,\n",
    "            layers=bijector_layers,\n",
    "            activation=activ_fun,\n",
    "            n_components=16,\n",
    "        )\n",
    "        \n",
    "        NF = partial(ConditionalRealNVP, \n",
    "                     n_layers=args.n_flow_layers, \n",
    "                     bijector_fn=bijector)\n",
    "\n",
    "        class NDE(hk.Module):\n",
    "            def __call__(self, y):\n",
    "                nvp = NF(args.n_params)(y)\n",
    "                return tfd.TransformedDistribution(\n",
    "                    nvp, tfb.Chain([tfb.Scale(scale), tfb.Shift(shift)])\n",
    "                )\n",
    "        \n",
    "elif args.nf == \"affine\":\n",
    "    bijector_layers = [128] * args.n_bijector_layers\n",
    "\n",
    "    bijector = partial(AffineCoupling, layers=bijector_layers, activation=activ_fun)\n",
    "\n",
    "    NF = partial(ConditionalRealNVP, n_layers=args.n_flow_layers, bijector_fn=bijector)\n",
    "\n",
    "    class NDE(hk.Module):\n",
    "        def __call__(self, y):\n",
    "            return NF(args.n_params)(y)\n",
    "        \n",
    "#FOR NPE NEED TO HAVE SMOOTH NORMALIZING FLOW\n",
    "if args.nf == \"affine\" and args.sbi_method == \"npe\" and args.score_weight > 0:\n",
    "    raise ValueError(\"NDE has to be smooth\")\n",
    "\n",
    "elif args.sbi_method == \"nle\":\n",
    "    nf_log_prob = hk.without_apply_rng(\n",
    "        hk.transform(lambda theta, y: NDE()(theta).log_prob(y).squeeze())\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"done ✓\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e42f54e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This should be used to create the compress dataset, in this simplified case\n",
    "#we are using it just to sample the prior, to be implemented the compression\n",
    "\n",
    "class JaxCompressedSimulator:\n",
    "    def __init__(self, ):\n",
    "\n",
    "        # self.compressor = compressor #not used\n",
    "        # self.params_compressor = params_compressor #not used\n",
    "        # self.opt_state = opt_state #not used\n",
    "\n",
    "        self.t_end = dist.Uniform(0.5, 10.0)\n",
    "        self.Mtot_plummer = dist.Uniform(1e3, 1e5)\n",
    "        self.a_plummer = dist.Uniform(0.1, 2.0)\n",
    "        self.M_NFW = dist.Uniform(5e11, 1.5e12)\n",
    "        self.r_s = dist.Uniform(1.0, 20.0)\n",
    "\n",
    "        self.stack = [\n",
    "            self.t_end,\n",
    "            self.Mtot_plummer,\n",
    "            self.a_plummer,\n",
    "            self.M_NFW,\n",
    "            self.r_s,\n",
    "        ]\n",
    "\n",
    "    def prior_sample(self, sample_shape, master_key):\n",
    "        samples = []\n",
    "\n",
    "        keys = jax.random.split(master_key, 6)\n",
    "\n",
    "        for i, distribution in enumerate(self.stack):\n",
    "            samples.append(distribution.sample(keys[i], sample_shape))\n",
    "\n",
    "        return jnp.stack(samples).T\n",
    "\n",
    "    def prior_log_prob(self, values):\n",
    "        logp = 0\n",
    "\n",
    "        for i, distribution in enumerate(self.stack):\n",
    "            logp += distribution.log_prob(values[..., i])\n",
    "\n",
    "        return logp\n",
    "\n",
    "compressed_simulator = JaxCompressedSimulator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edb1df00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... inference\n"
     ]
    }
   ],
   "source": [
    "######## INFERENCE ########\n",
    "print(\"... inference\")\n",
    "\n",
    "params_init = nf_log_prob.init(\n",
    "    master_seed, 0.5 * jnp.ones([1, args.n_params]), 0.5 * jnp.ones([1, args.n_params])\n",
    ")\n",
    "\n",
    "prior_mean = jnp.mean(compressed_simulator.prior_sample((1000,), master_seed), axis=0)\n",
    "\n",
    "nb_steps = args.total_steps - args.total_steps * 0.2\n",
    "\n",
    "lr_scheduler = optax.exponential_decay(\n",
    "    init_value=0.001,\n",
    "    transition_steps=nb_steps // 50,\n",
    "    decay_rate=0.9,\n",
    "    end_value=1e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "390ead59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLE:\n",
    "    def __init__(self, NDE, init_params_nde, args):\n",
    "        self.NDE = NDE\n",
    "        self.params = init_params_nde\n",
    "        self.dim = args.n_params\n",
    "\n",
    "    def log_prob_fn(self, params, theta, y):\n",
    "        return self.NDE.apply(params, theta, y)\n",
    "\n",
    "    def loss_nll_and_score(self, params, mu, batch, score, weight_score):\n",
    "        print(\"mu shape\", mu.shape)\n",
    "        print(\"batch shape\", batch.shape)   \n",
    "        lp, out = jax.vmap(\n",
    "            jax.value_and_grad(\n",
    "                lambda theta, x: self.log_prob_fn(\n",
    "                    params, theta.reshape([1, self.dim]), x.reshape([1, self.dim])\n",
    "                ).squeeze()\n",
    "            )\n",
    "        )(mu, batch)\n",
    "\n",
    "        return (\n",
    "            -jnp.mean(lp) + weight_score * jnp.sum((out - score) ** 2, axis=-1).mean()\n",
    "        )\n",
    "\n",
    "    def loss_nll(self, params, mu, batch, score, weight_score):\n",
    "        lp = self.log_prob_fn(params, mu, batch)\n",
    "\n",
    "        return -jnp.mean(lp)\n",
    "\n",
    "    def train(\n",
    "        self, data_path, learning_rate, total_steps=30_000, batch_size=128, score_weight=0\n",
    "    ):\n",
    "        #let's take only N<1000 \n",
    "        pattern = re.compile(r\"chunk_(\\d+)\\.npz\")  # capture any number of digits\n",
    "\n",
    "        files = sorted(\n",
    "            f for f in Path(data_path).glob(\"chunk_*.npz\")\n",
    "            if (m := pattern.fullmatch(f.name)) and int(m.group(1)) < 1000\n",
    "        )\n",
    "        theta_list, x_list, score_list = [], [], []\n",
    "        for f in files:\n",
    "            data = np.load(f)\n",
    "            theta_list.append(data[\"theta\"])\n",
    "            x_list.append(data[\"x\"])\n",
    "            score_list.append(data[\"score\"])\n",
    "        dataset_theta = jnp.concatenate(theta_list)\n",
    "        dataset_y = jnp.concatenate(x_list)\n",
    "        if score_weight != 0:\n",
    "            dataset_score = jnp.concatenate(score_list)\n",
    "            loss_fn = self.loss_nll_and_score\n",
    "        else:\n",
    "            loss_fn = self.loss_nll\n",
    "\n",
    "        nb_simu = len(dataset_theta)\n",
    "\n",
    "        print(\"nb of simulations used for training: \", nb_simu)\n",
    "\n",
    "        params = self.params\n",
    "        optimizer = optax.adam(learning_rate)\n",
    "        opt_state = optimizer.init(params)\n",
    "\n",
    "        @jax.jit\n",
    "        def update(params, opt_state, mu, batch, score, weight_score):\n",
    "            \"\"\"Single SGD update step.\"\"\"\n",
    "            loss, grads = jax.value_and_grad(loss_fn)(\n",
    "                params, mu, batch, score, weight_score\n",
    "            )\n",
    "            updates, new_opt_state = optimizer.update(grads, opt_state, params)\n",
    "            new_params = optax.apply_updates(params, updates)\n",
    "\n",
    "            return loss, new_params, new_opt_state\n",
    "\n",
    "        print(\"... start training\")\n",
    "\n",
    "        batch_loss = []\n",
    "        lr_scheduler_store = []\n",
    "        pbar = tqdm(range(total_steps))\n",
    "\n",
    "        for batch in pbar:\n",
    "            inds = np.random.randint(0, nb_simu, batch_size)\n",
    "            ex_theta = dataset_theta[inds]\n",
    "            ex_y = dataset_y[inds]\n",
    "            if score_weight != 0:\n",
    "                ex_score = dataset_score[inds]\n",
    "            else:\n",
    "                ex_score = None\n",
    "\n",
    "            if not jnp.isnan(ex_y).any():\n",
    "                l, params, opt_state = update(\n",
    "                    params, opt_state, ex_theta, ex_y, ex_score, score_weight\n",
    "                )\n",
    "\n",
    "                batch_loss.append(l)\n",
    "                pbar.set_description(f\"loss {l:.3f}\")\n",
    "\n",
    "                if jnp.isnan(l):\n",
    "                    break\n",
    "\n",
    "        self.params = params\n",
    "        self.loss = batch_loss\n",
    "\n",
    "        print(\"done ✓\")\n",
    "\n",
    "    def sample(\n",
    "        self,\n",
    "        log_prob_prior,\n",
    "        observation,\n",
    "        init_point,\n",
    "        key,\n",
    "        num_results=3e4,\n",
    "        num_burnin_steps=5e2,\n",
    "        num_chains=12,\n",
    "    ):\n",
    "        print(\"... running hmc\")\n",
    "\n",
    "        @jax.vmap\n",
    "        def unnormalized_log_prob(theta):\n",
    "            prior = log_prob_prior(theta)\n",
    "\n",
    "            likelihood = self.log_prob_fn(\n",
    "                self.params,\n",
    "                theta.reshape([1, self.dim]),\n",
    "                jnp.array(observation).reshape([1, self.dim]),\n",
    "            )\n",
    "\n",
    "            return likelihood + prior\n",
    "\n",
    "        # Initialize the HMC transition kernel.\n",
    "        adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(\n",
    "            tfp.mcmc.HamiltonianMonteCarlo(\n",
    "                target_log_prob_fn=unnormalized_log_prob,\n",
    "                num_leapfrog_steps=3,\n",
    "                step_size=1e-2,\n",
    "            ),\n",
    "            num_adaptation_steps=int(num_burnin_steps * 0.8),\n",
    "        )\n",
    "\n",
    "        # Run the chain (with burn-in).\n",
    "        # @jax.jit\n",
    "        def run_chain():\n",
    "            # Run the chain (with burn-in).\n",
    "            samples, is_accepted = tfp.mcmc.sample_chain(\n",
    "                num_results=num_results,\n",
    "                num_burnin_steps=num_burnin_steps,\n",
    "                current_state=jnp.array(init_point) * jnp.ones([num_chains, self.dim]),\n",
    "                kernel=adaptive_hmc,\n",
    "                trace_fn=lambda _, pkr: pkr.inner_results.is_accepted,\n",
    "                seed=key,\n",
    "            )\n",
    "\n",
    "            return samples, is_accepted\n",
    "\n",
    "        samples_hmc, is_accepted_hmc = run_chain()\n",
    "        sample_nd = samples_hmc[is_accepted_hmc]\n",
    "\n",
    "        print(\"done ✓\")\n",
    "\n",
    "        return sample_nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcf105bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.sbi_method == \"nle\":\n",
    "    inference = SNLE(NDE=nf_log_prob, init_params_nde=params_init, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2d763b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb of simulations used for training:  5000\n",
      "... start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu shape (64,)\n",
      "batch shape (64, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot reshape array of shape () (size 1) into shape [1, 5] (size 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43minference\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./data/data_NFW/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtotal_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbacth_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscore_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscore_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 89\u001b[39m, in \u001b[36mSNLE.train\u001b[39m\u001b[34m(self, data_path, learning_rate, total_steps, batch_size, score_weight)\u001b[39m\n\u001b[32m     86\u001b[39m     ex_score = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m jnp.isnan(ex_y).any():\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     l, params, opt_state = \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mex_theta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mex_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mex_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_weight\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     batch_loss.append(l)\n\u001b[32m     94\u001b[39m     pbar.set_description(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "    \u001b[31m[... skipping hidden 14 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mSNLE.train.<locals>.update\u001b[39m\u001b[34m(params, opt_state, mu, batch, score, weight_score)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;129m@jax\u001b[39m.jit\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupdate\u001b[39m(params, opt_state, mu, batch, score, weight_score):\n\u001b[32m     64\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Single SGD update step.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     loss, grads = \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_score\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m     updates, new_opt_state = optimizer.update(grads, opt_state, params)\n\u001b[32m     69\u001b[39m     new_params = optax.apply_updates(params, updates)\n",
      "    \u001b[31m[... skipping hidden 16 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mSNLE.loss_nll_and_score\u001b[39m\u001b[34m(self, params, mu, batch, score, weight_score)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mmu shape\u001b[39m\u001b[33m\"\u001b[39m, mu.shape)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mbatch shape\u001b[39m\u001b[33m\"\u001b[39m, batch.shape)   \n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m lp, out = \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue_and_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_prob_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m     22\u001b[39m     -jnp.mean(lp) + weight_score * jnp.sum((out - score) ** \u001b[32m2\u001b[39m, axis=-\u001b[32m1\u001b[39m).mean()\n\u001b[32m     23\u001b[39m )\n",
      "    \u001b[31m[... skipping hidden 23 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mSNLE.loss_nll_and_score.<locals>.<lambda>\u001b[39m\u001b[34m(theta, x)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mmu shape\u001b[39m\u001b[33m\"\u001b[39m, mu.shape)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mbatch shape\u001b[39m\u001b[33m\"\u001b[39m, batch.shape)   \n\u001b[32m     13\u001b[39m lp, out = jax.vmap(\n\u001b[32m     14\u001b[39m     jax.value_and_grad(\n\u001b[32m     15\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m theta, x: \u001b[38;5;28mself\u001b[39m.log_prob_fn(\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m             params, \u001b[43mtheta\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, x.reshape([\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.dim])\n\u001b[32m     17\u001b[39m         ).squeeze()\n\u001b[32m     18\u001b[39m     )\n\u001b[32m     19\u001b[39m )(mu, batch)\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m     22\u001b[39m     -jnp.mean(lp) + weight_score * jnp.sum((out - score) ** \u001b[32m2\u001b[39m, axis=-\u001b[32m1\u001b[39m).mean()\n\u001b[32m     23\u001b[39m )\n",
      "    \u001b[31m[... skipping hidden 2 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py:470\u001b[39m, in \u001b[36m_compute_newshape\u001b[39m\u001b[34m(arr, newshape)\u001b[39m\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    468\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(d, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m (*arr.shape, *newshape)) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    469\u001b[39m       arr.size != math.prod(newshape)):\n\u001b[32m--> \u001b[39m\u001b[32m470\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcannot reshape array of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marr.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marr.size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    471\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minto shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00morig_newshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmath.prod(newshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(-core.divide_shape_sizes(arr.shape, newshape)\n\u001b[32m    473\u001b[39m              \u001b[38;5;28;01mif\u001b[39;00m core.definitely_equal(d, -\u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m newshape)\n",
      "\u001b[31mTypeError\u001b[39m: cannot reshape array of shape () (size 1) into shape [1, 5] (size 5)"
     ]
    }
   ],
   "source": [
    "inference.train(\n",
    "    data_path=\"./data/data_NFW/\",\n",
    "    total_steps=args.total_steps,\n",
    "    batch_size=args.bacth_size,\n",
    "    score_weight=args.score_weight,\n",
    "    learning_rate=lr_scheduler,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13f6fed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbi_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
