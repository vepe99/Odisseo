{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "107c77d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 10:06:42.630855: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746173202.648654 1193022 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746173202.653981 1193022 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746173202.668754 1193022 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746173202.668777 1193022 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746173202.668778 1193022 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746173202.668779 1193022 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit\n",
    "import optax\n",
    "import haiku as hk\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "tfb = tfp.bijectors\n",
    "tfd = tfp.distributions\n",
    "\n",
    "\n",
    "from normflow_models import (AffineCoupling,\n",
    "                             AffineSigmoidCoupling,\n",
    "                             ConditionalRealNVP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02a5bb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class to Train the compressor, in our case we are going to train \n",
    "#the compressor with the vmim, so we will also need a Normalizing Flow \n",
    "#  which is going to be trained with the compressor\n",
    "\n",
    "class TrainModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        compressor,\n",
    "        nf,\n",
    "        optimizer,\n",
    "        loss_name,\n",
    "        dim=None,\n",
    "        info_compressor=None,\n",
    "    ):\n",
    "        self.compressor = compressor\n",
    "        self.nf = nf\n",
    "        self.optimizer = optimizer\n",
    "        self.dim = dim  # summary statistic dimension\n",
    "\n",
    "        if loss_name == \"train_compressor_mse\":\n",
    "            self.loss = self.loss_mse\n",
    "        elif loss_name == \"train_compressor_vmim\":\n",
    "            self.loss = self.loss_vmim\n",
    "        elif loss_name == \"train_compressor_gnll\":\n",
    "            self.loss = self.loss_gnll\n",
    "            if self.dim is None:\n",
    "                raise ValueError(\"dim should be specified when using gnll compressor\")\n",
    "        elif loss_name == \"loss_for_sbi\":\n",
    "            if info_compressor is None:\n",
    "                raise ValueError(\"sbi loss needs compressor informations\")\n",
    "            else:\n",
    "                self.info_compressor = info_compressor\n",
    "                self.loss = self.loss_nll\n",
    "\n",
    "    def loss_mse(self, params, theta, x, state_resnet):\n",
    "        \"\"\"Compute the Mean Squared Error loss\"\"\"\n",
    "        y, opt_state_resnet = self.compressor.apply(params, state_resnet, None, x)\n",
    "\n",
    "        loss = jnp.mean(jnp.sum((y - theta) ** 2, axis=1))\n",
    "\n",
    "        return loss, opt_state_resnet\n",
    "\n",
    "    def loss_mae(self, params, theta, x, state_resnet):\n",
    "        \"\"\"Compute the Mean Absolute Error loss\"\"\"\n",
    "        y, opt_state_resnet = self.compressor.apply(params, state_resnet, None, x)\n",
    "\n",
    "        loss = jnp.mean(jnp.sum(jnp.absolute(y - theta), axis=1))\n",
    "\n",
    "        return loss, opt_state_resnet\n",
    "\n",
    "    def loss_vmim(self, params, theta, x, state_resnet):\n",
    "        \"\"\"Compute the Variational Mutual Information Maximization loss\"\"\"\n",
    "        y, opt_state_resnet = self.compressor.apply(params, state_resnet, None, x)\n",
    "        log_prob = self.nf.apply(params, theta, y)\n",
    "\n",
    "        return -jnp.mean(log_prob), opt_state_resnet\n",
    "\n",
    "    def loss_gnll(self, params, theta, x, state_resnet):\n",
    "        \"\"\"Compute the Gaussian Negative Log Likelihood loss\"\"\"\n",
    "        y, opt_state_resnet = self.compressor.apply(params, state_resnet, None, x)\n",
    "        y_mean = y[..., : self.dim]\n",
    "        y_var = y[..., self.dim :]\n",
    "        y_var = tfb.FillScaleTriL(diag_bijector=tfb.Softplus(low=1e-3)).forward(y_var)\n",
    "\n",
    "        @jax.jit\n",
    "        @jax.vmap\n",
    "        def _get_log_prob(y_mean, y_var, theta):\n",
    "            likelihood = tfd.MultivariateNormalTriL(y_mean, y_var)\n",
    "            return likelihood.log_prob(theta)\n",
    "\n",
    "        loss = -jnp.mean(_get_log_prob(y_mean, y_var, theta))\n",
    "\n",
    "        return loss, opt_state_resnet\n",
    "\n",
    "    def loss_nll(self, params, theta, x, _):\n",
    "        \"\"\"Compute the Negative Log Likelihood loss.\n",
    "        This loss is for inference so it requires to have a trained compressor.\n",
    "        \"\"\"\n",
    "        y, _ = self.compressor.apply(\n",
    "            self.info_compressor[0], self.info_compressor[1], None, x\n",
    "        )\n",
    "        log_prob = self.nf.apply(params, theta, y)\n",
    "\n",
    "        return -jnp.mean(log_prob), _\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def update(self, model_params, opt_state, theta, x, state_resnet=None):\n",
    "        (loss, opt_state_resnet), grads = jax.value_and_grad(self.loss, has_aux=True)(\n",
    "            model_params, theta, x, state_resnet\n",
    "        )\n",
    "\n",
    "        updates, new_opt_state = self.optimizer.update(grads, opt_state)\n",
    "\n",
    "        new_params = optax.apply_updates(model_params, updates)\n",
    "\n",
    "        return loss, new_params, new_opt_state, opt_state_resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36dabbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "#mimic the argument parser used in the sbi_bm_lens\n",
    "\n",
    "class args_namedtuple(NamedTuple):\n",
    "\n",
    "    total_steps = 10_000,\n",
    "\n",
    "    loss = \"train_compressor_vmim\",\n",
    "    # loss = \"train_compressor_mse\",\n",
    "\n",
    "\n",
    "args = args_namedtuple()\n",
    "dim = 10\n",
    "N_particles = 10_000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17114efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### create compressor \n",
    "\n",
    "#nf \n",
    "bijector_layers_compressor = [128] * 2\n",
    "\n",
    "bijector_compressor = partial(\n",
    "    AffineCoupling, layers=bijector_layers_compressor, activation=jax.nn.silu\n",
    ")\n",
    "\n",
    "NF_compressor = partial(ConditionalRealNVP, n_layers=4, bijector_fn=bijector_compressor)\n",
    "\n",
    "\n",
    "class Flow_nd_Compressor(hk.Module):\n",
    "    def __call__(self, y):\n",
    "        nvp = NF_compressor(dim)(y)\n",
    "        return nvp\n",
    "\n",
    "\n",
    "nf = hk.without_apply_rng(\n",
    "    hk.transform(lambda theta, y: Flow_nd_Compressor()(y).log_prob(theta).squeeze())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03cdfdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.loss == \"train_compressor_gnll\":\n",
    "    compress_dim = int(dim + ((dim**2) - dim) / 2 + dim)\n",
    "else:\n",
    "    compress_dim = dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "045c7918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DeepSetsEncoder(hk.Module):\n",
    "#     def __init__(self, output_dim, hidden_dim: int = 128, name=None):\n",
    "#         super().__init__(name=name)\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.output_dim = output_dim\n",
    "\n",
    "#     def __call__(self, x):  # x: [N_particles, 6]\n",
    "#         # Ï† network: shared across all particles\n",
    "#         mlp_phi = hk.nets.MLP([self.hidden_dim, self.hidden_dim, self.output_dim])\n",
    "#         x_phi = mlp_phi(x)  # shape: [N_particles, output_dim]\n",
    "\n",
    "#         # Pooling over the set dimension (e.g., mean, sum)\n",
    "#         summary = jnp.mean(x_phi, axis=0)  # shape: [output_dim]\n",
    "\n",
    "#         return summary\n",
    "\n",
    "\n",
    "class DeepSetsEncoder(hk.Module):\n",
    "    def __init__(self, output_dim, hidden_dim: int = 128, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def __call__(self, x):  # x: [N_particles, 6] or [B, N_particles, 6]\n",
    "        mlp_phi = hk.nets.MLP([self.hidden_dim, self.hidden_dim, self.hidden_dim, self.output_dim])\n",
    "\n",
    "        if x.ndim == 2:\n",
    "            # Unbatched case: [N_particles, 6]\n",
    "            x_phi = mlp_phi(x)  # [N_particles, output_dim]\n",
    "            summary = jnp.mean(x_phi, axis=0)  # [output_dim]\n",
    "        elif x.ndim == 3:\n",
    "            # Batched case: [B, N_particles, 6]\n",
    "            # Flatten for MLP: [B * N_particles, 6]\n",
    "            B, N, D = x.shape\n",
    "            x_flat = x.reshape(-1, D)\n",
    "            x_phi_flat = mlp_phi(x_flat)  # [B * N_particles, output_dim]\n",
    "            x_phi = x_phi_flat.reshape(B, N, self.output_dim)\n",
    "            summary = jnp.mean(x_phi, axis=1)  # [B, output_dim]\n",
    "        else:\n",
    "            raise ValueError(f\"Input must be of shape (N, D) or (B, N, D), got {x.shape}\")\n",
    "\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3481ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = hk.transform_with_state(\n",
    "    lambda y: DeepSetsEncoder(compress_dim)(y)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0ebbc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAIN\n",
    "# init compressor\n",
    "parameters_SetNet, opt_state_SetNet = compressor.init(\n",
    "    jax.random.PRNGKey(0), y=jnp.ones([1, N_particles, 6])\n",
    ")\n",
    "\n",
    "# init nf\n",
    "params_nf = nf.init(\n",
    "    jax.random.PRNGKey(0), theta=0.5 * jnp.ones([1, 5]), y=0.5 * jnp.ones([1, dim])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9793d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/10001 [00:03<10:27:19,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN Loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if args.loss[0] == \"train_compressor_vmim\":\n",
    "    parameters_compressor = hk.data_structures.merge(parameters_SetNet, params_nf)\n",
    "elif args.loss[0] in [\n",
    "    \"train_compressor_mse\",\n",
    "    \"train_compressor_mae\",\n",
    "    \"train_compressor_gnll\",\n",
    "]:\n",
    "    parameters_compressor = parameters_SetNet\n",
    "\n",
    "\n",
    "# define optimizer\n",
    "total_steps = args.total_steps[0]\n",
    "\n",
    "if args.loss == \"train_compressor_gnll\":\n",
    "    start_lr = 0.0001\n",
    "\n",
    "else:\n",
    "    start_lr = 0.001\n",
    "\n",
    "lr_scheduler = optax.piecewise_constant_schedule(\n",
    "    init_value=start_lr,\n",
    "    boundaries_and_scales={\n",
    "        int(total_steps * 0.1): 0.7,\n",
    "        int(total_steps * 0.2): 0.7,\n",
    "        int(total_steps * 0.3): 0.7,\n",
    "        int(total_steps * 0.4): 0.7,\n",
    "        int(total_steps * 0.5): 0.7,\n",
    "        int(total_steps * 0.6): 0.7,\n",
    "        int(total_steps * 0.7): 0.7,\n",
    "        int(total_steps * 0.8): 0.7,\n",
    "        int(total_steps * 0.9): 0.7,\n",
    "    },\n",
    ")\n",
    "\n",
    "optimizer_c = optax.adam(learning_rate=lr_scheduler)\n",
    "opt_state_c = optimizer_c.init(parameters_compressor)\n",
    "\n",
    "model_compressor = TrainModel(\n",
    "    compressor=compressor,\n",
    "    nf=nf,\n",
    "    optimizer=optimizer_c,\n",
    "    loss_name=args.loss[0],\n",
    ")\n",
    "\n",
    "\n",
    "update = jax.jit(model_compressor.update)\n",
    "\n",
    "\n",
    "#load data\n",
    "data_path = './data/data_NFW/'\n",
    "pattern = re.compile(r\"chunk_(\\d+)\\.npz\")  # capture any number of digits\n",
    "files = sorted(\n",
    "    f for f in Path(data_path).glob(\"chunk_*.npz\")\n",
    "    if (m := pattern.fullmatch(f.name)) and int(m.group(1)) < 9500\n",
    ")\n",
    "theta_list, x_list, score_list = [], [], []\n",
    "\n",
    "for f in files:\n",
    "    data = np.load(f)\n",
    "    theta_list.append(data[\"theta\"])\n",
    "    x_list.append(data[\"x\"])\n",
    "    score_list.append(data[\"score\"]) \n",
    "\n",
    "\n",
    "\n",
    "# dataset_theta = np.array(theta_list,).reshape(-1, 5)\n",
    "# dataset_y = np.array(x_list, ).reshape(-1, 10_000, 6)\n",
    "# dataset_score = np.stack(score_list,).reshape(-1, 5)\n",
    "\n",
    "dataset_theta = np.array(theta_list)\n",
    "dataset_theta = dataset_theta[~np.isnan(dataset_theta)]\n",
    "dataset_theta = dataset_theta.reshape(-1, 5)\n",
    "dataset_y = np.array(x_list)\n",
    "dataset_y = dataset_y[~np.isnan(dataset_y)]\n",
    "dataset_y = dataset_y.reshape(-1, 10_000, 6)\n",
    "dataset_score = np.stack(score_list)\n",
    "dataset_score = dataset_score[~np.isnan(dataset_score)]\n",
    "dataset_score = dataset_score.reshape(-1, 5)\n",
    "\n",
    "\n",
    "\n",
    "#normalization function\n",
    "# @partial(jax.jit, static_argnums=(1,2))\n",
    "def normalize(dataset, is_observable = False, dataset_name = None):\n",
    "    if is_observable:\n",
    "        # the shape in this case is (N, N_particles, 6)\n",
    "        dataset_original_shape = dataset.shape\n",
    "        normalized_dataset = dataset.reshape(-1, dataset.shape[-1])\n",
    "        mean = np.mean(normalized_dataset, axis=0)\n",
    "        std = np.std(normalized_dataset, axis=0)\n",
    "        normalized_dataset = (normalized_dataset - mean)/ (std + 1e-8) \n",
    "        normalized_dataset = normalized_dataset.reshape(dataset_original_shape)\n",
    "    else:\n",
    "        mean = np.mean(dataset, axis=0)\n",
    "        std = np.std(dataset, axis=0)\n",
    "        normalized_dataset = (dataset - mean)/ (std + 1e-8) \n",
    "\n",
    "    print(f\"mean: {mean}, std: {std}\")\n",
    "    \n",
    "    if dataset_name is not None:\n",
    "        jnp.savez(\n",
    "            f\"./params_compressor/normalization_for_compressor_{dataset_name}.npz\",\n",
    "            mean=mean,\n",
    "            std=std,\n",
    "        )\n",
    "\n",
    "    return normalized_dataset\n",
    "\n",
    "dataset_theta = jnp.array(normalize(dataset_theta, dataset_name=\"theta\"))\n",
    "dataset_y = jnp.array(normalize(dataset_y, is_observable=True, dataset_name=\"y\"))\n",
    "dataset_score = jnp.array(normalize(dataset_score, dataset_name=\"score\"))\n",
    "\n",
    "\n",
    "store_loss = []\n",
    "for batch in tqdm(range(total_steps + 1)):\n",
    "    # batch = np.random.randint(0, len(dataset_theta), 1)\n",
    "    theta = dataset_theta[batch]\n",
    "    x = dataset_y[batch]\n",
    "    score = dataset_score[batch]\n",
    "    # print(f\"theta: {theta.shape}, x: {x.shape}, score: {score.shape}\")\n",
    "    if not jnp.isnan(score).any():\n",
    "        b_loss, parameters_compressor, opt_state_c, opt_state_SetNet = update(\n",
    "            model_params=parameters_compressor,\n",
    "            opt_state=opt_state_c,\n",
    "            theta=theta,\n",
    "            x=x,\n",
    "            state_resnet=opt_state_SetNet,\n",
    "        )\n",
    "        store_loss.append(b_loss)\n",
    "\n",
    "        if jnp.isnan(b_loss):\n",
    "            print(\"NaN Loss\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bea19ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(store_loss)\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.yscale('log')\n",
    "# plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76422251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 128\n",
    "# num_batches = dataset_theta.shape[0] // batch_size\n",
    "\n",
    "# def get_batch(x_data, theta_data, score_data, batch_size, batch_idx):\n",
    "#     start = batch_idx * batch_size\n",
    "#     end = start + batch_size\n",
    "#     return x_data[start:end], theta_data[start:end], score_data[start:end]\n",
    "\n",
    "# store_loss = []\n",
    "# for batch_idx in range(num_batches):\n",
    "#     x_batch, theta_batch, score_batch = get_batch(dataset_y, dataset_theta, dataset_score, batch_size, batch_idx)\n",
    "#     print(x_batch.shape, theta_batch.shape, score_batch.shape)\n",
    "    \n",
    "#     if not jnp.isnan(score_batch).any():\n",
    "#         b_loss, parameters_compressor, opt_state_c, opt_state_SetNet = update(\n",
    "#             model_params=parameters_compressor,\n",
    "#             opt_state=opt_state_c,\n",
    "#             theta=theta_batch,\n",
    "#             x=x_batch,\n",
    "#             state_resnet=opt_state_SetNet,\n",
    "#         )\n",
    "#         store_loss.append(b_loss)\n",
    "\n",
    "#         if jnp.isnan(b_loss):\n",
    "#             print(\"NaN Loss\")\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f187e57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the compressor \n",
    "\n",
    "compress_data, _ = compressor.apply(\n",
    "    parameters_compressor, opt_state_SetNet, None, dataset_y[:2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "540fd50a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compress_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e50cfe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model \n",
    "import pickle\n",
    "\n",
    "with open(\n",
    "    \"./params_compressor/params_nd_compressor_vmim.pkl\", \"wb\"\n",
    ") as fp:\n",
    "    pickle.dump(parameters_compressor, fp)\n",
    "\n",
    "with open(\"./params_compressor/opt_state_SetNet_vmim.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(opt_state_SetNet, fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac1754c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbi_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
