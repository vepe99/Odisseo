{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "107c77d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 17:07:27.904580: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746025647.922662 4040271 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746025647.927969 4040271 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746025647.942227 4040271 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746025647.942255 4040271 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746025647.942256 4040271 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746025647.942257 4040271 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "/export/home/vgiusepp/miniconda3/envs/sbi_ds/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit\n",
    "import optax\n",
    "import haiku as hk\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "tfb = tfp.bijectors\n",
    "tfd = tfp.distributions\n",
    "\n",
    "\n",
    "from normflow_models import (AffineCoupling,\n",
    "                             AffineSigmoidCoupling,\n",
    "                             ConditionalRealNVP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02a5bb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class to Train the compressor, in our case we are going to train \n",
    "#the compressor with the vmim, so we will also need a Normalizing Flow \n",
    "#  which is going to be trained with the compressor\n",
    "\n",
    "class TrainModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        compressor,\n",
    "        nf,\n",
    "        optimizer,\n",
    "        loss_name,\n",
    "        dim=None,\n",
    "        info_compressor=None,\n",
    "    ):\n",
    "        self.compressor = compressor\n",
    "        self.nf = nf\n",
    "        self.optimizer = optimizer\n",
    "        self.dim = dim  # summary statistic dimension\n",
    "\n",
    "        if loss_name == \"train_compressor_mse\":\n",
    "            self.loss = self.loss_mse\n",
    "        elif loss_name == \"train_compressor_vmim\":\n",
    "            self.loss = self.loss_vmim\n",
    "        elif loss_name == \"train_compressor_gnll\":\n",
    "            self.loss = self.loss_gnll\n",
    "            if self.dim is None:\n",
    "                raise ValueError(\"dim should be specified when using gnll compressor\")\n",
    "        elif loss_name == \"loss_for_sbi\":\n",
    "            if info_compressor is None:\n",
    "                raise ValueError(\"sbi loss needs compressor informations\")\n",
    "            else:\n",
    "                self.info_compressor = info_compressor\n",
    "                self.loss = self.loss_nll\n",
    "\n",
    "    def loss_mse(self, params, theta, x, state_resnet):\n",
    "        \"\"\"Compute the Mean Squared Error loss\"\"\"\n",
    "        y, opt_state_resnet = self.compressor.apply(params, state_resnet, None, x)\n",
    "\n",
    "        loss = jnp.mean(jnp.sum((y - theta) ** 2, axis=1))\n",
    "\n",
    "        return loss, opt_state_resnet\n",
    "\n",
    "    def loss_mae(self, params, theta, x, state_resnet):\n",
    "        \"\"\"Compute the Mean Absolute Error loss\"\"\"\n",
    "        y, opt_state_resnet = self.compressor.apply(params, state_resnet, None, x)\n",
    "\n",
    "        loss = jnp.mean(jnp.sum(jnp.absolute(y - theta), axis=1))\n",
    "\n",
    "        return loss, opt_state_resnet\n",
    "\n",
    "    def loss_vmim(self, params, theta, x, state_resnet):\n",
    "        \"\"\"Compute the Variational Mutual Information Maximization loss\"\"\"\n",
    "        y, opt_state_resnet = self.compressor.apply(params, state_resnet, None, x)\n",
    "        log_prob = self.nf.apply(params, theta, y)\n",
    "\n",
    "        return -jnp.mean(log_prob), opt_state_resnet\n",
    "\n",
    "    def loss_gnll(self, params, theta, x, state_resnet):\n",
    "        \"\"\"Compute the Gaussian Negative Log Likelihood loss\"\"\"\n",
    "        y, opt_state_resnet = self.compressor.apply(params, state_resnet, None, x)\n",
    "        y_mean = y[..., : self.dim]\n",
    "        y_var = y[..., self.dim :]\n",
    "        y_var = tfb.FillScaleTriL(diag_bijector=tfb.Softplus(low=1e-3)).forward(y_var)\n",
    "\n",
    "        @jax.jit\n",
    "        @jax.vmap\n",
    "        def _get_log_prob(y_mean, y_var, theta):\n",
    "            likelihood = tfd.MultivariateNormalTriL(y_mean, y_var)\n",
    "            return likelihood.log_prob(theta)\n",
    "\n",
    "        loss = -jnp.mean(_get_log_prob(y_mean, y_var, theta))\n",
    "\n",
    "        return loss, opt_state_resnet\n",
    "\n",
    "    def loss_nll(self, params, theta, x, _):\n",
    "        \"\"\"Compute the Negative Log Likelihood loss.\n",
    "        This loss is for inference so it requires to have a trained compressor.\n",
    "        \"\"\"\n",
    "        y, _ = self.compressor.apply(\n",
    "            self.info_compressor[0], self.info_compressor[1], None, x\n",
    "        )\n",
    "        log_prob = self.nf.apply(params, theta, y)\n",
    "\n",
    "        return -jnp.mean(log_prob), _\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def update(self, model_params, opt_state, theta, x, state_resnet=None):\n",
    "        (loss, opt_state_resnet), grads = jax.value_and_grad(self.loss, has_aux=True)(\n",
    "            model_params, theta, x, state_resnet\n",
    "        )\n",
    "\n",
    "        updates, new_opt_state = self.optimizer.update(grads, opt_state)\n",
    "\n",
    "        new_params = optax.apply_updates(model_params, updates)\n",
    "\n",
    "        return loss, new_params, new_opt_state, opt_state_resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36dabbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "#mimic the argument parser used in the sbi_bm_lens\n",
    "\n",
    "class args_namedtuple(NamedTuple):\n",
    "\n",
    "    total_steps = 1000,\n",
    "\n",
    "    loss = \"train_compressor_vmim\",\n",
    "\n",
    "\n",
    "args = args_namedtuple()\n",
    "dim = 10\n",
    "N_particles = 10_000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17114efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### create compressor \n",
    "\n",
    "#nf \n",
    "bijector_layers_compressor = [128] * 2\n",
    "\n",
    "bijector_compressor = partial(\n",
    "    AffineCoupling, layers=bijector_layers_compressor, activation=jax.nn.silu\n",
    ")\n",
    "\n",
    "NF_compressor = partial(ConditionalRealNVP, n_layers=4, bijector_fn=bijector_compressor)\n",
    "\n",
    "\n",
    "class Flow_nd_Compressor(hk.Module):\n",
    "    def __call__(self, y):\n",
    "        nvp = NF_compressor(dim)(y)\n",
    "        return nvp\n",
    "\n",
    "\n",
    "nf = hk.without_apply_rng(\n",
    "    hk.transform(lambda theta, y: Flow_nd_Compressor()(y).log_prob(theta).squeeze())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03cdfdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.loss == \"train_compressor_gnll\":\n",
    "    compress_dim = int(dim + ((dim**2) - dim) / 2 + dim)\n",
    "else:\n",
    "    compress_dim = dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "045c7918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DeepSetsEncoder(hk.Module):\n",
    "#     def __init__(self, output_dim, hidden_dim: int = 128, name=None):\n",
    "#         super().__init__(name=name)\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.output_dim = output_dim\n",
    "\n",
    "#     def __call__(self, x):  # x: [N_particles, 6]\n",
    "#         # φ network: shared across all particles\n",
    "#         mlp_phi = hk.nets.MLP([self.hidden_dim, self.hidden_dim, self.output_dim])\n",
    "#         x_phi = mlp_phi(x)  # shape: [N_particles, output_dim]\n",
    "\n",
    "#         # Pooling over the set dimension (e.g., mean, sum)\n",
    "#         summary = jnp.mean(x_phi, axis=0)  # shape: [output_dim]\n",
    "\n",
    "#         return summary\n",
    "\n",
    "\n",
    "class DeepSetsEncoder(hk.Module):\n",
    "    def __init__(self, output_dim, hidden_dim: int = 128, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def __call__(self, x):  # x: [N_particles, 6] or [B, N_particles, 6]\n",
    "        mlp_phi = hk.nets.MLP([self.hidden_dim, self.hidden_dim, self.output_dim])\n",
    "\n",
    "        if x.ndim == 2:\n",
    "            # Unbatched case: [N_particles, 6]\n",
    "            x_phi = mlp_phi(x)  # [N_particles, output_dim]\n",
    "            summary = jnp.mean(x_phi, axis=0)  # [output_dim]\n",
    "        elif x.ndim == 3:\n",
    "            # Batched case: [B, N_particles, 6]\n",
    "            # Flatten for MLP: [B * N_particles, 6]\n",
    "            B, N, D = x.shape\n",
    "            x_flat = x.reshape(-1, D)\n",
    "            x_phi_flat = mlp_phi(x_flat)  # [B * N_particles, output_dim]\n",
    "            x_phi = x_phi_flat.reshape(B, N, self.output_dim)\n",
    "            summary = jnp.mean(x_phi, axis=1)  # [B, output_dim]\n",
    "        else:\n",
    "            raise ValueError(f\"Input must be of shape (N, D) or (B, N, D), got {x.shape}\")\n",
    "\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3481ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = hk.transform_with_state(\n",
    "    lambda y: DeepSetsEncoder(compress_dim)(y)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0ebbc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AffineCoupling shapes handling\n",
      "AffineCoupling shapes handling\n",
      "AffineCoupling shapes handling\n",
      "AffineCoupling shapes handling\n",
      "AffineCoupling shapes handling\n",
      "AffineCoupling shapes handling\n",
      "AffineCoupling shapes handling\n",
      "AffineCoupling shapes handling\n"
     ]
    }
   ],
   "source": [
    "### TRAIN\n",
    "# init compressor\n",
    "parameters_SetNet, opt_state_SetNet = compressor.init(\n",
    "    jax.random.PRNGKey(0), y=jnp.ones([1, N_particles, 6])\n",
    ")\n",
    "\n",
    "# init nf\n",
    "params_nf = nf.init(\n",
    "    jax.random.PRNGKey(0), theta=0.5 * jnp.ones([1, 5]), y=0.5 * jnp.ones([1, dim])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9793d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1001 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AffineCoupling shapes handling\n",
      "AffineCoupling shapes handling\n",
      "AffineCoupling shapes handling\n",
      "AffineCoupling shapes handling\n",
      "AffineCoupling shapes handling\n",
      "AffineCoupling shapes handling\n",
      "AffineCoupling shapes handling\n",
      "AffineCoupling shapes handling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 368/1001 [00:04<00:02, 287.06it/s]"
     ]
    }
   ],
   "source": [
    "if args.loss[0] == \"train_compressor_vmim\":\n",
    "    parameters_compressor = hk.data_structures.merge(parameters_SetNet, params_nf)\n",
    "elif args.loss[0] in [\n",
    "    \"train_compressor_mse\",\n",
    "    \"train_compressor_mae\",\n",
    "    \"train_compressor_gnll\",\n",
    "]:\n",
    "    parameters_compressor = parameters_SetNet\n",
    "\n",
    "\n",
    "# define optimizer\n",
    "total_steps = args.total_steps[0]\n",
    "\n",
    "if args.loss == \"train_compressor_gnll\":\n",
    "    start_lr = 0.0001\n",
    "\n",
    "else:\n",
    "    start_lr = 0.001\n",
    "\n",
    "lr_scheduler = optax.piecewise_constant_schedule(\n",
    "    init_value=start_lr,\n",
    "    boundaries_and_scales={\n",
    "        int(total_steps * 0.1): 0.7,\n",
    "        int(total_steps * 0.2): 0.7,\n",
    "        int(total_steps * 0.3): 0.7,\n",
    "        int(total_steps * 0.4): 0.7,\n",
    "        int(total_steps * 0.5): 0.7,\n",
    "        int(total_steps * 0.6): 0.7,\n",
    "        int(total_steps * 0.7): 0.7,\n",
    "        int(total_steps * 0.8): 0.7,\n",
    "        int(total_steps * 0.9): 0.7,\n",
    "    },\n",
    ")\n",
    "\n",
    "optimizer_c = optax.adam(learning_rate=lr_scheduler)\n",
    "opt_state_c = optimizer_c.init(parameters_compressor)\n",
    "\n",
    "model_compressor = TrainModel(\n",
    "    compressor=compressor,\n",
    "    nf=nf,\n",
    "    optimizer=optimizer_c,\n",
    "    loss_name=args.loss[0],\n",
    ")\n",
    "\n",
    "\n",
    "update = jax.jit(model_compressor.update)\n",
    "\n",
    "\n",
    "#load data\n",
    "data_path = './data/data_NFW/'\n",
    "pattern = re.compile(r\"chunk_(\\d+)\\.npz\")  # capture any number of digits\n",
    "files = sorted(\n",
    "    f for f in Path(data_path).glob(\"chunk_*.npz\")\n",
    "    if (m := pattern.fullmatch(f.name)) and int(m.group(1)) < 1000\n",
    ")\n",
    "theta_list, x_list, score_list = [], [], []\n",
    "\n",
    "for f in files:\n",
    "    data = np.load(f)\n",
    "    theta_list.append(data[\"theta\"])\n",
    "    x_list.append(data[\"x\"])\n",
    "    score_list.append(data[\"score\"]) \n",
    "    \n",
    "dataset_theta = jnp.array(theta_list,).reshape(-1, 5)\n",
    "dataset_y = jnp.array(x_list, ).reshape(-1, 10_000, 6)\n",
    "dataset_score = jnp.stack(score_list,).reshape(-1, 5)\n",
    "\n",
    "\n",
    "#normalization function\n",
    "@partial(jax.jit, static_argnums=(1,))\n",
    "def normalize(dataset, is_observable = False):\n",
    "    if is_observable:\n",
    "        # the shape in this case is (N, N_particles, 6)\n",
    "        dataset_original_shape = dataset.shape\n",
    "        normalized_dataset = dataset.reshape(-1, dataset.shape[-1])\n",
    "        mean = np.mean(normalized_dataset, axis=0)\n",
    "        std = np.std(normalized_dataset, axis=0)\n",
    "        normalized_dataset = (normalized_dataset - mean)/ (std + 1e-8) \n",
    "        normalized_dataset = normalized_dataset.reshape(dataset_original_shape)\n",
    "    else:\n",
    "        mean = np.mean(dataset, axis=0)\n",
    "        std = np.std(dataset, axis=0)\n",
    "        normalized_dataset = (dataset - mean)/ (std + 1e-8) \n",
    "\n",
    "    return normalized_dataset\n",
    "\n",
    "dataset_theta = normalize(dataset_theta)\n",
    "dataset_y = normalize(dataset_y, is_observable=True)\n",
    "dataset_score = normalize(dataset_score)\n",
    "    \n",
    "\n",
    "store_loss = []\n",
    "for batch in tqdm(range(total_steps + 1)):\n",
    "    theta = dataset_theta[batch]\n",
    "    x = dataset_y[batch]\n",
    "    score = dataset_score[batch]\n",
    "    if not jnp.isnan(score).any():\n",
    "        b_loss, parameters_compressor, opt_state_c, opt_state_SetNet = update(\n",
    "            model_params=parameters_compressor,\n",
    "            opt_state=opt_state_c,\n",
    "            theta=theta,\n",
    "            x=x,\n",
    "            state_resnet=opt_state_SetNet,\n",
    "        )\n",
    "        store_loss.append(b_loss)\n",
    "\n",
    "        if jnp.isnan(b_loss):\n",
    "            print(\"NaN Loss\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bea19ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'store_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m plt.plot(\u001b[43mstore_loss\u001b[49m)\n\u001b[32m      2\u001b[39m plt.xlabel(\u001b[33m\"\u001b[39m\u001b[33mBatch\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m plt.ylabel(\u001b[33m\"\u001b[39m\u001b[33mLoss\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'store_loss' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(store_loss)\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76422251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 10000, 6) (128, 5) (128, 5)\n",
      "AffineCoupling shapes handling\n",
      "x (128, 5)\n",
      "y (128, 10)\n",
      "net (128, 15)\n",
      "net (128, 128)\n",
      "AffineCoupling shapes handling\n",
      "x (128, 5)\n",
      "y (128, 10)\n",
      "net (128, 15)\n",
      "net (128, 128)\n",
      "AffineCoupling shapes handling\n",
      "x (128, 5)\n",
      "y (128, 10)\n",
      "net (128, 15)\n",
      "net (128, 128)\n",
      "AffineCoupling shapes handling\n",
      "x (128, 5)\n",
      "y (128, 10)\n",
      "net (128, 15)\n",
      "net (128, 128)\n",
      "AffineCoupling shapes handling\n",
      "x (128, 5)\n",
      "y (128, 10)\n",
      "net (128, 15)\n",
      "net (128, 128)\n",
      "AffineCoupling shapes handling\n",
      "x (128, 5)\n",
      "y (128, 10)\n",
      "net (128, 15)\n",
      "net (128, 128)\n",
      "AffineCoupling shapes handling\n",
      "x (128, 5)\n",
      "y (128, 10)\n",
      "net (128, 15)\n",
      "net (128, 128)\n",
      "AffineCoupling shapes handling\n",
      "x (128, 5)\n",
      "y (128, 10)\n",
      "net (128, 15)\n",
      "net (128, 128)\n",
      "NaN Loss\n"
     ]
    }
   ],
   "source": [
    "# batch_size = 128\n",
    "# num_batches = dataset_theta.shape[0] // batch_size\n",
    "\n",
    "# def get_batch(x_data, theta_data, score_data, batch_size, batch_idx):\n",
    "#     start = batch_idx * batch_size\n",
    "#     end = start + batch_size\n",
    "#     return x_data[start:end], theta_data[start:end], score_data[start:end]\n",
    "\n",
    "# store_loss = []\n",
    "# for batch_idx in range(num_batches):\n",
    "#     x_batch, theta_batch, score_batch = get_batch(dataset_y, dataset_theta, dataset_score, batch_size, batch_idx)\n",
    "#     print(x_batch.shape, theta_batch.shape, score_batch.shape)\n",
    "    \n",
    "#     if not jnp.isnan(score_batch).any():\n",
    "#         b_loss, parameters_compressor, opt_state_c, opt_state_SetNet = update(\n",
    "#             model_params=parameters_compressor,\n",
    "#             opt_state=opt_state_c,\n",
    "#             theta=theta_batch,\n",
    "#             x=x_batch,\n",
    "#             state_resnet=opt_state_SetNet,\n",
    "#         )\n",
    "#         store_loss.append(b_loss)\n",
    "\n",
    "#         if jnp.isnan(b_loss):\n",
    "#             print(\"NaN Loss\")\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caddc06d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Shift.__init__() got an unexpected keyword argument 'event_ndims'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtfb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mShift\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_ndims\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: Shift.__init__() got an unexpected keyword argument 'event_ndims'"
     ]
    }
   ],
   "source": [
    "tfb.Shift(jnp.ones((128, 128)), event_ndims=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f187e57e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbi_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
