{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faf1cfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_996084/2302780936.py:431: UserWarning: Missing a plate statement for batch dimension -2 at site 'obs_backward'. You can use `numpyro.util.format_shapes` utility to check shapes at all sites of your model.\n",
      "  mcmc.run(rng_key, )\n",
      "/tmp/ipykernel_996084/2302780936.py:431: UserWarning: Missing a plate statement for batch dimension -2 at site 'obs_forward'. You can use `numpyro.util.format_shapes` utility to check shapes at all sites of your model.\n",
      "  mcmc.run(rng_key, )\n",
      "sample: 100%|██████████| 5100/5100 [3:44:36<00:00,  2.64s/it, acc. prob=0.23]  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '2' \n",
    "from autocvd import autocvd\n",
    "autocvd(num_gpus = 1)\n",
    "\n",
    "import jax \n",
    "import jax.numpy as jnp\n",
    "from jax import jit, random\n",
    "import equinox as eqx\n",
    "from jax.sharding import Mesh, PartitionSpec, NamedSharding\n",
    "\n",
    "# jax.config.update(\"jax_enable_x64\", True)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "from astropy import units as u\n",
    "from astropy import constants as c\n",
    "\n",
    "import odisseo\n",
    "from odisseo import construct_initial_state\n",
    "from odisseo.integrators import leapfrog\n",
    "from odisseo.dynamics import direct_acc, DIRECT_ACC, DIRECT_ACC_LAXMAP, DIRECT_ACC_FOR_LOOP, DIRECT_ACC_MATRIX, NO_SELF_GRAVITY\n",
    "from odisseo.option_classes import SimulationConfig, SimulationParams, MNParams, NFWParams, PlummerParams, PSPParams, MN_POTENTIAL, NFW_POTENTIAL, PSP_POTENTIAL, DIFFRAX_BACKEND, LEAPFROG\n",
    "from odisseo.option_classes import SEMIIMPLICITEULER, TSIT5\n",
    "from odisseo.initial_condition import Plummer_sphere, Plummer_sphere_reparam\n",
    "from odisseo.utils import center_of_mass\n",
    "from odisseo.time_integration import time_integration\n",
    "from odisseo.units import CodeUnits\n",
    "from odisseo.visualization import create_3d_gif, create_projection_gif, energy_angular_momentum_plot\n",
    "from odisseo.potentials import MyamotoNagai, NFW\n",
    "\n",
    "from odisseo.utils import halo_to_gd1_velocity_vmap, halo_to_gd1_vmap, projection_on_GD1\n",
    "from jax.test_util import check_grads\n",
    "from numpyro.infer import MCMC, NUTS, AIES\n",
    "import arviz as az\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.size': 20,\n",
    "    'axes.labelsize': 20,\n",
    "    'xtick.labelsize': 13,\n",
    "    'ytick.labelsize': 13,\n",
    "    'legend.fontsize': 15,\n",
    "})\n",
    "\n",
    "plt.style.use('default')\n",
    "\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from jax import jit\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "code_length = 10 * u.kpc\n",
    "code_mass = 1e4 * u.Msun\n",
    "G = 1\n",
    "code_time = 3 * u.Gyr\n",
    "code_units = CodeUnits(code_length, code_mass, G=1, unit_time = code_time )  \n",
    "\n",
    "\n",
    "config = SimulationConfig(N_particles = 1000, \n",
    "                          return_snapshots = True, \n",
    "                          num_snapshots = 1000, \n",
    "                          num_timesteps = 1000, \n",
    "                          external_accelerations=(NFW_POTENTIAL, MN_POTENTIAL, PSP_POTENTIAL), \n",
    "                          acceleration_scheme = DIRECT_ACC_MATRIX,\n",
    "                          softening = (0.1 * u.pc).to(code_units.code_length).value,\n",
    "                          integrator = DIFFRAX_BACKEND,\n",
    "                          differentation_mode=TSIT5,\n",
    "                          fixed_timestep=False,\n",
    "                          ) #default values\n",
    "\n",
    "params = SimulationParams(t_end = (3 * u.Gyr).to(code_units.code_time).value,  \n",
    "                          Plummer_params= PlummerParams(Mtot=(10**4.5 * u.Msun).to(code_units.code_mass).value,\n",
    "                                                        a=(8 * u.pc).to(code_units.code_length).value),\n",
    "                           MN_params= MNParams(M = (68_193_902_782.346756 * u.Msun).to(code_units.code_mass).value,\n",
    "                                              a = (3.0 * u.kpc).to(code_units.code_length).value,\n",
    "                                              b = (0.280 * u.kpc).to(code_units.code_length).value),\n",
    "                          NFW_params= NFWParams(Mvir=(4.3683325e11 * u.Msun).to(code_units.code_mass).value,\n",
    "                                               r_s= (16.0 * u.kpc).to(code_units.code_length).value,),      \n",
    "                          PSP_params= PSPParams(M = 4501365375.06545 * u.Msun.to(code_units.code_mass),\n",
    "                                                alpha = 1.8, \n",
    "                                                r_c = (1.9*u.kpc).to(code_units.code_length).value),                    \n",
    "                          G=code_units.G, ) \n",
    "\n",
    "\n",
    "key = random.PRNGKey(1)\n",
    "\n",
    "#set up the particles in the initial state\n",
    "positions, velocities, mass = Plummer_sphere(key=key, params=params, config=config)\n",
    "\n",
    "#the center of mass needs to be integrated backwards in time first \n",
    "config_com = config._replace(N_particles=1,)\n",
    "params_com = params._replace(t_end=-params.t_end,)\n",
    "\n",
    "#this is the final position of the cluster, we need to integrate backwards in time \n",
    "pos_com_final = jnp.array([[11.8, 0.79, 6.4]]) * u.kpc.to(code_units.code_length)\n",
    "vel_com_final = jnp.array([[109.5,-254.5,-90.3]]) * (u.km/u.s).to(code_units.code_velocity)\n",
    "\n",
    "\n",
    "mass_com = jnp.array([params_com.Plummer_params.Mtot])\n",
    "final_state_com = construct_initial_state(pos_com_final, vel_com_final)\n",
    "\n",
    "snapshots_com = time_integration(final_state_com, mass_com, config_com, params_com)\n",
    "pos_com, vel_com = snapshots_com.states[-1, :, 0], snapshots_com.states[-1, :, 1]\n",
    "\n",
    "\n",
    "# Add the center of mass position and velocity to the Plummer sphere particles\n",
    "positions = positions + pos_com\n",
    "velocities = velocities + vel_com\n",
    "\n",
    "#initialize the initial state\n",
    "initial_state_stream = construct_initial_state(positions, velocities)\n",
    "\n",
    "#run the simulation\n",
    "snapshots = time_integration(initial_state_stream, mass, config, params)\n",
    "\n",
    "final_state = snapshots.states[-1]\n",
    "stream_data = projection_on_GD1(final_state, code_units=code_units,)\n",
    "\n",
    "params_sim = params\n",
    "\n",
    "\n",
    "# ----------------------------- Load observation & precompute target densities ----------------\n",
    "true_GD1_observation_path = '/export/data/vgiusepp/odisseo_data/data_fix_position/true.npz'\n",
    "_obs = np.load(true_GD1_observation_path)\n",
    "stream_data = jnp.array(_obs['x']).reshape(1000, 6)  # will be used only to compute target densities\n",
    "true_theta = jnp.array(_obs['theta'])\n",
    "\n",
    "\n",
    "@jit\n",
    "def run_simulation(params):\n",
    "\n",
    "    #Final position and velocity of the center of mass\n",
    "    pos_com_final = jnp.array([[11.8, 0.79, 6.4]]) * u.kpc.to(code_units.code_length)\n",
    "    vel_com_final = jnp.array([[109.5,-254.5,-90.3]]) * (u.km/u.s).to(code_units.code_velocity)\n",
    "    \n",
    "    #we construmt the initial state of the com \n",
    "    initial_state_com = construct_initial_state(pos_com_final, vel_com_final,)\n",
    "\n",
    "    #function that integrates the com backwards and forwards in time and then the stream, and projects it on the sky\n",
    "    @jit\n",
    "    def assign_params_integrate_projection(t_end):\n",
    "        new_params = params_sim._replace(\n",
    "                        NFW_params=params_sim.NFW_params._replace(\n",
    "                            Mvir = params['M_NFW']*u.Msun.to(code_units.code_mass),\n",
    "                            # r_s = params['r_s']*u.kpc.to(code_units.code_length),xs\n",
    "                        ),\n",
    "                        MN_params=params_sim.MN_params._replace(\n",
    "                            M = params['M_MN']*u.Msun.to(code_units.code_mass),\n",
    "                            # a = params['a_MN']*u.kpc.to(code_units.code_length),\n",
    "                        ),\n",
    "                        t_end = t_end,)\n",
    "        snapshots = time_integration(initial_state_com, mass, config=config_com, params=new_params)\n",
    "        stream_coordinate = jax.vmap(projection_on_GD1, in_axes=(0, None))(snapshots.states, code_units)\n",
    "        return stream_coordinate\n",
    "\n",
    "    t_end_mag = 0.2 * u.Gyr.to(code_units.code_time)\n",
    "    t_end_array = jnp.array([-t_end_mag, t_end_mag])  # backward, forward\n",
    "    \n",
    "    # vmap over both parameters\n",
    "    stream_coordinate_com = jax.vmap(assign_params_integrate_projection)(t_end_array)\n",
    "\n",
    "    return stream_coordinate_com\n",
    "\n",
    "\n",
    "@jit\n",
    "def stream_loglikelihood(stream_coordinate_com, ):\n",
    "    stream_coordinate_com_backward, stream_coordinate_com_forward = stream_coordinate_com[0], stream_coordinate_com[1]\n",
    "    phi1_min, phi1_max = -120, 70\n",
    "    phi2_min, phi2_max = -8, 2\n",
    "\n",
    "    # Create masks for valid time steps\n",
    "    mask_window_backward = (stream_coordinate_com_backward[:, 0, 1] < phi1_max) & \\\n",
    "                          (stream_coordinate_com_backward[:, 0, 1] > phi1_min) & \\\n",
    "                          (stream_coordinate_com_backward[:, 0, 2] < phi2_max) & \\\n",
    "                          (stream_coordinate_com_backward[:, 0, 2] > phi2_min)\n",
    "    \n",
    "    mask_diff_backward = jnp.ediff1d(stream_coordinate_com_backward[:, 0, 1], to_begin=1) > 0\n",
    "    \n",
    "    mask_window_forward = (stream_coordinate_com_forward[:, 0, 1] < phi1_max) & \\\n",
    "                         (stream_coordinate_com_forward[:, 0, 1] > phi1_min) & \\\n",
    "                         (stream_coordinate_com_forward[:, 0, 2] < phi2_max) & \\\n",
    "                         (stream_coordinate_com_forward[:, 0, 2] > phi2_min)\n",
    "    \n",
    "    mask_diff_forward = jnp.ediff1d(stream_coordinate_com_forward[:, 0, 1], to_begin=-1) < 0\n",
    "\n",
    "    # Combined time step masks\n",
    "    valid_time_backward = mask_window_backward & mask_diff_backward\n",
    "    valid_time_forward = mask_window_forward & mask_diff_forward\n",
    "\n",
    "    # Create masked coordinates for interpolation (only valid time steps)\n",
    "    phi1_backward_valid = jnp.where(valid_time_backward, \n",
    "                                   stream_coordinate_com_backward[:, 0, 1], \n",
    "                                   100000.)\n",
    "    \n",
    "    \n",
    "    phi1_forward_valid = jnp.where(valid_time_forward, \n",
    "                                  stream_coordinate_com_forward[:, 0, 1], \n",
    "                                  100000.)\n",
    "    \n",
    "\n",
    "    # Stream data masks - which data points to use for each direction\n",
    "    mask_stream_backward = stream_data[:, 1] > stream_coordinate_com_backward[0, 0, 1]\n",
    "    mask_stream_forward = stream_data[:, 1] < stream_coordinate_com_forward[0, 0, 1]\n",
    "\n",
    "    \n",
    "    coord_indices=jnp.array([2, 3, 4, 5])\n",
    "\n",
    "    def interpolate_coord_backward(coord_idx):\n",
    "\n",
    "        coord_backward_valid = jnp.where(valid_time_backward, \n",
    "                                   stream_coordinate_com_backward[:, 0, coord_idx], \n",
    "                                   100000.0)\n",
    "\n",
    "        return jnp.interp(\n",
    "            stream_data[:, 1], \n",
    "            phi1_backward_valid, \n",
    "            coord_backward_valid\n",
    "        )\n",
    "    \n",
    "    def interpolate_coord_forward(coord_idx):\n",
    "\n",
    "        coord_forward_valid = jnp.where(valid_time_forward, \n",
    "                                   stream_coordinate_com_forward[:, 0, coord_idx], \n",
    "                                   100000.0)\n",
    "\n",
    "        return jnp.interp(\n",
    "            stream_data[:, 1], \n",
    "            phi1_forward_valid, \n",
    "            coord_forward_valid\n",
    "        )\n",
    "\n",
    "    # Apply interpolation to all coordinates\n",
    "    interp_tracks_backward = jax.vmap(interpolate_coord_backward)(coord_indices)  # Shape: (n_coords, n_data)\n",
    "    interp_tracks_forward = jax.vmap(interpolate_coord_forward)(coord_indices)  # Shape: (n_coords, n_data)\n",
    "\n",
    "    # Calculate residuals for all coordinates\n",
    "    data_coords = stream_data[:, coord_indices].T  # Shape: (n_coords, n_data)\n",
    "\n",
    "    #\n",
    "    sigma = jnp.array([0.5, 10., 2., 2. ]) \n",
    " \n",
    "    # Calculate chi2 using only the appropriate data points for each direction\n",
    "    residuals_backward = jnp.where(mask_stream_backward, \n",
    "                                  (data_coords - interp_tracks_backward)/sigma[:, None],\n",
    "                                  0.0)\n",
    "    residuals_forward = jnp.where(mask_stream_forward, \n",
    "                                 (data_coords - interp_tracks_forward)/sigma[:, None],\n",
    "                                 0.0)\n",
    "    \n",
    "    chi2_backward = jnp.sum(residuals_backward**2) \n",
    "    chi2_forward = jnp.sum(residuals_forward**2) \n",
    "    \n",
    "    # Use only backward for now (as in your original code)\n",
    "    chi2 = chi2_backward + chi2_forward\n",
    "\n",
    "    n_valid = jnp.sum(mask_stream_backward) + jnp.sum(mask_stream_forward)\n",
    "    log_norm = - 0.5*n_valid * jnp.sum(jnp.log(2 * jnp.pi * sigma**2))\n",
    "    log_likelihood = -0.5 * chi2 + log_norm\n",
    "\n",
    "    return log_likelihood\n",
    "    \n",
    " \n",
    "\n",
    "\n",
    "# --- keep your existing imports; add these ---\n",
    "import numpy as np\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import MCMC, NUTS\n",
    "from numpyro.infer.initialization import init_to_value, init_to_median\n",
    "from numpyro.handlers import reparam\n",
    "from jax import random\n",
    "\n",
    "# --- ASSUMPTION: run_simulation(params_dict) returns the stream coordinates for given params\n",
    "# --- and stream_loglikelihood(stream_coordinate_com) returns the log-likelihood (float)\n",
    "\n",
    "# Wrap run_simulation and stream_loglikelihood in jitted functions (you already have @jit in your code).\n",
    "# Here we assume run_simulation expects a dict with keys 'M_NFW','r_s','M_MN','a_MN' (floats).\n",
    "\n",
    "# --- NUMPYRO MODEL ---\n",
    "def numpyro_stream_model():\n",
    "    \"\"\"\n",
    "    Model parameterization:\n",
    "      - sample **log** masses / radii so variables are roughly on similar additive scales.\n",
    "      - this is a simple, effective reparameterization to improve NUTS geometry.\n",
    "    \"\"\"\n",
    "    # Priors in log-space (you can widen/narrow the stddev as you prefer)\n",
    "    # log_M_NFW = numpyro.sample(\"log_M_NFW\", dist.Normal(jnp.log(4.3683325e11), 1.5))\n",
    "    # log_M_MN  = numpyro.sample(\"log_M_MN\",  dist.Normal(jnp.log(6.8193902782e10), 1.5))  # (example)\n",
    "    # log_r_s   = numpyro.sample(\"log_r_s\",   dist.Normal(jnp.log(16.0), 0.6))\n",
    "    # log_a_MN  = numpyro.sample(\"log_a_MN\",  dist.Normal(jnp.log(3.0), 0.6))\n",
    "\n",
    "    # Deterministic (expose transformed parameters for diagnostics/traces)\n",
    "    # M_NFW = numpyro.deterministic(\"M_NFW\", jnp.exp(log_M_NFW))\n",
    "    # M_MN  = numpyro.deterministic(\"M_MN\",  jnp.exp(log_M_MN))\n",
    "    # r_s   = numpyro.deterministic(\"r_s\",   jnp.exp(log_r_s))\n",
    "    # a_MN  = numpyro.deterministic(\"a_MN\",  jnp.exp(log_a_MN))\n",
    "\n",
    "    # M_NFW = numpyro.sample(\"M_NFW\", dist.Uniform(4.3683325e11 * 0.25, 4.3683325e11 * 2.0))\n",
    "\n",
    "    # M_MN  = numpyro.sample(\"M_MN\",  dist.Uniform(68_193_902_782.346756 * 0.25, 68_193_902_782.346756 * 2.0))  # (example)\n",
    "    # LogUniform distribution (recommended for masses)\n",
    "    log_M_NFW = numpyro.sample(\"log_M_NFW\", dist.Uniform(jnp.log(4.3683325e11 * 0.5), jnp.log(4.3683325e11 * 2.0)))\n",
    "    log_M_MN = numpyro.sample(\"log_M_MN\", dist.Uniform(jnp.log(68_193_902_782.346756 * 0.5), jnp.log(68_193_902_782.346756 * 2.0)))\n",
    "\n",
    "    # Transform back to physical units\n",
    "    M_NFW = numpyro.deterministic(\"M_NFW\", jnp.exp(log_M_NFW))\n",
    "    M_MN = numpyro.deterministic(\"M_MN\", jnp.exp(log_M_MN))\n",
    "\n",
    "    params_dict = {\n",
    "        \"M_NFW\": M_NFW,\n",
    "        # \"r_s\": r_s,\n",
    "        \"M_MN\": M_MN,\n",
    "        # \"a_MN\": a_MN,\n",
    "    }\n",
    "\n",
    "\n",
    "    stream_coordinate_com = run_simulation(params_dict)   # returns shape (2, n_snapshots, ncols) as before\n",
    "\n",
    "    # Register the likelihood with NumPyro\n",
    "    # numpyro.factor adds an arbitrary log-probability term to the joint.\n",
    "    # numpyro.factor(\"sim_loglik\", log_like)\n",
    "    phi1_min, phi1_max = -90, 10\n",
    "    phi2_min, phi2_max = -8, 2\n",
    "\n",
    "    stream_coordinate_com_backward, stream_coordinate_com_forward = stream_coordinate_com[0], stream_coordinate_com[1]\n",
    "    \n",
    "\n",
    "    # Create masks for valid time steps\n",
    "    mask_window_backward = (stream_coordinate_com_backward[:, 0, 1] < phi1_max) & \\\n",
    "                          (stream_coordinate_com_backward[:, 0, 1] > phi1_min) & \\\n",
    "                          (stream_coordinate_com_backward[:, 0, 2] < phi2_max) & \\\n",
    "                          (stream_coordinate_com_backward[:, 0, 2] > phi2_min)\n",
    "    \n",
    "    mask_diff_backward = jnp.ediff1d(stream_coordinate_com_backward[:, 0, 1], to_begin=1) > 0\n",
    "    # New mask - True until first False appears\n",
    "    mask_diff_backward = jnp.cumprod(mask_diff_backward, dtype=bool)\n",
    "\n",
    "\n",
    "    mask_window_forward = (stream_coordinate_com_forward[:, 0, 1] < phi1_max) & \\\n",
    "                         (stream_coordinate_com_forward[:, 0, 1] > phi1_min) & \\\n",
    "                         (stream_coordinate_com_forward[:, 0, 2] < phi2_max) & \\\n",
    "                         (stream_coordinate_com_forward[:, 0, 2] > phi2_min)\n",
    "    \n",
    "    mask_diff_forward = jnp.ediff1d(stream_coordinate_com_forward[:, 0, 1], to_begin=-1) < 0\n",
    "    mask_diff_forward = jnp.cumprod(mask_diff_forward, dtype=bool)\n",
    "\n",
    "\n",
    "    # Combined time step masks\n",
    "    valid_time_backward = mask_window_backward & mask_diff_backward\n",
    "    valid_time_forward = mask_window_forward & mask_diff_forward\n",
    "\n",
    "    # Create masked coordinates for interpolation (only valid time steps)\n",
    "    phi1_backward_valid = jnp.where(valid_time_backward, \n",
    "                                   stream_coordinate_com_backward[:, 0, 1], \n",
    "                                   10000.)\n",
    "    \n",
    "    \n",
    "    phi1_forward_valid = jnp.where(valid_time_forward, \n",
    "                                  stream_coordinate_com_forward[:, 0, 1], \n",
    "                                  -10000.)\n",
    "    \n",
    "\n",
    "    # Stream data masks - which data points to use for each direction\n",
    "    mask_stream_backward = stream_data[:, 1] > stream_coordinate_com_backward[0, 0, 1]\n",
    "    mask_stream_forward = stream_data[:, 1] < stream_coordinate_com_forward[0, 0, 1]\n",
    "\n",
    "    mask_evaluate_inside_track_backward = (stream_data[:, 1] < jnp.max(phi1_backward_valid)) & (stream_data[:, 1] < phi1_max)\n",
    "    mask_evaluate_inside_track_forward = (stream_data[:, 1] > jnp.min(phi1_forward_valid)) & (stream_data[:, 1] > phi1_min)\n",
    "\n",
    "    def interpolate_coord_backward(coord_idx):\n",
    "\n",
    "        coord_backward_valid = jnp.where(valid_time_backward, \n",
    "                                   stream_coordinate_com_backward[:, 0, coord_idx], \n",
    "                                   -100000.0)\n",
    "\n",
    "        return jnp.interp(\n",
    "            jnp.where(mask_stream_backward & mask_evaluate_inside_track_backward, stream_data[:, 1], 100000.0), \n",
    "            phi1_backward_valid, \n",
    "            coord_backward_valid\n",
    "        )\n",
    "    \n",
    "    def interpolate_coord_forward(coord_idx):\n",
    "\n",
    "        coord_forward_valid = jnp.where(valid_time_forward, \n",
    "                                   stream_coordinate_com_forward[:, 0, coord_idx], \n",
    "                                   100000.0)\n",
    "\n",
    "        return jnp.interp(\n",
    "            -jnp.where(mask_stream_forward & mask_evaluate_inside_track_forward, stream_data[:, 1], -100000.0), \n",
    "            -phi1_forward_valid, \n",
    "            coord_forward_valid\n",
    "        )\n",
    "        \n",
    "    coord_indices=jnp.array([2, 3, 4, 5])\n",
    "\n",
    "    # Apply interpolation to all coordinates\n",
    "    interp_tracks_backward = jax.vmap(interpolate_coord_backward)(coord_indices)  # Shape: (n_coords, n_data)\n",
    "    interp_tracks_forward = jax.vmap(interpolate_coord_forward)(coord_indices)  # Shape: (n_coords, n_data)\n",
    "\n",
    "    # Calculate residuals for all coordinates\n",
    "    data_coords = stream_data[:, coord_indices].T  # Shape: (n_coords, n_data)\n",
    "    sigma = jnp.array([0.5, 10., 2., 2. ])\n",
    "\n",
    "    \n",
    "    mask_correct_interpolation_backward = phi1_backward_valid < 8\n",
    "    mask_correct_interpolation_forward = phi1_forward_valid > - 88\n",
    "\n",
    "    with numpyro.plate(\"N\", data_coords.shape[1]):\n",
    "        masked_dist_backward = dist.Normal(interp_tracks_backward, sigma[:, None], ).mask(mask_stream_backward & mask_evaluate_inside_track_backward & mask_correct_interpolation_backward)\n",
    "        masked_dist_forward = dist.Normal(interp_tracks_forward, sigma[:, None], ).mask(mask_stream_forward & mask_evaluate_inside_track_forward & mask_correct_interpolation_forward)\n",
    "        numpyro.sample('obs_backward', masked_dist_backward, obs=data_coords,)\n",
    "        numpyro.sample('obs_forward', masked_dist_forward, obs=data_coords,) \n",
    "   \n",
    "\n",
    "# --- RUN AIES / MCMC ---\n",
    "rng_key = random.PRNGKey(42)\n",
    "\n",
    "# NUTS kernel: try dense_mass=True for complex geometry; set target_accept higher if needed\n",
    "kernel = AIES(numpyro_stream_model, )  # try True if needed\n",
    "mcmc = MCMC(kernel, num_warmup=100, num_samples=5000, num_chains=100, chain_method='vectorized', progress_bar=True, jit_model_args=True,)\n",
    "\n",
    "# (Optional) choose a good init strategy:\n",
    "# - init_to_median() is a reasonable generic choice if the prior is informative\n",
    "# - init_to_value({'log_M_NFW': jnp.log(4.368e11), ...}) can be used to start near truth\n",
    "# for demonstration we run with default init; you can pass init_strategy to NUTS()\n",
    "mcmc.run(rng_key, )\n",
    "# mcmc.print_summary(exclude_deterministic=False)  # include deterministic fields (M_NFW, r_s, ...)\n",
    "\n",
    "#saving\n",
    "# numpyro_data = az.from_numpyro(mcmc, extra_fields=\"potential_energy\")\n",
    "# numpyro_data.to_json('./aies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce534308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                 mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "       M_MN 97683415040.00 12486968320.00 98729828352.00 74401325056.00 116830240768.00  21807.54      1.00\n",
      "      M_NFW 289782267904.00 56222973952.00 277026701312.00 218418380800.00 369257447424.00  24775.69      1.00\n",
      "   log_M_MN     25.30      0.13     25.32     25.09     25.53  21958.71      1.00\n",
      "  log_M_NFW     26.38      0.18     26.35     26.11     26.63  24172.18      1.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mcmc.print_summary(exclude_deterministic=False)  # include deterministic fields (M_NFW, r_s, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccd7a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<numpyro.infer.mcmc.MCMC at 0x7f17d8592e70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ffb7617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 02:14:11.269642: E external/xla/xla/service/gpu/gpu_hlo_schedule.cc:795] The byte size of input/output arguments (16004024048) exceeds the base limit (8654290944). This indicates an error in the calculation!\n",
      "2025-10-08 02:14:11.325524: W external/xla/xla/hlo/transforms/simplifiers/hlo_rematerialization.cc:3023] Can't reduce memory use below 0B (0 bytes) by rematerialization; only reduced to 14.90GiB (16000240000 bytes), down from 14.90GiB (16000244144 bytes) originally\n",
      "2025-10-08 02:14:23.239086: W external/xla/xla/tsl/framework/bfc_allocator.cc:501] Allocator (GPU_0_bfc) ran out of memory trying to allocate 7.45GiB (rounded to 8000000000)requested by op \n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2025-10-08 02:14:23.239203: W external/xla/xla/tsl/framework/bfc_allocator.cc:512] *********************************************************************************************_______\n",
      "E1008 02:14:23.239228  996084 pjrt_stream_executor_client.cc:2916] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 8000000000 bytes. [tf-allocator-allocation-error='']\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "RESOURCE_EXHAUSTED: Out of memory while trying to allocate 8000000000 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mXlaRuntimeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m AIES_data = \u001b[43maz\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_numpyro\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmcmc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_sim/lib/python3.12/site-packages/arviz/data/io_numpyro.py:494\u001b[39m, in \u001b[36mfrom_numpyro\u001b[39m\u001b[34m(posterior, prior, posterior_predictive, predictions, constant_data, predictions_constant_data, log_likelihood, index_origin, coords, dims, pred_dims, extra_event_dims, num_chains)\u001b[39m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_numpyro\u001b[39m(\n\u001b[32m    421\u001b[39m     posterior=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    422\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    434\u001b[39m     num_chains=\u001b[32m1\u001b[39m,\n\u001b[32m    435\u001b[39m ):\n\u001b[32m    436\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Convert NumPyro data into an InferenceData object.\u001b[39;00m\n\u001b[32m    437\u001b[39m \n\u001b[32m    438\u001b[39m \u001b[33;03m    If no dims are provided, this will infer batch dim names from NumPyro model plates.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    478\u001b[39m \u001b[33;03m        Number of chains used for sampling. Ignored if posterior is present.\u001b[39;00m\n\u001b[32m    479\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    480\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNumPyroConverter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposterior\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposterior\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprior\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprior\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposterior_predictive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposterior_predictive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconstant_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconstant_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredictions_constant_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpredictions_constant_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_likelihood\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_likelihood\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_origin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_origin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpred_dims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpred_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_event_dims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_event_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_chains\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_chains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_inference_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_sim/lib/python3.12/site-packages/arviz/data/io_numpyro.py:392\u001b[39m, in \u001b[36mNumPyroConverter.to_inference_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mto_inference_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    382\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Convert all available data to an InferenceData object.\u001b[39;00m\n\u001b[32m    383\u001b[39m \n\u001b[32m    384\u001b[39m \u001b[33;03m    Note that if groups can not be created (i.e., there is no `trace`, so\u001b[39;00m\n\u001b[32m    385\u001b[39m \u001b[33;03m    the `posterior` and `sample_stats` can not be extracted), then the InferenceData\u001b[39;00m\n\u001b[32m    386\u001b[39m \u001b[33;03m    will not have those groups.\u001b[39;00m\n\u001b[32m    387\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    388\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m InferenceData(\n\u001b[32m    389\u001b[39m         **{\n\u001b[32m    390\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mposterior\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.posterior_to_xarray(),\n\u001b[32m    391\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33msample_stats\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.sample_stats_to_xarray(),\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlog_likelihood\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_likelihood_to_xarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    393\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mposterior_predictive\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.posterior_predictive_to_xarray(),\n\u001b[32m    394\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpredictions\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.predictions_to_xarray(),\n\u001b[32m    395\u001b[39m             **\u001b[38;5;28mself\u001b[39m.priors_to_xarray(),\n\u001b[32m    396\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mobserved_data\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.observed_data_to_xarray(),\n\u001b[32m    397\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mconstant_data\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.constant_data_to_xarray(),\n\u001b[32m    398\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpredictions_constant_data\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.predictions_constant_data_to_xarray(),\n\u001b[32m    399\u001b[39m         }\n\u001b[32m    400\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_sim/lib/python3.12/site-packages/arviz/data/base.py:71\u001b[39m, in \u001b[36mrequires.__call__.<locals>.wrapped\u001b[39m\u001b[34m(cls)\u001b[39m\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m((\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, prop_i) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m prop_i \u001b[38;5;129;01min\u001b[39;00m prop)):\n\u001b[32m     70\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_sim/lib/python3.12/site-packages/arviz/data/base.py:71\u001b[39m, in \u001b[36mrequires.__call__.<locals>.wrapped\u001b[39m\u001b[34m(cls)\u001b[39m\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m((\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, prop_i) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m prop_i \u001b[38;5;129;01min\u001b[39;00m prop)):\n\u001b[32m     70\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_sim/lib/python3.12/site-packages/arviz/data/io_numpyro.py:266\u001b[39m, in \u001b[36mNumPyroConverter.log_likelihood_to_xarray\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(samples, \u001b[33m\"\u001b[39m\u001b[33m_asdict\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    265\u001b[39m     samples = samples._asdict()\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m log_likelihood_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnumpyro\u001b[49m\u001b[43m.\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_likelihood\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_kwargs\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m obs_name, log_like \u001b[38;5;129;01min\u001b[39;00m log_likelihood_dict.items():\n\u001b[32m    270\u001b[39m     shape = (\u001b[38;5;28mself\u001b[39m.nchains, \u001b[38;5;28mself\u001b[39m.ndraws) + log_like.shape[\u001b[32m1\u001b[39m:]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_sim/lib/python3.12/site-packages/numpyro/infer/util.py:1147\u001b[39m, in \u001b[36mlog_likelihood\u001b[39m\u001b[34m(model, posterior_samples, parallel, batch_ndims, *args, **kwargs)\u001b[39m\n\u001b[32m   1145\u001b[39m batch_size = \u001b[38;5;28mint\u001b[39m(np.prod(batch_shape))\n\u001b[32m   1146\u001b[39m chunk_size = batch_size \u001b[38;5;28;01mif\u001b[39;00m parallel \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msoft_vmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43msingle_loglik\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposterior_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_sim/lib/python3.12/site-packages/numpyro/util.py:453\u001b[39m, in \u001b[36msoft_vmap\u001b[39m\u001b[34m(fn, xs, batch_ndims, chunk_size)\u001b[39m\n\u001b[32m    447\u001b[39m     xs = jax.tree.map(\n\u001b[32m    448\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m x: jnp.reshape(x, prepend_shape + (chunk_size,) + jnp.shape(x)[\u001b[32m1\u001b[39m:]),\n\u001b[32m    449\u001b[39m         xs,\n\u001b[32m    450\u001b[39m     )\n\u001b[32m    451\u001b[39m     fn = vmap(fn)\n\u001b[32m--> \u001b[39m\u001b[32m453\u001b[39m ys = \u001b[43mlax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m num_chunks > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m fn(xs)\n\u001b[32m    454\u001b[39m map_ndims = \u001b[38;5;28mint\u001b[39m(num_chunks > \u001b[32m1\u001b[39m) + \u001b[38;5;28mint\u001b[39m(chunk_size > \u001b[32m1\u001b[39m)\n\u001b[32m    455\u001b[39m ys = jax.tree.map(\n\u001b[32m    456\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m y: jnp.reshape(\n\u001b[32m    457\u001b[39m         y, (\u001b[38;5;28mint\u001b[39m(np.prod(jnp.shape(y)[:map_ndims])),) + jnp.shape(y)[map_ndims:]\n\u001b[32m    458\u001b[39m     )[:batch_size],\n\u001b[32m    459\u001b[39m     ys,\n\u001b[32m    460\u001b[39m )\n",
      "    \u001b[31m[... skipping hidden 14 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sbi_sim/lib/python3.12/site-packages/jax/_src/interpreters/pxla.py:1297\u001b[39m, in \u001b[36mExecuteReplicated.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1294\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.ordered_effects \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_unordered_effects\n\u001b[32m   1295\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_host_callbacks):\n\u001b[32m   1296\u001b[39m   input_bufs = \u001b[38;5;28mself\u001b[39m._add_tokens_to_inputs(input_bufs)\n\u001b[32m-> \u001b[39m\u001b[32m1297\u001b[39m   results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mxla_executable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_sharded\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1298\u001b[39m \u001b[43m      \u001b[49m\u001b[43minput_bufs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m   1299\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1301\u001b[39m   result_token_bufs = results.disassemble_prefix_into_single_device_arrays(\n\u001b[32m   1302\u001b[39m       \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.ordered_effects))\n\u001b[32m   1303\u001b[39m   sharded_runtime_token = results.consume_token()\n",
      "\u001b[31mXlaRuntimeError\u001b[39m: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 8000000000 bytes."
     ]
    }
   ],
   "source": [
    "AIES_data = az.from_numpyro(mcmc, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8589e956",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbi_sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
